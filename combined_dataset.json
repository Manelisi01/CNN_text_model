[
  {
    "text": "Recent advances in diffusion large language models (dLLMs) have introduced a promising alternative to autoregressive (AR) LLMs for natural language generation tasks, leveraging full attention and denoising-based decoding strategies. However, the deployment of these models on edge devices remains challenging due to their massive parameter scale and high resource demands. While post-training quantization (PTQ) has emerged as a widely adopted technique for compressing AR LLMs, its applicability to dLLMs remains largely unexplored. In this work, we present the first systematic study on quantizing diffusion-based language models. We begin by identifying the presence of activation outliers, characterized by abnormally large activation values that dominate the dynamic range. These outliers pose a key challenge to low-bit quantization, as they make it difficult to preserve precision for the majority of values. More importantly, we implement state-of-the-art PTQ methods and conduct a comprehensive evaluation across multiple task types and model variants. Our analysis is structured along four key dimensions: bit-width, quantization method, task category, and model type. Through this multi-perspective evaluation, we offer practical insights into the quantization behavior of dLLMs under different configurations. We hope our findings provide a foundation for future research in efficient dLLM deployment. All codes and experimental setups will be released to support the community.",
    "label": "human"
  },
  {
    "text": "The rapid progress in AI and Robotics may lead to a profound societal transformation, as humans and robots begin to coexist within shared communities, introducing both opportunities and challenges. To explore this future, we present Virtual Community-an open-world platform for humans, robots, and society-built on a universal physics engine and grounded in real-world 3D scenes. With Virtual Community, we aim to study embodied social intelligence at scale: 1) How robots can intelligently cooperate or compete; 2) How humans develop social relations and build community; 3) More importantly, how intelligent robots and humans can co-exist in an open world. To support these, Virtual Community features: 1) An open-source multi-agent physics simulator that supports robots, humans, and their interactions within a society; 2) A large-scale, real-world aligned community generation pipeline, including vast outdoor space, diverse indoor scenes, and a community of grounded agents with rich characters and appearances. Leveraging Virtual Community, we propose two novel challenges. The Community Planning Challenge evaluates multi-agent reasoning and planning ability in open-world settings, such as cooperating to help agents with daily activities and efficiently connecting other agents. The Community Robot Challenge requires multiple heterogeneous robots to collaborate in solving complex open-world tasks. We evaluate various baselines on these tasks and demonstrate the challenges in both high-level open-world task planning and low-level cooperation controls. We hope that Virtual Community will unlock further study of human-robot coexistence within open-world environments.",
    "label": "human"
  },
  {
    "text": "Prompt engineering has rapidly emerged as a critical skill for effective interaction with large language models (LLMs). However, the cognitive and neural underpinnings of this expertise remain largely unexplored. This paper presents findings from a cross-sectional pilot fMRI study investigating differences in brain functional connectivity and network activity between experts and intermediate prompt engineers. Our results reveal distinct neural signatures associated with higher prompt engineering literacy, including increased functional connectivity in brain regions such as the left middle temporal gyrus and the left frontal pole, as well as altered power-frequency dynamics in key cognitive networks. These findings offer initial insights into the neurobiological basis of prompt engineering proficiency. We discuss the implications of these neurocognitive markers in Natural Language Processing (NLP). Understanding the neural basis of human expertise in interacting with LLMs can inform the design of more intuitive human-AI interfaces, contribute to cognitive models of LLM interaction, and potentially guide the development of AI systems that better align with human cognitive workflows. This interdisciplinary approach aims to bridge the gap between human cognition and machine intelligence, fostering a deeper understanding of how humans learn and adapt to complex AI systems.",
    "label": "human"
  },
  {
    "text": "Temporal graph learning is crucial for dynamic networks where nodes and edges evolve over time and new nodes continuously join the system. Inductive representation learning in such settings faces two major challenges: effectively representing unseen nodes and mitigating noisy or redundant graph information. We propose GTGIB, a versatile framework that integrates Graph Structure Learning (GSL) with Temporal Graph Information Bottleneck (TGIB). We design a novel two-step GSL-based structural enhancer to enrich and optimize node neighborhoods and demonstrate its effectiveness and efficiency through theoretical proofs and experiments. The TGIB refines the optimized graph by extending the information bottleneck principle to temporal graphs, regularizing both edges and features based on our derived tractable TGIB objective function via variational approximation, enabling stable and efficient optimization. GTGIB-based models are evaluated to predict links on four real-world datasets; they outperform existing methods in all datasets under the inductive setting, with significant and consistent improvement in the transductive setting.",
    "label": "human"
  },
  {
    "text": "General Matrix Multiplication (GEMM) is a critical operation underpinning a wide range of applications in high-performance computing (HPC) and artificial intelligence (AI). The emergence of hardware optimized for low-precision arithmetic necessitates a reevaluation of numerical algorithms to leverage mixed-precision computations, achieving improved performance and energy efficiency. This research introduces an adaptive mixed-precision GEMM framework that supports different precision formats at fine-grained tile/block levels. We utilize the PaRSEC runtime system to balance workloads across various architectures. The performance scales well on ARM CPU-based Fugaku supercomputer, Nvidia GPU-based A100 DGX, and AMD GPU-based Frontier supercomputer. This research aims to enhance computational efficiency and accuracy by bridging algorithmic advancements and hardware innovations, driving transformative progress in various applications.",
    "label": "human"
  },
  {
    "text": "Accurately predicting enzyme functionality remains one of the major challenges in computational biology, particularly for enzymes with limited structural annotations or sequence homology. We present a novel multimodal Quantum Machine Learning (QML) framework that enhances Enzyme Commission (EC) classification by integrating four complementary biochemical modalities: protein sequence embeddings, quantum-derived electronic descriptors, molecular graph structures, and 2D molecular image representations. Quantum Vision Transformer (QVT) backbone equipped with modality-specific encoders and a unified cross-attention fusion module. By integrating graph features and spatial patterns, our method captures key stereoelectronic interactions behind enzyme function. Experimental results demonstrate that our multimodal QVT model achieves a top-1 accuracy of 85.1%, outperforming sequence-only baselines by a substantial margin and achieving better performance results compared to other QML models.",
    "label": "human"
  },
  {
    "text": "We prove a square-root space simulation for deterministic multitape Turing machines, showing ${\\rm TIME}[[t] \\subseteq {\\rm SPACE}[O(\\sqrt{t})]$. The key step is a Height Compression Theorem that uniformly (and in logspace) reshapes the canonical left-deep succinct computation tree for a block-respecting run into a binary tree whose evaluation-stack depth along any DFS path is $O(\\log T)$ for $T = \\lceil t/b \\rceil$, while preserving $O(b)$ work at leaves, $O(1)$ at internal nodes, and edges that are logspace-checkable; semantic correctness across merges is witnessed by an exact $O(b)$ window replay at the unique interface. The proof uses midpoint (balanced) recursion, a per-path potential that bounds simultaneously active interfaces by $O(\\log T)$, and an indegree-capping replacement of multiway merges by balanced binary combiners. Algorithmically, an Algebraic Replay Engine with constant-degree maps over a constant-size field, together with pointerless DFS and index-free streaming, ensures constant-size per-level tokens and eliminates wide counters, yielding the additive tradeoff $S(b)=O(b + \\log(t/b))$ for block sizes $b \\ge b_0$ with $b_0 = \\Theta(\\log t)$, which at the canonical choice $b = \\Theta(\\sqrt{t})$ gives $O(\\sqrt{t})$ space; the $b_0$ threshold rules out degenerate blocks where addressing scratch would dominate the window footprint. The construction is uniform, relativizes, and is robust to standard model choices. Consequences include branching-program upper bounds $2^{O(\\sqrt{s})}$ for size-$s$ bounded-fan-in circuits, tightened quadratic-time lower bounds for SPACE$[n]$-complete problems via the standard hierarchy argument, and $O(\\sqrt{t})$-space certifying interpreters; under explicit locality assumptions, the framework extends to geometric $d$-dimensional models.",
    "label": "human"
  },
  {
    "text": "Scaling inference through long chains-of-thought (CoTs) has unlocked impressive reasoning capabilities in large language models (LLMs), yet the reasoning process remains almost exclusively English-centric. We construct translated versions of two popular English reasoning datasets, fine-tune Qwen 2.5 (7B) and Qwen 3 (8B) models, and present a systematic study of long CoT generation across French, Japanese, Latvian, and Swahili. Our experiments reveal three key findings. First, the efficacy of using English as a pivot language varies by language: it provides no benefit for French, improves performance when used as the reasoning language for Japanese and Latvian, and proves insufficient for Swahili where both task comprehension and reasoning remain poor. Second, extensive multilingual pretraining in Qwen 3 narrows but does not eliminate the cross-lingual performance gap. A lightweight fine-tune using only 1k traces still improves performance by over 30\\% in Swahili. Third, data quality versus scale trade-offs are language dependent: small, carefully curated datasets suffice for English and French, whereas larger but noisier corpora prove more effective for Swahili and Latvian. Together, these results clarify when and why long CoTs transfer across languages and provide translated datasets to foster equitable multilingual reasoning research.",
    "label": "human"
  },
  {
    "text": "The role of Artificial Intelligence (AI) in education is undergoing a rapid transformation, moving beyond its historical function as an instructional tool towards a new potential as an active participant in the learning process. This shift is driven by the emergence of agentic AI, autonomous systems capable of proactive, goal-directed action. However, the field lacks a robust conceptual framework to understand, design, and evaluate this new paradigm of human-AI interaction in learning. This paper addresses this gap by proposing a novel conceptual framework (the APCP framework) that charts the transition from AI as a tool to AI as a collaborative partner. We present a four-level model of escalating AI agency within human-AI collaborative learning: (1) the AI as an Adaptive Instrument, (2) the AI as a Proactive Assistant, (3) the AI as a Co-Learner, and (4) the AI as a Peer Collaborator. Grounded in sociocultural theories of learning and Computer-Supported Collaborative Learning (CSCL), this framework provides a structured vocabulary for analysing the shifting roles and responsibilities between human and AI agents. The paper further engages in a critical discussion of the philosophical underpinnings of collaboration, examining whether an AI, lacking genuine consciousness or shared intentionality, can be considered a true collaborator. We conclude that while AI may not achieve authentic phenomenological partnership, it can be designed as a highly effective functional collaborator. This distinction has significant implications for pedagogy, instructional design, and the future research agenda for AI in education, urging a shift in focus towards creating learning environments that harness the complementary strengths of both human and AI.",
    "label": "human"
  },
  {
    "text": "Electronic health records (EHRs) are long, noisy, and often redundant, posing a major challenge for the clinicians who must navigate them. Large language models (LLMs) offer a promising solution for extracting and reasoning over this unstructured text, but the length of clinical notes often exceeds even state-of-the-art models' extended context windows. Retrieval-augmented generation (RAG) offers an alternative by retrieving task-relevant passages from across the entire EHR, potentially reducing the amount of required input tokens. In this work, we propose three clinical tasks designed to be replicable across health systems with minimal effort: 1) extracting imaging procedures, 2) generating timelines of antibiotic use, and 3) identifying key diagnoses. Using EHRs from actual hospitalized patients, we test three state-of-the-art LLMs with varying amounts of provided context, using either targeted text retrieval or the most recent clinical notes. We find that RAG closely matches or exceeds the performance of using recent notes, and approaches the performance of using the models' full context while requiring drastically fewer input tokens. Our results suggest that RAG remains a competitive and efficient approach even as newer models become capable of handling increasingly longer amounts of text.",
    "label": "human"
  },
  {
    "text": "The integration of information and communication technology (ICT) with traditional power grids has led to the emergence of smart grids. Advanced metering infrastructure (AMI) plays a crucial role in smart grids by facilitating two-way communication between smart meters and the utility provider. This bidirectional communication allows intelligent meters to report fine-grained consumption data at predefined intervals, enabling accurate billing, efficient grid monitoring and management, and rapid outage detection. However, the collection of detailed consumption data can inadvertently disclose consumers' daily activities, raising privacy concerns and potentially leading to privacy violations. To address these issues and preserve individuals' privacy, we propose a lightweight privacy-preserving smart metering protocol specifically designed to support real-time tariff billing service with dynamic policy adjustment. Our scheme employs an efficient data perturbation technique to obscure precise energy usage data from internal adversaries, including the intermediary gateways and the utility provider. Subsequently, we validate the efficiency and security of our protocol through comprehensive performance and privacy evaluations. We examined the computational, memory, and communication overhead of the proposed scheme. The execution time of our secure and privacy-aware billing system is approximately 3.94540 seconds for a complete year. Furthermore, we employed the Jensen-Shannon divergence as a privacy metric to demonstrate that our protocol can effectively safeguard users' privacy by increasing the noise scale.",
    "label": "human"
  },
  {
    "text": "Most existing illumination-editing approaches fail to simultaneously provide customized control of light effects and preserve content integrity. This makes them less effective for practical lighting stylization requirements, especially in the challenging task of transferring complex light effects from a reference image to a user-specified target image. To address this problem, we propose TransLight, a novel framework that enables high-fidelity and high-freedom transfer of light effects. Extracting the light effect from the reference image is the most critical and challenging step in our method. The difficulty lies in the complex geometric structure features embedded in light effects that are highly coupled with content in real-world scenarios. To achieve this, we first present Generative Decoupling, where two fine-tuned diffusion models are used to accurately separate image content and light effects, generating a newly curated, million-scale dataset of image-content-light triplets. Then, we employ IC-Light as the generative model and train our model with our triplets, injecting the reference lighting image as an additional conditioning signal. The resulting TransLight model enables customized and natural transfer of diverse light effects. Notably, by thoroughly disentangling light effects from reference images, our generative decoupling strategy endows TransLight with highly flexible illumination control. Experimental results establish TransLight as the first method to successfully transfer light effects across disparate images, delivering more customized illumination control than existing techniques and charting new directions for research in illumination harmonization and editing.",
    "label": "human"
  },
  {
    "text": "Prior medical image registration approaches, particularly learning-based methods, often require large amounts of training data, which constrains clinical adoption. To overcome this limitation, we propose a training-free pipeline that relies on a frozen DINOv3 encoder and test-time optimization of the deformation field in feature space. Across two representative benchmarks, the method is accurate and yields regular deformations. On Abdomen MR-CT, it attained the best mean Dice score (DSC) of 0.790 together with the lowest 95th percentile Hausdorff Distance (HD95) of 4.9+-5.0 and the lowest standard deviation of Log-Jacobian (SDLogJ) of 0.08+-0.02. On ACDC cardiac MRI, it improves mean DSC to 0.769 and reduces SDLogJ to 0.11 and HD95 to 4.8, a marked gain over the initial alignment. The results indicate that operating in a compact foundation feature space at test time offers a practical and general solution for clinical registration without additional training.",
    "label": "human"
  },
  {
    "text": "Whether AI models can introspect is an increasingly important practical question. But there is no consensus on how introspection is to be defined. Beginning from a recently proposed ''lightweight'' definition, we argue instead for a thicker one. According to our proposal, introspection in AI is any process which yields information about internal states through a process more reliable than one with equal or lower computational cost available to a third party. Using experiments where LLMs reason about their internal temperature parameters, we show they can appear to have lightweight introspection while failing to meaningfully introspect per our proposed definition.",
    "label": "human"
  },
  {
    "text": "Urban transportation systems encounter diverse challenges across multiple tasks, such as traffic forecasting, electric vehicle (EV) charging demand prediction, and taxi dispatch. Existing approaches suffer from two key limitations: small-scale deep learning models are task-specific and data-hungry, limiting their generalizability across diverse scenarios, while large language models (LLMs), despite offering flexibility through natural language interfaces, struggle with structured spatiotemporal data and numerical reasoning in transportation domains. To address these limitations, we propose TransLLM, a unified foundation framework that integrates spatiotemporal modeling with large language models through learnable prompt composition. Our approach features a lightweight spatiotemporal encoder that captures complex dependencies via dilated temporal convolutions and dual-adjacency graph attention networks, seamlessly interfacing with LLMs through structured embeddings. A novel instance-level prompt routing mechanism, trained via reinforcement learning, dynamically personalizes prompts based on input characteristics, moving beyond fixed task-specific templates. The framework operates by encoding spatiotemporal patterns into contextual representations, dynamically composing personalized prompts to guide LLM reasoning, and projecting the resulting representations through specialized output layers to generate task-specific predictions. Experiments across seven datasets and three tasks demonstrate the exceptional effectiveness of TransLLM in both supervised and zero-shot settings. Compared to ten baseline models, it delivers competitive performance on both regression and planning problems, showing strong generalization and cross-task adaptability. Our code is available at https://github.com/BiYunying/TransLLM.",
    "label": "human"
  },
  {
    "text": "Providing individualized scaffolding for physics problem solving at scale remains an instructional challenge. We investigate (1) students' perceptions of a Socratic Artificial Intelligence (AI) chatbot's impact on problem-solving skills and confidence and (2) how the specificity of students' questions during tutoring relates to performance. We deployed a custom Socratic AI chatbot in a large-enrollment introductory mechanics course at a Midwestern public university, logging full dialogue transcripts from 150 first-year STEM majors. Post-interaction surveys revealed median ratings of 4.0/5 for knowledge-based skills and 3.4/5 for overall effectiveness. Transcript analysis showed question specificity rose from approximately 10-15% in the first turn to 100% by the final turn, and specificity correlated positively with self reported expected course grade (Pearson r = 0.43). These findings demonstrate that AI-driven Socratic dialogue not only fosters expert-like reasoning but also generates fine-grained analytics for physics education research, establishing a scalable dual-purpose tool for instruction and learning analytics.",
    "label": "human"
  },
  {
    "text": "Designing therapeutic peptides with tailored properties is hindered by the vastness of sequence space, limited experimental data, and poor interpretability of current generative models. To address these challenges, we introduce PepThink-R1, a generative framework that integrates large language models (LLMs) with chain-of-thought (CoT) supervised fine-tuning and reinforcement learning (RL). Unlike prior approaches, PepThink-R1 explicitly reasons about monomer-level modifications during sequence generation, enabling interpretable design choices while optimizing for multiple pharmacological properties. Guided by a tailored reward function balancing chemical validity and property improvements, the model autonomously explores diverse sequence variants. We demonstrate that PepThink-R1 generates cyclic peptides with significantly enhanced lipophilicity, stability, and exposure, outperforming existing general LLMs (e.g., GPT-5) and domain-specific baseline in both optimization success and interpretability. To our knowledge, this is the first LLM-based peptide design framework that combines explicit reasoning with RL-driven property control, marking a step toward reliable and transparent peptide optimization for therapeutic discovery.",
    "label": "human"
  },
  {
    "text": "Qualitative analysis is typically limited to small datasets because it is time-intensive. Moreover, a second human rater is required to ensure reliable findings. Artificial intelligence tools may replace human raters if we demonstrate high reliability compared to human ratings. We investigated the inter-rater reliability of state-of-the-art Large Language Models (LLMs), ChatGPT-4o and ChatGPT-4.5-preview, in rating audio transcripts coded manually. We explored prompts and hyperparameters to optimize model performance. The participants were 14 undergraduate student groups from a university in the midwestern United States who discussed problem-solving strategies for a project. We prompted an LLM to replicate manual coding, and calculated Cohen's Kappa for inter-rater reliability. After optimizing model hyperparameters and prompts, the results showed substantial agreement (${\\kappa}>0.6$) for three themes and moderate agreement on one. Our findings demonstrate the potential of GPT-4o and GPT-4.5 for efficient, scalable qualitative analysis in physics education and identify their limitations in rating domain-general constructs.",
    "label": "human"
  },
  {
    "text": "Students in introductory physics courses often rely on ineffective strategies, focusing on final answers rather than understanding underlying principles. Integrating scientific argumentation into problem-solving fosters critical thinking and links conceptual knowledge with practical application. By facilitating learners to articulate their scientific arguments for solving problems, and by providing real-time feedback on students' strategies, we aim to enable students to develop superior problem-solving skills. Providing timely, individualized feedback to students in large-enrollment physics courses remains a challenge. Recent advances in Artificial Intelligence (AI) offer promising solutions. This study investigates the potential of AI-generated feedback on students' written scientific arguments in an introductory physics class. Using Open AI's GPT-4o, we provided delayed feedback on student written scientific arguments and surveyed them about the perceived usefulness and accuracy of this feedback. Our findings offer insights into the viability of implementing real-time AI feedback to enhance students' problem-solving and metacognitive skills in large-enrollment classrooms.",
    "label": "human"
  },
  {
    "text": "We present a method for generating large numbers of isomorphic physics problems using ChatGPT through prompt chaining and tool use. This approach enables precise control over structural variations-such as numeric values and spatial relations-while supporting diverse contextual variations in the problem body. By utilizing the Python code interpreter, the method supports automatic solution validation and simple diagram generation, addressing key limitations in existing LLM-based methods. We generated two example isomorphic problem banks and compared the outcome against simpler prompt-based approaches. Results show that prompt-chaining produces significantly higher quality and more consistent outputs than simpler, non-chaining prompts. This work demonstrates a promising method for efficient problem creation accessible to the average instructor, which opens new possibilities for personalized adaptive testing and automated content development.",
    "label": "human"
  },
  {
    "text": "Current SMILES-based diffusion models for molecule generation typically support only unimodal constraint. They inject conditioning signals at the start of the training process and require retraining a new model from scratch whenever the constraint changes. However, real-world applications often involve multiple constraints across different modalities, and additional constraints may emerge over the course of a study. This raises a challenge: how to extend a pre-trained diffusion model not only to support cross-modality constraints but also to incorporate new ones without retraining. To tackle this problem, we propose the Cross-Modality Controlled Molecule Generation with Diffusion Language Model (CMCM-DLM), demonstrated by two distinct cross modalities: molecular structure and chemical properties. Our approach builds upon a pre-trained diffusion model, incorporating two trainable modules, the Structure Control Module (SCM) and the Property Control Module (PCM), and operates in two distinct phases during the generation process. In Phase I, we employs the SCM to inject structural constraints during the early diffusion steps, effectively anchoring the molecular backbone. Phase II builds on this by further introducing PCM to guide the later stages of inference to refine the generated molecules, ensuring their chemical properties match the specified targets. Experimental results on multiple datasets demonstrate the efficiency and adaptability of our approach, highlighting CMCM-DLM's significant advancement in molecular generation for drug discovery applications.",
    "label": "human"
  },
  {
    "text": "Large language models (LLMs) are increasingly applied in multilingual contexts, yet their capacity for consistent, logically grounded alignment across languages remains underexplored. We present a controlled evaluation framework for multilingual natural language inference (NLI) that generates synthetic, logic-based premise-hypothesis pairs and translates them into a typologically diverse set of languages. This design enables precise control over semantic relations and allows testing in both monolingual and mixed-language (code-switched) conditions. Surprisingly, code-switching does not degrade, and can even improve, performance, suggesting that translation-induced lexical variation may serve as a regularization signal. We validate semantic preservation through embedding-based similarity analyses and cross-lingual alignment visualizations, confirming the fidelity of translated pairs. Our findings expose both the potential and the brittleness of current LLM cross-lingual reasoning, and identify code-switching as a promising lever for improving multilingual robustness. Code available at: https://github.com/KurbanIntelligenceLab/nli-stress-testing",
    "label": "human"
  },
  {
    "text": "In many real-world scenarios, acquiring all features of a data instance can be expensive or impractical due to monetary cost, latency, or privacy concerns. Active Feature Acquisition (AFA) addresses this challenge by dynamically selecting a subset of informative features for each data instance, trading predictive performance against acquisition cost. While numerous methods have been proposed for AFA, ranging from greedy information-theoretic strategies to non-myopic reinforcement learning approaches, fair and systematic evaluation of these methods has been hindered by the lack of standardized benchmarks. In this paper, we introduce AFABench, the first benchmark framework for AFA. Our benchmark includes a diverse set of synthetic and real-world datasets, supports a wide range of acquisition policies, and provides a modular design that enables easy integration of new methods and tasks. We implement and evaluate representative algorithms from all major categories, including static, greedy, and reinforcement learning-based approaches. To test the lookahead capabilities of AFA policies, we introduce a novel synthetic dataset, AFAContext, designed to expose the limitations of greedy selection. Our results highlight key trade-offs between different AFA strategies and provide actionable insights for future research. The benchmark code is available at: https://github.com/Linusaronsson/AFA-Benchmark.",
    "label": "human"
  },
  {
    "text": "Recently, the Manna-Pnueli Hierarchy has been used to define the temporal logics LTLfp and PPLTLp, which allow to use finite-trace LTLf/PPLTL techniques in infinite-trace settings while achieving the expressiveness of full LTL. In this paper, we present the first actual solvers for reactive synthesis in these logics. These are based on games on graphs that leverage DFA-based techniques from LTLf/PPLTL to construct the game arena. We start with a symbolic solver based on Emerson-Lei games, which reduces lower-class properties (guarantee, safety) to higher ones (recurrence, persistence) before solving the game. We then introduce Manna-Pnueli games, which natively embed Manna-Pnueli objectives into the arena. These games are solved by composing solutions to a DAG of simpler Emerson-Lei games, resulting in a provably more efficient approach. We implemented the solvers and practically evaluated their performance on a range of representative formulas. The results show that Manna-Pnueli games often offer significant advantages, though not universally, indicating that combining both approaches could further enhance practical performance.",
    "label": "human"
  },
  {
    "text": "Data augmentation is a critical technique in deep learning. Traditional methods like Back-translation typically focus on lexical-level rephrasing, which primarily produces variations with the same semantics. While large language models (LLMs) have enhanced text augmentation by their \"knowledge emergence\" capability, controlling the style and structure of these outputs remains challenging and requires meticulous prompt engineering. In this paper, we propose LMTransplant, a novel text augmentation paradigm leveraging LLMs. The core idea of LMTransplant is transplant-then-regenerate: incorporating seed text into a context expanded by LLM, and asking the LLM to regenerate a variant based on the expanded context. This strategy allows the model to create more diverse and creative content-level variants by fully leveraging the knowledge embedded in LLMs, while preserving the core attributes of the original text. We evaluate LMTransplant across various text-related tasks, demonstrating its superior performance over existing text augmentation methods. Moreover, LMTransplant demonstrates exceptional scalability as the size of augmented data grows.",
    "label": "human"
  },
  {
    "text": "Cyber-Physical Systems (CPS) are complex systems that require powerful models for tasks like verification, diagnosis, or debugging. Often, suitable models are not available and manual extraction is difficult. Data-driven approaches then provide a solution to, e.g., diagnosis tasks and verification problems based on data collected from the system. In this paper, we consider CPS with a discrete abstraction in the form of a Mealy machine. We propose a data-driven approach to determine the safety probability of the system on a finite horizon of n time steps. The approach is based on the Probably Approximately Correct (PAC) learning paradigm. Thus, we elaborate a connection between discrete logic and probabilistic reachability analysis of systems, especially providing an additional confidence on the determined probability. The learning process follows an active learning paradigm, where new learning data is sampled in a guided way after an initial learning set is collected. We validate the approach with a case study on an automated lane-keeping system.",
    "label": "human"
  },
  {
    "text": "Deploying speech enhancement (SE) systems in wearable devices, such as smart glasses, is challenging due to the limited computational resources on the device. Although deep learning methods have achieved high-quality results, their computational cost limits their feasibility on embedded platforms. This work presents an efficient end-to-end SE framework that leverages a Differentiable Digital Signal Processing (DDSP) vocoder for high-quality speech synthesis. First, a compact neural network predicts enhanced acoustic features from noisy speech: spectral envelope, fundamental frequency (F0), and periodicity. These features are fed into the DDSP vocoder to synthesize the enhanced waveform. The system is trained end-to-end with STFT and adversarial losses, enabling direct optimization at the feature and waveform levels. Experimental results show that our method improves intelligibility and quality by 4% (STOI) and 19% (DNSMOS) over strong baselines without significantly increasing computation, making it well-suited for real-time applications.",
    "label": "human"
  },
  {
    "text": "Despite the success of large language models (LLMs) in various domains, their potential in Traditional Chinese Medicine (TCM) remains largely underexplored due to two critical barriers: (1) the scarcity of high-quality TCM data and (2) the inherently multimodal nature of TCM diagnostics, which involve looking, listening, smelling, and pulse-taking. These sensory-rich modalities are beyond the scope of conventional LLMs. To address these challenges, we present ShizhenGPT, the first multimodal LLM tailored for TCM. To overcome data scarcity, we curate the largest TCM dataset to date, comprising 100GB+ of text and 200GB+ of multimodal data, including 1.2M images, 200 hours of audio, and physiological signals. ShizhenGPT is pretrained and instruction-tuned to achieve deep TCM knowledge and multimodal reasoning. For evaluation, we collect recent national TCM qualification exams and build a visual benchmark for Medicinal Recognition and Visual Diagnosis. Experiments demonstrate that ShizhenGPT outperforms comparable-scale LLMs and competes with larger proprietary models. Moreover, it leads in TCM visual understanding among existing multimodal LLMs and demonstrates unified perception across modalities like sound, pulse, smell, and vision, paving the way toward holistic multimodal perception and diagnosis in TCM. Datasets, models, and code are publicly available. We hope this work will inspire further exploration in this field.",
    "label": "human"
  },
  {
    "text": "We study payoff manipulation in repeated multi-objective Stackelberg games, where a leader may strategically influence a follower's deterministic best response, e.g., by offering a share of their own payoff. We assume that the follower's utility function, representing preferences over multiple objectives, is unknown but linear, and its weight parameter must be inferred through interaction. This introduces a sequential decision-making challenge for the leader, who must balance preference elicitation with immediate utility maximisation. We formalise this problem and propose manipulation policies based on expected utility (EU) and long-term expected utility (longEU), which guide the leader in selecting actions and offering incentives that trade off short-term gains with long-term impact. We prove that under infinite repeated interactions, longEU converges to the optimal manipulation. Empirical results across benchmark environments demonstrate that our approach improves cumulative leader utility while promoting mutually beneficial outcomes, all without requiring explicit negotiation or prior knowledge of the follower's utility function.",
    "label": "human"
  },
  {
    "text": "The Model Context Protocol has emerged as a transformative standard for connecting large language models to external data sources and tools, rapidly gaining adoption across major AI providers and development platforms. However, existing benchmarks are overly simplistic and fail to capture real application challenges such as long-horizon reasoning and large, unfamiliar tool spaces. To address this critical gap, we introduce MCP-Universe, the first comprehensive benchmark specifically designed to evaluate LLMs in realistic and hard tasks through interaction with real-world MCP servers. Our benchmark encompasses 6 core domains spanning 11 different MCP servers: Location Navigation, Repository Management, Financial Analysis, 3D Design, Browser Automation, and Web Searching. To ensure rigorous evaluation, we implement execution-based evaluators, including format evaluators for agent format compliance, static evaluators for time-invariant content matching, and dynamic evaluators that automatically retrieve real-time ground truth for temporally sensitive tasks. Through extensive evaluation of leading LLMs, we find that even SOTA models such as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit significant performance limitations. In addition, our benchmark poses a significant long-context challenge for LLM agents, as the number of input tokens increases rapidly with the number of interaction steps. Moreover, it introduces an unknown-tools challenge, as LLM agents often lack familiarity with the precise usage of the MCP servers. Notably, enterprise-level agents like Cursor cannot achieve better performance than standard ReAct frameworks. Beyond evaluation, we open-source our extensible evaluation framework with UI support, enabling researchers and practitioners to seamlessly integrate new agents and MCP servers while fostering innovation in the rapidly evolving MCP ecosystem.",
    "label": "human"
  },
  {
    "text": "Credit card fraud detection (CCFD) is a critical application of Machine Learning (ML) in the financial sector, where accurately identifying fraudulent transactions is essential for mitigating financial losses. ML models have demonstrated their effectiveness in fraud detection task, in particular with the tabular dataset. While adversarial attacks have been extensively studied in computer vision and deep learning, their impacts on the ML models, particularly those trained on CCFD tabular datasets, remains largely unexplored. These latent vulnerabilities pose significant threats to the security and stability of the financial industry, especially in high-value transactions where losses could be substantial. To address this gap, in this paper, we present a holistic framework that investigate the robustness of CCFD ML model against adversarial perturbations under different circumstances. Specifically, the gradient-based attack methods are incorporated into the tabular credit card transaction data in both black- and white-box adversarial attacks settings. Our findings confirm that tabular data is also susceptible to subtle perturbations, highlighting the need for heightened awareness among financial technology practitioners regarding ML model security and trustworthiness. Furthermore, the experiments by transferring adversarial samples from gradient-based attack method to non-gradient-based models also verify our findings. Our results demonstrate that such attacks remain effective, emphasizing the necessity of developing robust defenses for CCFD algorithms.",
    "label": "human"
  },
  {
    "text": "This study investigates how U.S. news media framed the use of ChatGPT in higher education from November 2022 to October 2024. Employing Framing Theory and combining temporal and sentiment analysis of 198 news articles, we trace the evolving narratives surrounding generative AI. We found that the media discourse largely centered on institutional responses; policy changes and teaching practices showed the most consistent presence and positive sentiment over time. Conversely, coverage of topics such as human-centered learning, the job market, and skill development appeared more sporadically, with initially uncertain portrayals gradually shifting toward cautious optimism. Importantly, media sentiment toward ChatGPT's role in college admissions remained predominantly negative. Our findings suggest that media narratives prioritize institutional responses to generative AI over long-term, broader ethical, social, and labor-related implications, shaping an emerging sociotechnical imaginary that frames generative AI in education primarily through the lens of adaptation and innovation.",
    "label": "human"
  },
  {
    "text": "Pre-trained foundation models have demonstrated remarkable success in vision and language, yet their potential for general machine signal modeling-covering acoustic, vibration, and other industrial sensor data-remains under-explored. Existing approach using sub-band-based encoders has achieved competitive results but are limited by fixed input lengths, and the absence of explicit frequency positional encoding. In this work, we propose a novel foundation model that integrates an advanced band-split architecture with relative frequency positional embeddings, enabling precise spectral localization across arbitrary sampling configurations. The model supports inputs of arbitrary length without padding or segmentation, producing a concise embedding that retains both temporal and spectral fidelity. We evaluate our method on SIREN (https://github.com/yucongzh/SIREN), a newly introduced large-scale benchmark for machine signal encoding that unifies multiple datasets, including all DCASE task 2 challenges (2020-2025) and widely-used industrial signal corpora. Experimental results demonstrate consistent state-of-the-art performance in anomaly detection and fault identification, confirming the effectiveness and generalization capability of the proposed model. We open-sourced ECHO on https://github.com/yucongzh/ECHO.",
    "label": "human"
  },
  {
    "text": "Time-series prediction involves forecasting future values using machine learning models. Feature engineering, whereby existing features are transformed to make new ones, is critical for enhancing model performance, but is often manual and time-intensive. Existing automation attempts rely on exhaustive enumeration, which can be computationally costly and lacks domain-specific insights. We introduce ELATE (Evolutionary Language model for Automated Time-series Engineering), which leverages a language model within an evolutionary framework to automate feature engineering for time-series data. ELATE employs time-series statistical measures and feature importance metrics to guide and prune features, while the language model proposes new, contextually relevant feature transformations. Our experiments demonstrate that ELATE improves forecasting accuracy by an average of 8.4% across various domains.",
    "label": "human"
  },
  {
    "text": "Aiming to enhance the consistency and thus long-term accuracy of Extended Kalman Filters for terrestrial vehicle localization, this paper introduces the Manifold Error State Extended Kalman Filter (M-ESEKF). By representing the robot's pose in a space with reduced dimensionality, the approach ensures feasible estimates on generic smooth surfaces, without introducing artificial constraints or simplifications that may degrade a filter's performance. The accompanying measurement models are compatible with common loosely- and tightly-coupled sensor modalities and also implicitly account for the ground geometry. We extend the formulation by introducing a novel correction scheme that embeds additional domain knowledge into the sensor data, giving more accurate uncertainty approximations and further enhancing filter consistency. The proposed estimator is seamlessly integrated into a validated modular state estimation framework, demonstrating compatibility with existing implementations. Extensive Monte Carlo simulations across diverse scenarios and dynamic sensor configurations show that the M-ESEKF outperforms classical filter formulations in terms of consistency and stability. Moreover, it eliminates the need for scenario-specific parameter tuning, enabling its application in a variety of real-world settings.",
    "label": "human"
  },
  {
    "text": "In recent years, the increasing frequency of extreme urban rainfall events has posed significant challenges to emergency scheduling systems. Urban flooding often leads to severe traffic congestion and service disruptions, threatening public safety and mobility. However, effective decision making remains hindered by three key challenges: (1) managing trade-offs among competing goals (e.g., traffic flow, task completion, and risk mitigation) requires dynamic, context-aware strategies; (2) rapidly evolving environmental conditions render static rules inadequate; and (3) LLM-generated strategies frequently suffer from semantic instability and execution inconsistency. Existing methods fail to align perception, global optimization, and multi-agent coordination within a unified framework. To tackle these challenges, we introduce H-J, a hierarchical multi-agent framework that integrates knowledge-guided prompting, entropy-constrained generation, and feedback-driven optimization. The framework establishes a closed-loop pipeline spanning from multi-source perception to strategic execution and continuous refinement. We evaluate H-J on real-world urban topology and rainfall data under three representative conditions: extreme rainfall, intermittent bursts, and daily light rain. Experiments show that H-J outperforms rule-based and reinforcement-learning baselines in traffic smoothness, task success rate, and system robustness. These findings highlight the promise of uncertainty-aware, knowledge-constrained LLM-based approaches for enhancing resilience in urban flood response.",
    "label": "human"
  },
  {
    "text": "Data plays a pivotal role in the groundbreaking advancements in artificial intelligence. The quantitative analysis of data significantly contributes to model training, enhancing both the efficiency and quality of data utilization. However, existing data analysis tools often lag in accuracy. For instance, many of these tools even assume that the loss function of neural networks is convex. These limitations make it challenging to implement current methods effectively. In this paper, we introduce a new formulation to approximate a sample's influence by accumulating the differences in influence between consecutive learning steps, which we term Diff-In. Specifically, we formulate the sample-wise influence as the cumulative sum of its changes/differences across successive training iterations. By employing second-order approximations, we approximate these difference terms with high accuracy while eliminating the need for model convexity required by existing methods. Despite being a second-order method, Diff-In maintains computational complexity comparable to that of first-order methods and remains scalable. This efficiency is achieved by computing the product of the Hessian and gradient, which can be efficiently approximated using finite differences of first-order gradients. We assess the approximation accuracy of Diff-In both theoretically and empirically. Our theoretical analysis demonstrates that Diff-In achieves significantly lower approximation error compared to existing influence estimators. Extensive experiments further confirm its superior performance across multiple benchmark datasets in three data-centric tasks: data cleaning, data deletion, and coreset selection. Notably, our experiments on data pruning for large-scale vision-language pre-training show that Diff-In can scale to millions of data points and outperforms strong baselines.",
    "label": "human"
  },
  {
    "text": "Local life service is a vital scenario in Kuaishou App, where video recommendation is intrinsically linked with store's location information. Thus, recommendation in our scenario is challenging because we should take into account user's interest and real-time location at the same time. In the face of such complex scenarios, end-to-end generative recommendation has emerged as a new paradigm, such as OneRec in the short video scenario, OneSug in the search scenario, and EGA in the advertising scenario. However, in local life service, an end-to-end generative recommendation model has not yet been developed as there are some key challenges to be solved. The first challenge is how to make full use of geographic information. The second challenge is how to balance multiple objectives, including user interests, the distance between user and stores, and some other business objectives. To address the challenges, we propose OneLoc. Specifically, we leverage geographic information from different perspectives: (1) geo-aware semantic ID incorporates both video and geographic information for tokenization, (2) geo-aware self-attention in the encoder leverages both video location similarity and user's real-time location, and (3) neighbor-aware prompt captures rich context information surrounding users for generation. To balance multiple objectives, we use reinforcement learning and propose two reward functions, i.e., geographic reward and GMV reward. With the above design, OneLoc achieves outstanding offline and online performance. In fact, OneLoc has been deployed in local life service of Kuaishou App. It serves 400 million active users daily, achieving 21.016% and 17.891% improvements in terms of gross merchandise value (GMV) and orders numbers.",
    "label": "human"
  },
  {
    "text": "Geometry problems are a crucial testbed for AI reasoning capabilities. Most existing geometry solving systems cannot express problems within a unified framework, thus are difficult to integrate with other mathematical fields. Besides, since most geometric proofs rely on intuitive diagrams, verifying geometry problems is particularly challenging. To address these gaps, we introduce LeanGeo, a unified formal system for formalizing and solving competition-level geometry problems within the Lean 4 theorem prover. LeanGeo features a comprehensive library of high-level geometric theorems with Lean's foundational logic, enabling rigorous proof verification and seamless integration with Mathlib. We also present LeanGeo-Bench, a formal geometry benchmark in LeanGeo, comprising problems from the International Mathematical Olympiad (IMO) and other advanced sources. Our evaluation demonstrates the capabilities and limitations of state-of-the-art Large Language Models on this benchmark, highlighting the need for further advancements in automated geometric reasoning. We open source the theorem library and the benchmark of LeanGeo at https://github.com/project-numina/LeanGeo/tree/master.",
    "label": "human"
  },
  {
    "text": "The ability to coordinate actions across multiple agents is critical for solving complex, real-world problems. Large Language Models (LLMs) have shown strong capabilities in communication, planning, and reasoning, raising the question of whether they can also support effective collaboration in multi-agent settings. In this work, we investigate the use of LLM agents to solve a structured victim rescue task that requires division of labor, prioritization, and cooperative planning. Agents operate in a fully known graph-based environment and must allocate resources to victims with varying needs and urgency levels. We systematically evaluate their performance using a suite of coordination-sensitive metrics, including task success rate, redundant actions, room conflicts, and urgency-weighted efficiency. This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements.",
    "label": "human"
  },
  {
    "text": "This paper examines the implications of using the Scale-Invariant Signal-to-Distortion Ratio (SI-SDR) as both evaluation and training objective in supervised speech separation, when the training references contain noise, as is the case with the de facto benchmark WSJ0-2Mix. A derivation of the SI-SDR with noisy references reveals that noise limits the achievable SI-SDR, or leads to undesired noise in the separated outputs. To address this, a method is proposed to enhance references and augment the mixtures with WHAM!, aiming to train models that avoid learning noisy references. Two models trained on these enhanced datasets are evaluated with the non-intrusive NISQA.v2 metric. Results show reduced noise in separated speech but suggest that processing references may introduce artefacts, limiting overall quality gains. Negative correlation is found between SI-SDR and perceived noisiness across models on the WSJ0-2Mix and Libri2Mix test sets, underlining the conclusion from the derivation.",
    "label": "human"
  },
  {
    "text": "Component-based synthesis (CBS) aims to generate loop-free programs from a set of libraries whose methods are annotated with specifications and whose output must satisfy a set of logical constraints, expressed as a query. The effectiveness of a CBS algorithm critically depends on the severity of the constraints imposed by the query. The more exact these constraints are, the sparser the space of feasible solutions. This maxim also applies when we enrich the expressiveness of the specifications affixed to library methods. In both cases, the search must now contend with constraints that may only hold over a small number of the possible execution paths that can be enumerated by a CBS procedure.   In this paper, we address this challenge by equipping CBS search with the ability to reason about logical similarities among the paths it explores. Our setting considers library methods equipped with refinement-type specifications that enrich ordinary base types with a set of rich logical qualifiers to constrain the set of values accepted by that type. We perform a search over a tree automata variant called Qualified Tree Automata that intelligently records information about enumerated terms, leveraging subtyping constraints over the refinement types associated with these terms to enable reasoning about similarity among candidate solutions as search proceeds, thereby avoiding exploration of semantically similar paths.   We present an implementation of this idea in a tool called \\name, and provide a comprehensive evaluation that demonstrates \\name's ability to synthesize solutions to complex CBS queries that go well-beyond the capabilities of the existing state-of-the-art.",
    "label": "human"
  },
  {
    "text": "Point cloud videos capture dynamic 3D motion while reducing the effects of lighting and viewpoint variations, making them highly effective for recognizing subtle and continuous human actions. Although Selective State Space Models (SSMs) have shown good performance in sequence modeling with linear complexity, the spatio-temporal disorder of point cloud videos hinders their unidirectional modeling when directly unfolding the point cloud video into a 1D sequence through temporally sequential scanning. To address this challenge, we propose the Unified Spatio-Temporal State Space Model (UST-SSM), which extends the latest advancements in SSMs to point cloud videos. Specifically, we introduce Spatial-Temporal Selection Scanning (STSS), which reorganizes unordered points into semantic-aware sequences through prompt-guided clustering, thereby enabling the effective utilization of points that are spatially and temporally distant yet similar within the sequence. For missing 4D geometric and motion details, Spatio-Temporal Structure Aggregation (STSA) aggregates spatio-temporal features and compensates. To improve temporal interaction within the sampled sequence, Temporal Interaction Sampling (TIS) enhances fine-grained temporal dependencies through non-anchor frame utilization and expanded receptive fields. Experimental results on the MSR-Action3D, NTU RGB+D, and Synthia 4D datasets validate the effectiveness of our method. Our code is available at https://github.com/wangzy01/UST-SSM.",
    "label": "human"
  },
  {
    "text": "Heterogeneous accelerator-centric compute clusters are emerging as efficient solutions for diverse AI workloads. However, current integration strategies often compromise data movement efficiency and encounter compatibility issues in hardware and software. This prevents a unified approach that balances performance and ease of use. To this end, we present SNAX, an open-source integrated HW-SW framework enabling efficient multi-accelerator platforms through a novel hybrid-coupling scheme, consisting of loosely coupled asynchronous control and tightly coupled data access. SNAX brings reusable hardware modules designed to enhance compute accelerator utilization, and its customizable MLIR-based compiler to automate key system management tasks, jointly enabling rapid development and deployment of customized multi-accelerator compute clusters. Through extensive experimentation, we demonstrate SNAX's efficiency and flexibility in a low-power heterogeneous SoC. Accelerators can easily be integrated and programmed to achieve > 10x improvement in neural network performance compared to other accelerator systems while maintaining accelerator utilization of > 90% in full system operation.",
    "label": "human"
  },
  {
    "text": "Since the introduction of Industry 4.0, digital twin technology has significantly evolved, laying the groundwork for a transition toward Industry 5.0 principles centered on human-centricity, sustainability, and resilience. Through digital twins, real-time connected production systems are anticipated to be more efficient, resilient, and sustainable, facilitating communication and connectivity between digital and physical systems. However, environmental performance and integration with virtual reality (VR) and artificial intelligence (AI) of such systems remain challenging. Further exploration of digital twin technologies is needed to validate the real-world impact and benefits. This paper investigates these challenges by implementing a real-time digital twin based on the ISO 23247 standard, connecting the physical factory and simulation software with VR capabilities. This digital twin system provides cognitive assistance and a user-friendly interface for operators, thereby improving cognitive ergonomics. The connection of the Internet of Things (IoT) platform allows the digital twin to have real-time bidirectional communication, collaboration, monitoring, and assistance. A lab-scale drone factory was used as the digital twin application to test and evaluate the ISO 23247 standard and its potential benefits. Additionally, AI integration and environmental performance Key Performance Indicators (KPIs) have been considered as the next stages in improving VR-integrated digital twins. With a solid theoretical foundation and a demonstration of the VR-integrated digital twins, this paper addresses integration issues between various technologies and advances the framework of digital twins based on ISO 23247.",
    "label": "human"
  },
  {
    "text": "The emergence of new intelligent applications has fostered the development of a task-oriented communication paradigm, where a comprehensive, universal, and practical metric is crucial for unleashing the potential of this paradigm. To this end, we introduce an innovative metric, the Task-oriented Age of Information (TAoI), to measure whether the content of information is relevant to the system task, thereby assisting the system in efficiently completing designated tasks. We apply TAoI to a wireless monitoring system tasked with identifying targets and transmitting their images for subsequent analysis. To minimize TAoI and determine the optimal transmission policy, we formulate the dynamic transmission problem as a Semi-Markov Decision Process (SMDP) and transform it into an equivalent Markov Decision Process (MDP). Our analysis demonstrates that the optimal policy is threshold-based with respect to TAoI. Building on this, we propose a low-complexity relative value iteration algorithm tailored to this threshold structure to derive the optimal transmission policy. Additionally, we introduce a simpler single-threshold policy, which, despite a slight performance degradation, offers faster convergence. Comprehensive experiments and simulations validate the superior performance of our optimal transmission policy compared to two established baseline approaches.",
    "label": "human"
  },
  {
    "text": "One of the most common methods to train machine learning algorithms today is the stochastic gradient descent (SGD). In a distributed setting, SGD-based algorithms have been shown to converge theoretically under specific circumstances. A substantial number of works in the distributed SGD setting assume a fixed topology for the edge devices. These papers also assume that the contribution of nodes to the global model is uniform. However, experiments have shown that such assumptions are suboptimal and a non uniform aggregation strategy coupled with a dynamically shifting topology and client selection can significantly improve the performance of such models. This paper details a unified framework that covers several Local-Update SGD-based distributed algorithms with dynamic topologies and provides improved or matching theoretical guarantees on convergence compared to existing work.",
    "label": "human"
  },
  {
    "text": "Recent advances in large language models (LLMs) and reasoning frameworks have opened new possibilities for improving the perspective -taking capabilities of autonomous agents. However, tasks that involve active perception, collaborative reasoning, and perspective taking (understanding what another agent can see or knows) pose persistent challenges for current LLM-based systems. This study investigates the potential of structured examples derived from transformed solution graphs generated by the Fast Downward planner to improve the performance of LLM-based agents within a ReAct framework. We propose a structured solution-processing pipeline that generates three distinct categories of examples: optimal goal paths (G-type), informative node paths (E-type), and step-by-step optimal decision sequences contrasting alternative actions (L-type). These solutions are further converted into ``thought-action'' examples by prompting an LLM to explicitly articulate the reasoning behind each decision. While L-type examples slightly reduce clarification requests and overall action steps, they do not yield consistent improvements. Agents are successful in tasks requiring basic attentional filtering but struggle in scenarios that required mentalising about occluded spaces or weighing the costs of epistemic actions. These findings suggest that structured examples alone are insufficient for robust perspective-taking, underscoring the need for explicit belief tracking, cost modelling, and richer environments to enable socially grounded collaboration in LLM-based agents.",
    "label": "human"
  },
  {
    "text": "We introduce a new music source separation model tailored for accurate vocal isolation. Unlike Transformer-based approaches, which often fail to capture intermittently occurring vocals, our model leverages Mamba2, a recent state space model, to better capture long-range temporal dependencies. To handle long input sequences efficiently, we combine a band-splitting strategy with a dual-path architecture. Experiments show that our approach outperforms recent state-of-the-art models, achieving a cSDR of 11.03 dB-the best reported to date-and delivering substantial gains in uSDR. Moreover, the model exhibits stable and consistent performance across varying input lengths and vocal occurrence patterns. These results demonstrate the effectiveness of Mamba-based models for high-resolution audio processing and open up new directions for broader applications in audio research.",
    "label": "human"
  },
  {
    "text": "To address the challenges of localization drift and perception-planning coupling in unmanned aerial vehicles (UAVs) operating in open-top scenarios (e.g., collapsed buildings, roofless mazes), this paper proposes EAROL, a novel framework with a downward-mounted tilted LiDAR configuration (20{\\deg} inclination), integrating a LiDAR-Inertial Odometry (LIO) system and a hierarchical trajectory-yaw optimization algorithm. The hardware innovation enables constraint enhancement via dense ground point cloud acquisition and forward environmental awareness for dynamic obstacle detection. A tightly-coupled LIO system, empowered by an Iterative Error-State Kalman Filter (IESKF) with dynamic motion compensation, achieves high level 6-DoF localization accuracy in feature-sparse environments. The planner, augmented by environment, balancing environmental exploration, target tracking precision, and energy efficiency. Physical experiments demonstrate 81% tracking error reduction, 22% improvement in perceptual coverage, and near-zero vertical drift across indoor maze and 60-meter-scale outdoor scenarios. This work proposes a hardware-algorithm co-design paradigm, offering a robust solution for UAV autonomy in post-disaster search and rescue missions. We will release our software and hardware as an open-source package for the community. Video: https://youtu.be/7av2ueLSiYw.",
    "label": "human"
  },
  {
    "text": "Over time, software systems have reached a level of complexity that makes it difficult for their developers and users to explain particular decisions made by them. In this paper, we focus on the explainability of component-based systems for Question Answering (QA). These components often conduct processes driven by AI methods, in which behavior and decisions cannot be clearly explained or justified, s.t., even for QA experts interpreting the executed process and its results is hard. To address this challenge, we present an approach that considers the components' input and output data flows as a source for representing the behavior and provide explanations for the components, enabling users to comprehend what happened. In the QA framework used here, the data flows of the components are represented as SPARQL queries (inputs) and RDF triples (outputs). Hence, we are also providing valuable insights on verbalization regarding these data types. In our experiments, the approach generates explanations while following template-based settings (baseline) or via the use of Large Language Models (LLMs) with different configurations (automatic generation). Our evaluation shows that the explanations generated via LLMs achieve high quality and mostly outperform template-based approaches according to the users' ratings. Therefore, it enables us to automatically explain the behavior and decisions of QA components to humans while using RDF and SPARQL as a context for explanations.",
    "label": "human"
  },
  {
    "text": "We study the problem of optimizing Large Language Model (LLM) inference scheduling to minimize total latency. LLM inference is an online and multi-task service process and also heavily energy consuming by which a pre-trained LLM processes input requests and generates output tokens sequentially. Therefore, it is vital to improve its scheduling efficiency and reduce the power consumption while a great amount of prompt requests are arriving. A key challenge in LLM inference scheduling is that while the prompt length is known upon arrival, the output length, which critically impacts memory usage and processing time, is unknown. To address this uncertainty, we propose algorithms that leverage machine learning to predict output lengths, assuming the prediction provides an interval classification (min-max range) for each request.   We first design a conservative algorithm, $\\mathcal{A}_{\\max}$, which schedules requests based on the upper bound of predicted output lengths to prevent memory overflow. However, this approach is overly conservative: as prediction accuracy decreases, performance degrades significantly due to potential overestimation. To overcome this limitation, we propose $\\mathcal{A}_{\\min}$, an adaptive algorithm that initially treats the predicted lower bound as the output length and dynamically refines this estimate during inferencing. We prove that $\\mathcal{A}_{\\min}$ achieves a log-scale competitive ratio. Through numerical simulations, we demonstrate that $\\mathcal{A}_{\\min}$ often performs nearly as well as the hindsight scheduler, highlighting both its efficiency and robustness in practical scenarios. Moreover, $\\mathcal{A}_{\\min}$ relies solely on the lower bound of the prediction interval--an advantageous design choice since upper bounds on output length are typically more challenging to predict accurately.",
    "label": "human"
  },
  {
    "text": "In this paper, we address the problem of manual debugging, which nowadays remains resource-intensive and in some parts archaic. This problem is especially evident in increasingly complex and distributed software systems. Therefore, our objective of this work is to introduce an approach that can possibly be applied to any system, at both the macro- and micro-level, to ease this debugging process. This approach utilizes a system's process data, in conjunction with generative AI, to generate natural-language explanations. These explanations are generated from the actual process data, interface information, and documentation to guide the developers more efficiently to understand the behavior and possible errors of a process and its sub-processes. Here, we present a demonstrator that employs this approach on a component-based Java system. However, our approach is language-agnostic. Ideally, the generated explanations will provide a good understanding of the process, even if developers are not familiar with all the details of the considered system. Our demonstrator is provided as an open-source web application that is freely accessible to all users.",
    "label": "human"
  },
  {
    "text": "The performance of Deep Q-Networks (DQN) is critically dependent on the ability of its underlying neural network to accurately approximate the action-value function. Standard function approximators, such as multi-layer perceptrons, may struggle to efficiently represent the complex value landscapes inherent in many reinforcement learning problems. This paper introduces a novel architecture, the Chebyshev-DQN (Ch-DQN), which integrates a Chebyshev polynomial basis into the DQN framework to create a more effective feature representation. By leveraging the powerful function approximation properties of Chebyshev polynomials, we hypothesize that the Ch-DQN can learn more efficiently and achieve higher performance. We evaluate our proposed model on the CartPole-v1 benchmark and compare it against a standard DQN with a comparable number of parameters. Our results demonstrate that the Ch-DQN with a moderate polynomial degree (N=4) achieves significantly better asymptotic performance, outperforming the baseline by approximately 39\\%. However, we also find that the choice of polynomial degree is a critical hyperparameter, as a high degree (N=8) can be detrimental to learning. This work validates the potential of using orthogonal polynomial bases in deep reinforcement learning while also highlighting the trade-offs involved in model complexity.",
    "label": "human"
  },
  {
    "text": "We introduce EffiFusion-GAN (Efficient Fusion Generative Adversarial Network), a lightweight yet powerful model for speech enhancement. The model integrates depthwise separable convolutions within a multi-scale block to capture diverse acoustic features efficiently. An enhanced attention mechanism with dual normalization and residual refinement further improves training stability and convergence. Additionally, dynamic pruning is applied to reduce model size while maintaining performance, making the framework suitable for resource-constrained environments. Experimental evaluation on the public VoiceBank+DEMAND dataset shows that EffiFusion-GAN achieves a PESQ score of 3.45, outperforming existing models under the same parameter settings.",
    "label": "human"
  },
  {
    "text": "Spiking neural networks (SNNs) offer advantages in computational efficiency via event-driven computing, compared to traditional artificial neural networks (ANNs). While direct training methods tackle the challenge of non-differentiable activation mechanisms in SNNs, they often suffer from high computational and energy costs during training. As a result, ANN-to-SNN conversion approach still remains a valuable and practical alternative. These conversion-based methods aim to leverage the discrete output produced by the quantization layer to obtain SNNs with low latency. Although the theoretical minimum latency is one timestep, existing conversion methods have struggled to realize such ultra-low latency without accuracy loss. Moreover, current quantization approaches often discard negative-value information following batch normalization and are highly sensitive to the hyperparameter configuration, leading to degraded performance. In this work, we, for the first time, analyze the information loss introduced by quantization layers through the lens of information entropy. Building on our analysis, we introduce Polarity Multi-Spike Mapping (PMSM) and a hyperparameter adjustment strategy tailored for the quantization layer. Our method achieves nearly lossless ANN-to-SNN conversion at the extremity, i.e., the first timestep, while also leveraging the temporal dynamics of SNNs across multiple timesteps to maintain stable performance on complex tasks. Experimental results show that our PMSM achieves state-of-the-art accuracies of 98.5% on CIFAR-10, 89.3% on CIFAR-100 and 81.6% on ImageNet with only one timestep on ViT-S architecture, establishing a new benchmark for efficient conversion. In addition, our method reduces energy consumption by over 5x under VGG-16 on CIFAR-10 and CIFAR-100, compared to the baseline method.",
    "label": "human"
  },
  {
    "text": "Large-scale industrial recommendation systems typically employ a two-stage paradigm of retrieval and ranking to handle huge amounts of information. Recent research focuses on improving the performance of retrieval model. A promising way is to introduce extensive information about users and items. On one hand, lifelong sequential behavior is valuable. Existing lifelong behavior modeling methods in ranking stage focus on the interaction of lifelong behavior and candidate items from retrieval stage. In retrieval stage, it is difficult to utilize lifelong behavior because of a large corpus of candidate items. On the other hand, existing retrieval methods mostly relay on interaction information, potentially disregarding valuable multi-modal information. To solve these problems, we represent the pioneering exploration of leveraging multi-modal information and lifelong sequence model within the advanced tree-based retrieval model. We propose Multi-modal Indexing and Searching with lifelong Sequence (MISS), which contains a multi-modal index tree and a multi-modal lifelong sequence modeling module. Specifically, for better index structure, we propose multi-modal index tree, which is built using the multi-modal embedding to precisely represent item similarity. To precisely capture diverse user interests in user lifelong sequence, we propose collaborative general search unit (Co-GSU) and multi-modal general search unit (MM-GSU) for multi-perspective interests searching.",
    "label": "human"
  },
  {
    "text": "This paper proposes a high-precision semantic segmentation method based on an improved TransUNet architecture to address the challenges of complex lesion structures, blurred boundaries, and significant scale variations in skin lesion images. The method integrates a transformer module into the traditional encoder-decoder framework to model global semantic information, while retaining a convolutional branch to preserve local texture and edge features. This enhances the model's ability to perceive fine-grained structures. A boundary-guided attention mechanism and multi-scale upsampling path are also designed to improve lesion boundary localization and segmentation consistency. To verify the effectiveness of the approach, a series of experiments were conducted, including comparative studies, hyperparameter sensitivity analysis, data augmentation effects, input resolution variation, and training data split ratio tests. Experimental results show that the proposed model outperforms existing representative methods in mIoU, mDice, and mAcc, demonstrating stronger lesion recognition accuracy and robustness. In particular, the model achieves better boundary reconstruction and structural recovery in complex scenarios, making it well-suited for the key demands of automated segmentation tasks in skin lesion analysis.",
    "label": "human"
  },
  {
    "text": "Domain-specific datasets are the foundation for unleashing artificial intelligence (AI)-driven wireless innovation. Yet existing wireless AI corpora are slow to produce, offer limited modeling fidelity, and cover only narrow scenario types. To address the challenges, we create DeepTelecom, a three-dimension (3D) digital-twin channel dataset. Specifically, a large language model (LLM)-assisted pipeline first builds the third level of details (LoD3) outdoor and indoor scenes with segmentable material-parameterizable surfaces. Then, DeepTelecom simulates full radio-wave propagation effects based on Sionna's ray-tracing engine. Leveraging GPU acceleration, DeepTelecom streams ray-path trajectories and real-time signal-strength heat maps, compiles them into high-frame-rate videos, and simultaneously outputs synchronized multi-view images, channel tensors, and multi-scale fading traces. By efficiently streaming large-scale, high-fidelity, and multimodal channel data, DeepTelecom not only furnishes a unified benchmark for wireless AI research but also supplies the domain-rich training substrate that enables foundation models to tightly fuse large model intelligence with future communication systems.",
    "label": "human"
  },
  {
    "text": "The detection of anomalies in manufacturing processes is crucial to ensure product quality and identify process deviations. Statistical and data-driven approaches remain the standard in industrial anomaly detection, yet their adaptability and usability are constrained by the dependence on extensive annotated datasets and limited flexibility under dynamic production conditions. Recent advances in the perception capabilities of foundation models provide promising opportunities for their adaptation to this downstream task. This paper presents PB-IAD (Prompt-based Industrial Anomaly Detection), a novel framework that leverages the multimodal and reasoning capabilities of foundation models for industrial anomaly detection. Specifically, PB-IAD addresses three key requirements of dynamic production environments: data sparsity, agile adaptability, and domain user centricity. In addition to the anomaly detection, the framework includes a prompt template that is specifically designed for iteratively implementing domain-specific process knowledge, as well as a pre-processing module that translates domain user inputs into effective system prompts. This user-centric design allows domain experts to customise the system flexibly without requiring data science expertise. The proposed framework is evaluated by utilizing GPT-4.1 across three distinct manufacturing scenarios, two data modalities, and an ablation study to systematically assess the contribution of semantic instructions. Furthermore, PB-IAD is benchmarked to state-of-the-art methods for anomaly detection such as PatchCore. The results demonstrate superior performance, particularly in data-sparse scenarios and low-shot settings, achieved solely through semantic instructions.",
    "label": "human"
  },
  {
    "text": "This study proposes an anomaly detection method based on the Transformer architecture with integrated multiscale feature perception, aiming to address the limitations of temporal modeling and scale-aware feature representation in cloud service environments. The method first employs an improved Transformer module to perform temporal modeling on high-dimensional monitoring data, using a self-attention mechanism to capture long-range dependencies and contextual semantics. Then, a multiscale feature construction path is introduced to extract temporal features at different granularities through downsampling and parallel encoding. An attention-weighted fusion module is designed to dynamically adjust the contribution of each scale to the final decision, enhancing the model's robustness in anomaly pattern modeling. In the input modeling stage, standardized multidimensional time series are constructed, covering core signals such as CPU utilization, memory usage, and task scheduling states, while positional encoding is used to strengthen the model's temporal awareness. A systematic experimental setup is designed to evaluate performance, including comparative experiments and hyperparameter sensitivity analysis, focusing on the impact of optimizers, learning rates, anomaly ratios, and noise levels. Experimental results show that the proposed method outperforms mainstream baseline models in key metrics, including precision, recall, AUC, and F1-score, and maintains strong stability and detection performance under various perturbation conditions, demonstrating its superior capability in complex cloud environments.",
    "label": "human"
  },
  {
    "text": "Shapley values are widely recognized as a principled method for attributing importance to input features in machine learning. However, the exact computation of Shapley values scales exponentially with the number of features, severely limiting the practical application of this powerful approach. The challenge is further compounded when the predictive model is probabilistic - as in Gaussian processes (GPs) - where the outputs are random variables rather than point estimates, necessitating additional computational effort in modeling higher-order moments. In this work, we demonstrate that for an important class of GPs known as FANOVA GP, which explicitly models all main effects and interactions, *exact* Shapley attributions for both local and global explanations can be computed in *quadratic time*. For local, instance-wise explanations, we define a stochastic cooperative game over function components and compute the exact stochastic Shapley value in quadratic time only, capturing both the expected contribution and uncertainty. For global explanations, we introduce a deterministic, variance-based value function and compute exact Shapley values that quantify each feature's contribution to the model's overall sensitivity. Our methods leverage a closed-form (stochastic) M\\\"{o}bius representation of the FANOVA decomposition and introduce recursive algorithms, inspired by Newton's identities, to efficiently compute the mean and variance of Shapley values. Our work enhances the utility of explainable AI, as demonstrated by empirical studies, by providing more scalable, axiomatically sound, and uncertainty-aware explanations for predictions generated by structured probabilistic models.",
    "label": "human"
  },
  {
    "text": "Neuronal spikes directly drive muscles and endow animals with agile movements, but applying the spike-based control signals to actuators in artificial sensor-motor systems inevitably causes a collapse of learning. We developed a system that can vary \\emph{the number of independent synaptic bundles} in sensor-to-motor connections. This paper demonstrates the following four findings: (i) Learning collapses once the number of motor neurons or the number of independent synaptic bundles exceeds a critical limit. (ii) The probability of learning failure is increased by a smaller number of motor neurons, while (iii) if learning succeeds, a smaller number of motor neurons leads to faster learning. (iv) The number of weight updates that move in the opposite direction of the optimal weight can quantitatively explain these results. The functions of spikes remain largely unknown. Identifying the parameter range in which learning systems using spikes can be constructed will make it possible to study the functions of spikes that were previously inaccessible due to the difficulty of learning.",
    "label": "human"
  },
  {
    "text": "This paper presents the open-system submission by the In2x research team for the WMT25 General Machine Translation Shared Task. Our submission focuses on Japanese-related translation tasks, aiming to explore a generalizable paradigm for extending large language models (LLMs) to other languages. This paradigm encompasses aspects such as data construction methods and reward model design. The ultimate goal is to enable large language model systems to achieve exceptional performance in low-resource or less commonly spoken languages.",
    "label": "human"
  },
  {
    "text": "The increasing adoption of low-cost environmental sensors and AI-enabled applications has accelerated the demand for scalable and resilient data infrastructures, particularly in data-scarce and resource-constrained regions. This paper presents the design, implementation, and evaluation of the AirQo data pipeline: a modular, cloud-native Extract-Transform-Load (ETL) system engineered to support both real-time and batch processing of heterogeneous air quality data across urban deployments in Africa. It is Built using open-source technologies such as Apache Airflow, Apache Kafka, and Google BigQuery. The pipeline integrates diverse data streams from low-cost sensors, third-party weather APIs, and reference-grade monitors to enable automated calibration, forecasting, and accessible analytics. We demonstrate the pipeline's ability to ingest, transform, and distribute millions of air quality measurements monthly from over 400 monitoring devices while achieving low latency, high throughput, and robust data availability, even under constrained power and connectivity conditions. The paper details key architectural features, including workflow orchestration, decoupled ingestion layers, machine learning-driven sensor calibration, and observability frameworks. Performance is evaluated across operational metrics such as resource utilization, ingestion throughput, calibration accuracy, and data availability, offering practical insights into building sustainable environmental data platforms. By open-sourcing the platform and documenting deployment experiences, this work contributes a reusable blueprint for similar initiatives seeking to advance environmental intelligence through data engineering in low-resource settings.",
    "label": "human"
  },
  {
    "text": "We introduce Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer language model designed to increase throughput for reasoning workloads while achieving state-of-the-art accuracy compared to similarly-sized models. Nemotron-Nano-9B-v2 builds on the Nemotron-H architecture, in which the majority of the self-attention layers in the common Transformer architecture are replaced with Mamba-2 layers, to achieve improved inference speed when generating the long thinking traces needed for reasoning. We create Nemotron-Nano-9B-v2 by first pre-training a 12-billion-parameter model (Nemotron-Nano-12B-v2-Base) on 20 trillion tokens using an FP8 training recipe. After aligning Nemotron-Nano-12B-v2-Base, we employ the Minitron strategy to compress and distill the model with the goal of enabling inference on up to 128k tokens on a single NVIDIA A10G GPU (22GiB of memory, bfloat16 precision). Compared to existing similarly-sized models (e.g., Qwen3-8B), we show that Nemotron-Nano-9B-v2 achieves on-par or better accuracy on reasoning benchmarks while achieving up to 6x higher inference throughput in reasoning settings like 8k input and 16k output tokens. We are releasing Nemotron-Nano-9B-v2, Nemotron-Nano12B-v2-Base, and Nemotron-Nano-9B-v2-Base checkpoints along with the majority of our pre- and post-training datasets on Hugging Face.",
    "label": "human"
  },
  {
    "text": "Humans regularly navigate an overwhelming amount of information via text media, whether reading articles, browsing social media, or interacting with chatbots. Confusion naturally arises when new information conflicts with or exceeds a reader's comprehension or prior knowledge, posing a challenge for learning. In this study, we present a multimodal investigation of reading-induced confusion using EEG and eye tracking. We collected neural and gaze data from 11 adult participants as they read short paragraphs sampled from diverse, real-world sources. By isolating the N400 event-related potential (ERP), a well-established neural marker of semantic incongruence, and integrating behavioral markers from eye tracking, we provide a detailed analysis of the neural and behavioral correlates of confusion during naturalistic reading. Using machine learning, we show that multimodal (EEG + eye tracking) models improve classification accuracy by 4-22% over unimodal baselines, reaching an average weighted participant accuracy of 77.3% and a best accuracy of 89.6%. Our results highlight the dominance of the brain's temporal regions in these neural signatures of confusion, suggesting avenues for wearable, low-electrode brain-computer interfaces (BCI) for real-time monitoring. These findings lay the foundation for developing adaptive systems that dynamically detect and respond to user confusion, with potential applications in personalized learning, human-computer interaction, and accessibility.",
    "label": "human"
  },
  {
    "text": "Advancements in AI have led to agents in networked environments increasingly mirroring human behavior, thereby blurring the boundary between artificial and human actors in specific contexts. This shift brings about significant challenges in trust, responsibility, ethics, security and etc. The difficulty in supervising of agent behaviors may lead to issues such as data contamination and unclear accountability. To address these challenges, this paper proposes the \"Network Behavior Lifecycle\" model, which divides network behavior into 6 stages and systematically analyzes the behavioral differences between humans and agents at each stage. Based on these insights, the paper further introduces the \"Agent for Agent (A4A)\" paradigm and the \"Human-Agent Behavioral Disparity (HABD)\" model, which examine the fundamental distinctions between human and agent behaviors across 5 dimensions: decision mechanism, execution efficiency, intention-behavior consistency, behavioral inertia, and irrational patterns. The effectiveness of the model is verified through real-world cases such as red team penetration and blue team defense. Finally, the paper discusses future research directions in dynamic cognitive governance architecture, behavioral disparity quantification, and meta-governance protocol stacks, aiming to provide a theoretical foundation and technical roadmap for secure and trustworthy human-agent collaboration.",
    "label": "human"
  },
  {
    "text": "Optimization Modeling (OM) is essential for solving complex decision-making problems. However, the process remains time-consuming and error-prone, heavily relying on domain experts. While Large Language Models (LLMs) show promise in addressing these challenges through their natural language understanding and reasoning capabilities, current approaches face three critical limitations: high benchmark labeling error rates reaching up to 42\\%, narrow evaluation scope that only considers optimal values, and computational inefficiency due to heavy reliance on multi-agent systems or model fine-tuning. In this work, we first enhance existing datasets through systematic error correction and more comprehensive annotation. Additionally, we introduce LogiOR, a new optimization modeling benchmark from the logistics domain, containing more complex problems with standardized annotations. Furthermore, we present ORThought, a novel framework that leverages expert-level optimization modeling principles through chain-of-thought reasoning to automate the OM process. Through extensive empirical evaluation, we demonstrate that ORThought outperforms existing approaches, including multi-agent frameworks, with particularly significant advantages on complex optimization problems. Finally, we provide a systematic analysis of our method, identifying critical success factors and failure modes, providing valuable insights for future research on LLM-based optimization modeling.",
    "label": "human"
  },
  {
    "text": "Large language models (LLMs) have been shown to possess a degree of self-recognition capability-the ability to identify whether a given text was generated by themselves. Prior work has demonstrated that this capability is reliably expressed under the Pair Presentation Paradigm (PPP), where the model is presented with two texts and asked to choose which one it authored. However, performance deteriorates sharply under the Individual Presentation Paradigm (IPP), where the model is given a single text to judge authorship. Although this phenomenon has been observed, its underlying causes have not been systematically analyzed. In this paper, we first replicate existing findings to confirm that LLMs struggle to distinguish self- from other-generated text under IPP. We then investigate the reasons for this failure and attribute it to a phenomenon we term Implicit Territorial Awareness (ITA)-the model's latent ability to distinguish self- and other-texts in representational space, which remains unexpressed in its output behavior. To awaken the ITA of LLMs, we propose Cognitive Surgery (CoSur), a novel framework comprising four main modules: representation extraction, territory construction, authorship discrimination and cognitive editing. Experimental results demonstrate that our proposed method improves the performance of three different LLMs in the IPP scenario, achieving average accuracies of 83.25%, 66.19%, and 88.01%, respectively.",
    "label": "human"
  },
  {
    "text": "Users often take notes for instructional videos to access key knowledge later without revisiting long videos. Automated note generation tools enable users to obtain informative notes efficiently. However, notes generated by existing research or off-the-shelf tools fail to preserve the information conveyed in the original videos comprehensively, nor can they satisfy users' expectations for diverse presentation formats and interactive features when using notes digitally. In this work, we present NoteIt, a system, which automatically converts instructional videos to interactable notes using a novel pipeline that faithfully extracts hierarchical structure and multimodal key information from videos. With NoteIt's interface, users can interact with the system to further customize the content and presentation formats of the notes according to their preferences. We conducted both a technical evaluation and a comparison user study (N=36). The solid performance in objective metrics and the positive user feedback demonstrated the effectiveness of the pipeline and the overall usability of NoteIt. Project website: https://zhaorunning.github.io/NoteIt/",
    "label": "human"
  },
  {
    "text": "Relation extraction enables the construction of structured knowledge for many downstream applications. While large language models (LLMs) have shown great promise in this domain, most existing methods concentrate on relation classification, which predicts the semantic relation type between a related entity pair. However, we observe that LLMs often struggle to reliably determine whether a relation exists, especially in cases involving complex sentence structures or intricate semantics, which leads to spurious predictions. Such hallucinations can introduce noisy edges in knowledge graphs, compromising the integrity of structured knowledge and downstream reliability. To address these challenges, we propose DEPTH, a framework that integrates Dependency-aware sEntence simPlification and Two-tiered Hierarchical refinement into the relation extraction pipeline. Given a sentence and its candidate entity pairs, DEPTH operates in two stages: (1) the Grounding module extracts relations for each pair by leveraging their shortest dependency path, distilling the sentence into a minimal yet coherent relational context that reduces syntactic noise while preserving key semantics; (2) the Refinement module aggregates all local predictions and revises them based on a holistic understanding of the sentence, correcting omissions and inconsistencies. We further introduce a causality-driven reward model that mitigates reward hacking by disentangling spurious correlations, enabling robust fine-tuning via reinforcement learning with human feedback. Experiments on six benchmarks demonstrate that DEPTH reduces the average hallucination rate to 7.0\\% while achieving a 17.2\\% improvement in average F1 score over state-of-the-art baselines.",
    "label": "human"
  },
  {
    "text": "As Large Language Models (LLMs) are increasingly deployed in decision-critical domains, it becomes essential to ensure that their confidence estimates faithfully correspond to their actual correctness. Existing calibration methods have primarily focused on post-hoc adjustments or auxiliary model training; however, many of these approaches necessitate additional supervision or parameter updates. In this work, we propose a novel prompt-based calibration framework inspired by the Credence Calibration Game. Our method establishes a structured interaction loop wherein LLMs receive feedback based on the alignment of their predicted confidence with correctness. Through feedback-driven prompting and natural language summaries of prior performance, our framework dynamically improves model calibration. Extensive experiments across models and game configurations demonstrate consistent improvements in evaluation metrics. Our results highlight the potential of game-based prompting as an effective strategy for LLM calibration. Code and data are available at https://anonymous.4open.science/r/LLM-Calibration/.",
    "label": "human"
  },
  {
    "text": "Effective responses to cyberattacks require fast decisions, even when information about the attack is incomplete or inaccurate. However, most decision-support frameworks for incident response rely on a detailed system model that describes the incident, which restricts their practical utility. In this paper, we address this limitation and present an online method for incident response planning under model misspecification, which we call MOBAL: Misspecified Online Bayesian Learning. MOBAL iteratively refines a conjecture about the model through Bayesian learning as new information becomes available, which facilitates model adaptation as the incident unfolds. To determine effective responses online, we quantize the conjectured model into a finite Markov model, which enables efficient response planning through dynamic programming. We prove that Bayesian learning is asymptotically consistent with respect to the information feedback. Additionally, we establish bounds on misspecification and quantization errors. Experiments on the CAGE-2 benchmark show that MOBAL outperforms the state of the art in terms of adaptability and robustness to model misspecification.",
    "label": "human"
  },
  {
    "text": "We propose injecting notions of fairness into multi-robot motion planning. When robots have competing interests, it is important to optimize for some kind of fairness in their usage of resources. In this work, we explore how the robots' energy expenditures might be fairly distributed among them, while maintaining mission success. We formulate a distributed fair motion planner and integrate it with safe controllers in a algorithm called FiReFly. For simulated reach-avoid missions, FiReFly produces fairer trajectories and improves mission success rates over a non-fair planner. We find that real-time performance is achievable up to 15 UAVs, and that scaling up to 50 UAVs is possible with trade-offs between runtime and fairness improvements.",
    "label": "human"
  },
  {
    "text": "Urban Air Mobility (UAM) is an emerging transportation paradigm in which Uncrewed Aerial Systems (UAS) autonomously transport passengers and goods in cities. The UAS have different operators with different, sometimes competing goals, yet must share the airspace. We propose a negotiated, semi-distributed flight planner that optimizes UAS' flight lengths {\\em in a fair manner}. Current flight planners might result in some UAS being given disproportionately shorter flight paths at the expense of others. We introduce Fair-CoPlan, a planner in which operators and a Provider of Service to the UAM (PSU) together compute \\emph{fair} flight paths. Fair-CoPlan has three steps: First, the PSU constrains take-off and landing choices for flights based on capacity at and around vertiports. Then, operators plan independently under these constraints. Finally, the PSU resolves any conflicting paths, optimizing for path length fairness. By fairly spreading the cost of deconfliction Fair-CoPlan encourages wider participation in UAM, ensures safety of the airspace and the areas below it, and promotes greater operator flexibility. We demonstrate Fair-CoPlan through simulation experiments and find fairer outcomes than a non-fair planner with minor delays as a trade-off.",
    "label": "human"
  },
  {
    "text": "Large language models (LLMs) have demonstrated potential in educational applications, yet their capacity to accurately assess the cognitive alignment of reading materials with students' developmental stages remains insufficiently explored. This gap is particularly critical given the foundational educational principle of the Zone of Proximal Development (ZPD), which emphasizes the need to match learning resources with Students' Cognitive Abilities (SCA). Despite the importance of this alignment, there is a notable absence of comprehensive studies investigating LLMs' ability to evaluate reading comprehension difficulty across different student age groups, especially in the context of Chinese language education. To fill this gap, we introduce ZPD-SCA, a novel benchmark specifically designed to assess stage-level Chinese reading comprehension difficulty. The benchmark is annotated by 60 Special Grade teachers, a group that represents the top 0.15% of all in-service teachers nationwide. Experimental results reveal that LLMs perform poorly in zero-shot learning scenarios, with Qwen-max and GLM even falling below the probability of random guessing. When provided with in-context examples, LLMs performance improves substantially, with some models achieving nearly double the accuracy of their zero-shot baselines. These results reveal that LLMs possess emerging abilities to assess reading difficulty, while also exposing limitations in their current training for educationally aligned judgment. Notably, even the best-performing models display systematic directional biases, suggesting difficulties in accurately aligning material difficulty with SCA. Furthermore, significant variations in model performance across different genres underscore the complexity of task. We envision that ZPD-SCA can provide a foundation for evaluating and improving LLMs in cognitively aligned educational applications.",
    "label": "human"
  },
  {
    "text": "Computing-In-Memory (CIM) offers a potential solution to the memory wall issue and can achieve high energy efficiency by minimizing data movement, making it a promising architecture for edge AI devices. Lightweight models like MobileNet and EfficientNet, which utilize depthwise convolution for feature extraction, have been developed for these devices. However, CIM macros often face challenges in accelerating depthwise convolution, including underutilization of CIM memory and heavy buffer traffic. The latter, in particular, has been overlooked despite its significant impact on latency and energy consumption. To address this, we introduce a novel CIM dataflow that significantly reduces buffer traffic by maximizing data reuse and improving memory utilization during depthwise convolution. The proposed dataflow is grounded in solid theoretical principles, fully demonstrated in this paper. When applied to MobileNet and EfficientNet models, our dataflow reduces buffer traffic by 77.4-87.0%, leading to a total reduction in data traffic energy and latency by 10.1-17.9% and 15.6-27.8%, respectively, compared to the baseline (conventional weight-stationary dataflow).",
    "label": "human"
  },
  {
    "text": "Category-level object pose estimation aims to predict the 6D pose and 3D size of objects within given categories. Existing approaches for this task rely solely on 6D poses as supervisory signals without explicitly capturing the intrinsic continuity of poses, leading to inconsistencies in predictions and reduced generalization to unseen poses. To address this limitation, we propose HRC-Pose, a novel depth-only framework for category-level object pose estimation, which leverages contrastive learning to learn point cloud representations that preserve the continuity of 6D poses. HRC-Pose decouples object pose into rotation and translation components, which are separately encoded and leveraged throughout the network. Specifically, we introduce a contrastive learning strategy for multi-task, multi-category scenarios based on our 6D pose-aware hierarchical ranking scheme, which contrasts point clouds from multiple categories by considering rotational and translational differences as well as categorical information. We further design pose estimation modules that separately process the learned rotation-aware and translation-aware embeddings. Our experiments demonstrate that HRC-Pose successfully learns continuous feature spaces. Results on REAL275 and CAMERA25 benchmarks show that our method consistently outperforms existing depth-only state-of-the-art methods and runs in real-time, demonstrating its effectiveness and potential for real-world applications. Our code is at https://github.com/zhujunli1993/HRC-Pose.",
    "label": "human"
  },
  {
    "text": "Recent advances in large language models (LLMs) have enabled new possibilities in simulating complex physiological systems. We introduce Organ-Agents, a multi-agent framework that simulates human physiology via LLM-driven agents. Each Simulator models a specific system (e.g., cardiovascular, renal, immune). Training consists of supervised fine-tuning on system-specific time-series data, followed by reinforcement-guided coordination using dynamic reference selection and error correction. We curated data from 7,134 sepsis patients and 7,895 controls, generating high-resolution trajectories across 9 systems and 125 variables. Organ-Agents achieved high simulation accuracy on 4,509 held-out patients, with per-system MSEs <0.16 and robustness across SOFA-based severity strata. External validation on 22,689 ICU patients from two hospitals showed moderate degradation under distribution shifts with stable simulation. Organ-Agents faithfully reproduces critical multi-system events (e.g., hypotension, hyperlactatemia, hypoxemia) with coherent timing and phase progression. Evaluation by 15 critical care physicians confirmed realism and physiological plausibility (mean Likert ratings 3.9 and 3.7). Organ-Agents also enables counterfactual simulations under alternative sepsis treatment strategies, generating trajectories and APACHE II scores aligned with matched real-world patients. In downstream early warning tasks, classifiers trained on synthetic data showed minimal AUROC drops (<0.04), indicating preserved decision-relevant patterns. These results position Organ-Agents as a credible, interpretable, and generalizable digital twin for precision diagnosis, treatment simulation, and hypothesis testing in critical care.",
    "label": "human"
  },
  {
    "text": "In one-stage multi-object detection tasks, various intersection over union (IoU)-based solutions aim at smooth and stable convergence near the targets during training. However, IoU-based losses fail to correctly update the gradient of small objects due to an extremely flat gradient. During the update of multiple objects, the learning of small objects' gradients suffers more because of insufficient gradient updates. Therefore, we propose an inter-class relational loss to efficiently update the gradient of small objects while not sacrificing the learning efficiency of other objects based on the simple fact that an object has a spatial relationship to another object (e.g., a car plate is attached to a car in a similar position). When the predicted car plate's bounding box is not within its car, a loss punishment is added to guide the learning, which is inversely proportional to the overlapped area of the car's and predicted car plate's bounding box. By leveraging the spatial relationship at the inter-class level, the loss guides small object predictions using larger objects and enhances latent information in deeper feature maps. In this paper, we present twofold contributions using license plate detection as a case study: (1) a new small vehicle multi-license plate dataset (SVMLP), featuring diverse real-world scenarios with high-quality annotations; and (2) a novel inter-class relational loss function designed to promote effective detection performance. We highlight the proposed ICR loss penalty can be easily added to existing IoU-based losses and enhance the performance. These contributions improve the standard mean Average Precision (mAP) metric, achieving gains of 10.3% and 1.6% in mAP$^{\\text{test}}_{50}$ for YOLOv12-T and UAV-DETR, respectively, without any additional hyperparameter tuning. Code and dataset will be available soon.",
    "label": "human"
  },
  {
    "text": "Poaching poses significant threats to wildlife and biodiversity. A valuable step in reducing poaching is to forecast poacher behavior, which can inform patrol planning and other conservation interventions. Existing poaching prediction methods based on linear models or decision trees lack the expressivity to capture complex, nonlinear spatiotemporal patterns. Recent advances in generative modeling, particularly flow matching, offer a more flexible alternative. However, training such models on real-world poaching data faces two central obstacles: imperfect detection of poaching events and limited data. To address imperfect detection, we integrate flow matching with an occupancy-based detection model and train the flow in latent space to infer the underlying occupancy state. To mitigate data scarcity, we adopt a composite flow initialized from a linear-model prediction rather than random noise which is the standard in diffusion models, injecting prior knowledge and improving generalization. Evaluations on datasets from two national parks in Uganda show consistent gains in predictive accuracy.",
    "label": "human"
  },
  {
    "text": "Autonomous Cyber Operations (ACO) rely on Reinforcement Learning (RL) to train agents to make effective decisions in the cybersecurity domain. However, existing ACO applications require agents to learn from scratch, leading to slow convergence and poor early-stage performance. While teacher-guided techniques have demonstrated promise in other domains, they have not yet been applied to ACO. In this study, we implement four distinct teacher-guided techniques in the simulated CybORG environment and conduct a comparative evaluation. Our results demonstrate that teacher integration can significantly improve training efficiency in terms of early policy performance and convergence speed, highlighting its potential benefits for autonomous cybersecurity.",
    "label": "human"
  },
  {
    "text": "Large Artificial Intelligence (AI) training workloads spanning several tens of thousands of GPUs present unique power management challenges. These arise due to the high variability in power consumption during the training. Given the synchronous nature of these jobs, during every iteration there is a computation-heavy phase, where each GPU works on the local data, and a communication-heavy phase where all the GPUs synchronize on the data. Because compute-heavy phases require much more power than communication phases, large power swings occur. The amplitude of these power swings is ever increasing with the increase in the size of training jobs. An even bigger challenge arises from the frequency spectrum of these power swings which, if harmonized with critical frequencies of utilities, can cause physical damage to the power grid infrastructure. Therefore, to continue scaling AI training workloads safely, we need to stabilize the power of such workloads. This paper introduces the challenge with production data and explores innovative solutions across the stack: software, GPU hardware, and datacenter infrastructure. We present the pros and cons of each of these approaches and finally present a multi-pronged approach to solving the challenge. The proposed solutions are rigorously tested using a combination of real hardware and Microsoft's in-house cloud power simulator, providing critical insights into the efficacy of these interventions under real-world conditions.",
    "label": "human"
  },
  {
    "text": "Large language models (LLMs) have demonstrated impressive capabilities across diverse tasks, but they remain susceptible to hallucinations--generating content that appears plausible but contains factual inaccuracies. We present Finch-Zk, a black-box framework that leverages FINe-grained Cross-model consistency to detect and mitigate Hallucinations in LLM outputs without requiring external knowledge sources. Finch-Zk introduces two key innovations: 1) a cross-model consistency checking strategy that reveals fine-grained inaccuracies by comparing responses generated by diverse models from semantically-equivalent prompts, and 2) a targeted mitigation technique that applies precise corrections to problematic segments while preserving accurate content. Experiments on the FELM dataset show Finch-Zk improves hallucination detection F1 scores by 6-39\\% compared to existing approaches. For mitigation, Finch-Zk achieves 7-8 absolute percentage points improvement in answer accuracy on the GPQA-diamond dataset when applied to state-of-the-art models like Llama 4 Maverick and Claude 4 Sonnet. Extensive evaluation across multiple models demonstrates that Finch-Zk provides a practical, deployment-ready safeguard for enhancing factual reliability in production LLM systems.",
    "label": "human"
  },
  {
    "text": "Test-time scaling (TTS) for large language models (LLMs) has thus far fallen into two largely separate paradigms: (1) reinforcement learning (RL) methods that optimize sparse outcome-based rewards, yet suffer from instability and low sample efficiency; and (2) search-based techniques guided by independently trained, static process reward models (PRMs), which require expensive human- or LLM-generated labels and often degrade under distribution shifts. In this paper, we introduce AIRL-S, the first natural unification of RL-based and search-based TTS. Central to AIRL-S is the insight that the reward function learned during RL training inherently represents the ideal PRM for guiding downstream search. Specifically, we leverage adversarial inverse reinforcement learning (AIRL) combined with group relative policy optimization (GRPO) to learn a dense, dynamic PRM directly from correct reasoning traces, entirely eliminating the need for labeled intermediate process data. At inference, the resulting PRM simultaneously serves as the critic for RL rollouts and as a heuristic to effectively guide search procedures, facilitating robust reasoning chain extension, mitigating reward hacking, and enhancing cross-task generalization. Experimental results across eight benchmarks, including mathematics, scientific reasoning, and code generation, demonstrate that our unified approach improves performance by 9 % on average over the base model, matching GPT-4o. Furthermore, when integrated into multiple search algorithms, our PRM consistently outperforms all baseline PRMs trained with labeled data. These results underscore that, indeed, your reward function for RL is your best PRM for search, providing a robust and cost-effective solution to complex reasoning tasks in LLMs.",
    "label": "human"
  },
  {
    "text": "There is an increasing appreciation that one may need to consider multiple measures of fairness, e.g., considering multiple group and individual fairness notions. The relative weights of the fairness regularisers are a priori unknown, may be time varying, and need to be learned on the fly. We consider the learning of time-varying convexifications of multiple fairness measures with limited graph-structured feedback.",
    "label": "human"
  },
  {
    "text": "Deploying Large Language Models (LLMs) on edge hardware demands aggressive, prompt-aware dynamic pruning to reduce computation without degrading quality. Static or predictor-based schemes either lock in a single sparsity pattern or incur extra runtime overhead, and recent zero-shot methods that rely on statistics from a single prompt fail on short prompt and/or long generation scenarios. We introduce A/I-GLASS: Activation- and Impact-based Global-Local neural importance Aggregation for feed-forward network SparSification, two training-free methods that dynamically select FFN units using a rank-aggregation of prompt local and model-intrinsic global neuron statistics. Empirical results across multiple LLMs and benchmarks demonstrate that GLASS significantly outperforms prior training-free methods, particularly in challenging long-form generation scenarios, without relying on auxiliary predictors or adding any inference overhead.",
    "label": "human"
  },
  {
    "text": "Traditional protocol fuzzing techniques, such as those employed by AFL-based systems, often lack effectiveness due to a limited semantic understanding of complex protocol grammars and rigid seed mutation strategies. Recent works, such as ChatAFL, have integrated Large Language Models (LLMs) to guide protocol fuzzing and address these limitations, pushing protocol fuzzers to wider exploration of the protocol state space. But ChatAFL still faces issues like unreliable output, LLM hallucinations, and assumptions of LLM knowledge about protocol specifications. This paper introduces MultiFuzz, a novel dense retrieval-based multi-agent system designed to overcome these limitations by integrating semantic-aware context retrieval, specialized agents, and structured tool-assisted reasoning. MultiFuzz utilizes agentic chunks of protocol documentation (RFC Documents) to build embeddings in a vector database for a retrieval-augmented generation (RAG) pipeline, enabling agents to generate more reliable and structured outputs, enhancing the fuzzer in mutating protocol messages with enhanced state coverage and adherence to syntactic constraints. The framework decomposes the fuzzing process into modular groups of agents that collaborate through chain-of-thought reasoning to dynamically adapt fuzzing strategies based on the retrieved contextual knowledge. Experimental evaluations on the Real-Time Streaming Protocol (RTSP) demonstrate that MultiFuzz significantly improves branch coverage and explores deeper protocol states and transitions over state-of-the-art (SOTA) fuzzers such as NSFuzz, AFLNet, and ChatAFL. By combining dense retrieval, agentic coordination, and language model reasoning, MultiFuzz establishes a new paradigm in autonomous protocol fuzzing, offering a scalable and extensible foundation for future research in intelligent agentic-based fuzzing systems.",
    "label": "human"
  },
  {
    "text": "We introduce Pixels2Play-0.1 (P2P0.1), a foundation model that learns to play a wide range of 3D video games with recognizable human-like behavior. Motivated by emerging consumer and developer use cases - AI teammates, controllable NPCs, personalized live-streamers, assistive testers - we argue that an agent must rely on the same pixel stream available to players and generalize to new titles with minimal game-specific engineering. P2P0.1 is trained end-to-end with behavior cloning: labeled demonstrations collected from instrumented human game-play are complemented by unlabeled public videos, to which we impute actions via an inverse-dynamics model. A decoder-only transformer with auto-regressive action output handles the large action space while remaining latency-friendly on a single consumer GPU. We report qualitative results showing competent play across simple Roblox and classic MS-DOS titles, ablations on unlabeled data, and outline the scaling and evaluation steps required to reach expert-level, text-conditioned control.",
    "label": "human"
  },
  {
    "text": "We propose a neurosymbolic approach to the explanation of complex sequences of decisions that combines the strengths of decision procedures and Large Language Models (LLMs). We demonstrate this approach by producing explanations for the solutions of Hitori puzzles. The rules of Hitori include local constraints that are effectively explained by short resolution proofs. However, they also include a connectivity constraint that is more suitable for visual explanations. Hence, Hitori provides an excellent testing ground for a flexible combination of SAT solvers and LLMs. We have implemented a tool that assists humans in solving Hitori puzzles, and we present experimental evidence of its effectiveness.",
    "label": "human"
  },
  {
    "text": "Accurate detection of vascular occlusions during endovascular thrombectomy (EVT) is critical in acute ischemic stroke (AIS). Interpretation of digital subtraction angiography (DSA) sequences poses challenges due to anatomical complexity and time constraints. This work proposes OccluNet, a spatio-temporal deep learning model that integrates YOLOX, a single-stage object detector, with transformer-based temporal attention mechanisms to automate occlusion detection in DSA sequences. We compared OccluNet with a YOLOv11 baseline trained on either individual DSA frames or minimum intensity projections. Two spatio-temporal variants were explored for OccluNet: pure temporal attention and divided space-time attention. Evaluation on DSA images from the MR CLEAN Registry revealed the model's capability to capture temporally consistent features, achieving precision and recall of 89.02% and 74.87%, respectively. OccluNet significantly outperformed the baseline models, and both attention variants attained similar performance. Source code is available at https://github.com/anushka-kore/OccluNet.git",
    "label": "human"
  },
  {
    "text": "Fine-tuning large language models (LLMs) with low-rank adaptaion (LoRA) is a cost-effective way to incorporate information from a specific dataset. However, it is often unclear how well the fine-tuned LLM will generalize, i.e., how well it will perform on unseen datasets. Methods have been proposed to improve generalization by optimizing with in-context prompts, or by using meta-learning to fine-tune LLMs. However, these methods are expensive in memory and computation, requiring either long-context prompts or saving copies of parameters and using second-order gradient updates. To address these challenges, we propose Amortized Bayesian Meta-Learning for LoRA (ABMLL). This method builds on amortized Bayesian meta-learning for smaller models, adapting this approach to LLMs while maintaining its computational efficiency. We reframe task-specific and global parameters in the context of LoRA and use a set of new hyperparameters to balance reconstruction accuracy and the fidelity of task-specific parameters to the global ones. ABMLL provides effective generalization and scales to large models such as Llama3-8B. Furthermore, as a result of using a Bayesian framework, ABMLL provides improved uncertainty quantification. We test ABMLL on Unified-QA and CrossFit datasets and find that it outperforms existing methods on these benchmarks in terms of both accuracy and expected calibration error.",
    "label": "human"
  },
  {
    "text": "Despite the growing importance of dental CBCT scans for diagnosis and treatment planning, generating anatomically realistic scans with fine-grained control remains a challenge in medical image synthesis. In this work, we propose a novel conditional diffusion framework for 3D dental volume generation, guided by tooth-level binary attributes that allow precise control over tooth presence and configuration. Our approach integrates wavelet-based denoising diffusion, FiLM conditioning, and masked loss functions to focus learning on relevant anatomical structures. We evaluate the model across diverse tasks, such as tooth addition, removal, and full dentition synthesis, using both paired and distributional similarity metrics. Results show strong fidelity and generalization with low FID scores, robust inpainting performance, and SSIM values above 0.91 even on unseen scans. By enabling realistic, localized modification of dentition without rescanning, this work opens opportunities for surgical planning, patient communication, and targeted data augmentation in dental AI workflows. The codes are available at: https://github.com/djafar1/tooth-diffusion.",
    "label": "human"
  },
  {
    "text": "Connecting LLMs with formal knowledge representation and reasoning is a promising approach to address their shortcomings. Embeddings and sparse autoencoders are widely used to represent textual content, but the semantics are entangled with syntactic and language-specific information. We propose a method that isolates concept semantics in Large Langue Models by averaging concept activations derived via Sparse Autoencoders. We create English text representations from OWL ontology classes, translate the English into French and Chinese and then pass these texts as prompts to the Gemma 2B LLM. Using the open source Gemma Scope suite of Sparse Autoencoders, we obtain concept activations for each class and language version. We average the different language activations to derive a conceptual average. We then correlate the conceptual averages with a ground truth mapping between ontology classes. Our results give a strong indication that the conceptual average aligns to the true relationship between classes when compared with a single language by itself. The result hints at a new technique which enables mechanistic interpretation of internal network states with higher accuracy.",
    "label": "human"
  },
  {
    "text": "We present an efficient Angluin-style learning algorithm for weak deterministic B\\\"uchi automata (wDBAs). Different to ordinary deterministic B\\\"uchi and co-B\\\"uchi automata, wDBAs have a minimal normal form, and we show that we can learn this minimal normal form efficiently. We provide an improved result on the number of queries required and show on benchmarks that this theoretical advantage translates into significantly fewer queries: while previous approaches require a quintic number of queries, we only require quadratically many queries in the size of the canonic wDBA that recognises the target language.",
    "label": "human"
  },
  {
    "text": "As researchers increasingly adopt LLMs as writing assistants, generating high-quality research paper introductions remains both challenging and essential. We introduce Scientific Introduction Generation (SciIG), a task that evaluates LLMs' ability to produce coherent introductions from titles, abstracts, and related works. Curating new datasets from NAACL 2025 and ICLR 2025 papers, we assess five state-of-the-art models, including both open-source (DeepSeek-v3, Gemma-3-12B, LLaMA 4-Maverick, MistralAI Small 3.1) and closed-source GPT-4o systems, across multiple dimensions: lexical overlap, semantic similarity, content coverage, faithfulness, consistency, citation correctness, and narrative quality. Our comprehensive framework combines automated metrics with LLM-as-a-judge evaluations. Results demonstrate LLaMA-4 Maverick's superior performance on most metrics, particularly in semantic similarity and faithfulness. Moreover, three-shot prompting consistently outperforms fewer-shot approaches. These findings provide practical insights into developing effective research writing assistants and set realistic expectations for LLM-assisted academic writing. To foster reproducibility and future research, we will publicly release all code and datasets.",
    "label": "human"
  },
  {
    "text": "The clinical deployment of deep learning models for high-stakes tasks such as diabetic retinopathy (DR) grading requires demonstrable reliability. While models achieve high accuracy, their clinical utility is limited by a lack of robust uncertainty quantification. Conformal prediction (CP) offers a distribution-free framework to generate prediction sets with statistical guarantees of coverage. However, the interaction between standard training practices like data augmentation and the validity of these guarantees is not well understood. In this study, we systematically investigate how different data augmentation strategies affect the performance of conformal predictors for DR grading. Using the DDR dataset, we evaluate two backbone architectures -- ResNet-50 and a Co-Scale Conv-Attentional Transformer (CoaT) -- trained under five augmentation regimes: no augmentation, standard geometric transforms, CLAHE, Mixup, and CutMix. We analyze the downstream effects on conformal metrics, including empirical coverage, average prediction set size, and correct efficiency. Our results demonstrate that sample-mixing strategies like Mixup and CutMix not only improve predictive accuracy but also yield more reliable and efficient uncertainty estimates. Conversely, methods like CLAHE can negatively impact model certainty. These findings highlight the need to co-design augmentation strategies with downstream uncertainty quantification in mind to build genuinely trustworthy AI systems for medical imaging.",
    "label": "human"
  },
  {
    "text": "Magnetic domain wall motion has recently garnered significant interest as a physical mechanism to enable energy-efficient, next-generation brain-inspired computing architectures. However, realizing all behaviors required for neuromorphic computing within standard material systems remains a significant challenge, as these functionalities often rely on competing interactions. Here, we demonstrate how spontaneous domain wall motion in response to locally engineered lateral exchange coupling in transition metal-rare earth ferrimagnets can be leveraged to achieve numerous neuromorphic computing functionalities in devices with minimal complexity. Through experiments and micromagnetic simulations, we show how tuning the feature size, material composition, and chiral interaction strength controls the speed of self-driven domain wall motion. When integrated with spin-orbit torque, this control gives rise to behaviors essential for neuromorphic computing, including leaky integration and passive resetting of artificial neuron potential. These results establish locally engineered ferrimagnets as a tunable, scalable, and straightforward platform for domain wall-based computing architectures.",
    "label": "human"
  },
  {
    "text": "This paper presents a novel robust predictive controller for constrained nonlinear systems that is able to track piece-wise constant setpoint signals. The tracking model predictive controller presented in this paper extends the nonlinear MPC for tracking to the more complex case of nonlinear systems subject to bounded and not necessarily additive perturbations. The optimal control problem that is solved at each step penalizes the deviation of the predicted nominal system trajectory from an artificial reference, which is added as a decision variable, as well as the distance between the artificial reference and the setpoint. Robust feasibility is ensured by imposing conservative constraints that take into account the effect of uncertainties and convergence to a neighborhood of any feasible setpoint is guaranteed by means of an appropriate terminal cost and an extended stabilizing terminal constraint. In the case of unreachable setpoints, convergence to a neighborhood of the optimal reachable steady output is also proved.",
    "label": "human"
  },
  {
    "text": "Multilevel selection occurs when short-term individual-level reproductive interests conflict with longer-term group-level fitness effects. Detecting and quantifying this phenomenon is key to understanding evolution of traits ranging from multicellularity to pathogen virulence. Multilevel selection is particularly important in artificial life research due to its connection to major evolutionary transitions, a hallmark of open-ended evolution. Bonetti Franceschi & Volz (2024) proposed to detect multilevel selection dynamics by screening for mutations that appear more often in a population than expected by chance (due to individual-level fitness benefits) but are ultimately associated with negative longer-term fitness outcomes (i.e., smaller, shorter-lived descendant clades). Here, we use agent-based modeling with known ground truth to assess the efficacy of this approach. To test these methods under challenging conditions broadly comparable to the original dataset explored by Bonetti Franceschi & Volz (2024), we use an epidemiological framework to model multilevel selection in trade-offs between within-host growth rate and between-host transmissibility. To achieve success on our in silico data, we develop an alternate normalization procedure for identifying clade-level fitness effects. We find the method to be sensitive in detecting genome sites under multilevel selection with 30% effect sizes on fitness, but do not see sensitivity to smaller 10% mutation effect sizes. To test the robustness of this methodology, we conduct additional experiments incorporating extrinsic, time-varying environmental changes and adaptive turnover in population compositions, and find that screen performance remains generally consistent with baseline conditions. This work represents a promising step towards rigorous generalizable quantification of multilevel selection effects.",
    "label": "human"
  },
  {
    "text": "As AI agents become more widely deployed, we are likely to see an increasing number of incidents: events involving AI agent use that directly or indirectly cause harm. For example, agents could be prompt-injected to exfiltrate private information or make unauthorized purchases. Structured information about such incidents (e.g., user prompts) can help us understand their causes and prevent future occurrences. However, existing incident reporting processes are not sufficient for understanding agent incidents. In particular, such processes are largely based on publicly available data, which excludes useful, but potentially sensitive, information such as an agent's chain of thought or browser history. To inform the development of new, emerging incident reporting processes, we propose an incident analysis framework for agents. Drawing on systems safety approaches, our framework proposes three types of factors that can cause incidents: system-related (e.g., CBRN training data), contextual (e.g., prompt injections), and cognitive (e.g., misunderstanding a user request). We also identify specific information that could help clarify which factors are relevant to a given incident: activity logs, system documentation and access, and information about the tools an agent uses. We provide recommendations for 1) what information incident reports should include and 2) what information developers and deployers should retain and make available to incident investigators upon request. As we transition to a world with more agents, understanding agent incidents will become increasingly crucial for managing risks.",
    "label": "human"
  },
  {
    "text": "Objective: This study aims to uncover the opaque decision-making process of an artificial intelligence (AI) agent for automatic treatment planning.   Approach: We examined a previously developed AI agent based on the Actor-Critic with Experience Replay (ACER) network, which automatically tunes treatment planning parameters (TPPs) for inverse planning in prostate cancer intensity modulated radiotherapy. We selected multiple checkpoint ACER agents from different stages of training and applied an explainable AI (EXAI) method to analyze the attribution from dose-volume histogram (DVH) inputs to TPP-tuning decisions. We then assessed each agent's planning efficacy and efficiency and evaluated their policy and final TPP tuning spaces. Combining these analyses, we systematically examined how ACER agents generated high-quality treatment plans in response to different DVH inputs.   Results: Attribution analysis revealed that ACER agents progressively learned to identify dose-violation regions from DVH inputs and promote appropriate TPP-tuning actions to mitigate them. Organ-wise similarities between DVH attributions and dose-violation reductions ranged from 0.25 to 0.5 across tested agents. Agents with stronger attribution-violation similarity required fewer tuning steps (~12-13 vs. 22), exhibited a more concentrated TPP-tuning space with lower entropy (~0.3 vs. 0.6), converged on adjusting only a few TPPs, and showed smaller discrepancies between practical and theoretical tuning steps. Putting together, these findings indicate that high-performing ACER agents can effectively identify dose violations from DVH inputs and employ a global tuning strategy to achieve high-quality treatment planning, much like skilled human planners.   Significance: Better interpretability of the agent's decision-making process may enhance clinician trust and inspire new strategies for automatic treatment planning.",
    "label": "human"
  },
  {
    "text": "This paper proposes a hybrid infrastructure-to-vehicle (I2V) communication framework to support future 6G-enabled intelligent transportation systems (ITS) in smart cities. Leveraging existing LED streetlighting infrastructure, the system simultaneously delivers energy-efficient illumination and high-speed wireless connectivity. The proposed scheme integrates visible light communication (VLC) with a complementary ter-ahertz (THz) antenna array to overcome VLC limitations under high ambient light and adverse weather conditions. Key con-tributions include the design of a VLC/THz access network, seamless integration with lighting infrastructure, a proposed switching-combination (PSC) mechanism, and a physical layout optimization strategy. Using a grid search method, thousands of configurations were evaluated to maximize lighting coverage, re-ceived power, signal-to-noise ratio (SNR), signal-to-interference-and-noise ratio (SINR), and minimize outage probability. Results show that optimized lighting coverage improves from 35% to 97%, while hybrid communication coverage increases from 49%to 99.9% at the same power level. Under extreme environmental conditions, the hybrid system maintains up to 99% coverage, compared to 69% with VLC alone. These results demonstrate the scalability, cost-efficiency, and practicality of the proposed system for next-generation ITS deployment.",
    "label": "human"
  },
  {
    "text": "Emotions exert an immense influence over human behavior and cognition in both commonplace and high-stress tasks. Discussions of whether or how to integrate large language models (LLMs) into everyday life (e.g., acting as proxies for, or interacting with, human agents), should be informed by an understanding of how these tools evaluate emotionally loaded stimuli or situations. A model's alignment with human behavior in these cases can inform the effectiveness of LLMs for certain roles or interactions. To help build this understanding, we elicited ratings from multiple popular LLMs for datasets of words and images that were previously rated for their emotional content by humans. We found that when performing the same rating tasks, GPT-4o responded very similarly to human participants across modalities, stimuli and most rating scales (r = 0.9 or higher in many cases). However, arousal ratings were less well aligned between human and LLM raters, while happiness ratings were most highly aligned. Overall LLMs aligned better within a five-category (happiness, anger, sadness, fear, disgust) emotion framework than within a two-dimensional (arousal and valence) organization. Finally, LLM ratings were substantially more homogenous than human ratings. Together these results begin to describe how LLM agents interpret emotional stimuli and highlight similarities and differences among biological and artificial intelligence in key behavioral domains.",
    "label": "human"
  },
  {
    "text": "Video Anomaly Detection (VAD) has emerged as a pivotal task in computer vision, with broad relevance across multiple fields. Recent advances in deep learning have driven significant progress in this area, yet the field remains fragmented across domains and learning paradigms. This survey offers a comprehensive perspective on VAD, systematically organizing the literature across various supervision levels, as well as adaptive learning methods such as online, active, and continual learning. We examine the state of VAD across three major application categories: human-centric, vehicle-centric, and environment-centric scenarios, each with distinct challenges and design considerations. In doing so, we identify fundamental contributions and limitations of current methodologies. By consolidating insights from subfields, we aim to provide the community with a structured foundation for advancing both theoretical understanding and real-world applicability of VAD systems. This survey aims to support researchers by providing a useful reference, while also drawing attention to the broader set of open challenges in anomaly detection, including both fundamental research questions and practical obstacles to real-world deployment.",
    "label": "human"
  },
  {
    "text": "This contribution addresses vessel trajectory prediction (VTP), focusing on the evaluation of different deep learning-based approaches. The objective is to assess model performance in diverse traffic complexities and compare the reliability of the approaches. While previous VTP models overlook the specific traffic situation complexity and lack reliability assessments, this research uses a probability of detection analysis to quantify model reliability in varying traffic scenarios, thus going beyond common error distribution analyses. All models are evaluated on test samples categorized according to their traffic situation during the prediction horizon, with performance metrics and reliability estimates obtained for each category. The results of this comprehensive evaluation provide a deeper understanding of the strengths and weaknesses of the different prediction approaches, along with their reliability in terms of the prediction horizon lengths for which safe forecasts can be guaranteed. These findings can inform the development of more reliable vessel trajectory prediction approaches, enhancing safety and efficiency in future inland waterways navigation.",
    "label": "human"
  },
  {
    "text": "We introduce RynnEC, a video multimodal large language model designed for embodied cognition. Built upon a general-purpose vision-language foundation model, RynnEC incorporates a region encoder and a mask decoder, enabling flexible region-level video interaction. Despite its compact architecture, RynnEC achieves state-of-the-art performance in object property understanding, object segmentation, and spatial reasoning. Conceptually, it offers a region-centric video paradigm for the brain of embodied agents, providing fine-grained perception of the physical world and enabling more precise interactions. To mitigate the scarcity of annotated 3D datasets, we propose an egocentric video based pipeline for generating embodied cognition data. Furthermore, we introduce RynnEC-Bench, a region-centered benchmark for evaluating embodied cognitive capabilities. We anticipate that RynnEC will advance the development of general-purpose cognitive cores for embodied agents and facilitate generalization across diverse embodied tasks. The code, model checkpoints, and benchmark are available at: https://github.com/alibaba-damo-academy/RynnEC",
    "label": "human"
  },
  {
    "text": "Text-prompted image segmentation enables fine-grained visual understanding and is critical for applications such as human-computer interaction and robotics. However, existing supervised fine-tuning methods typically ignore explicit chain-of-thought (CoT) reasoning at test time, which limits their ability to generalize to unseen prompts and domains. To address this issue, we introduce LENS, a scalable reinforcement-learning framework that jointly optimizes the reasoning process and segmentation in an end-to-end manner. We propose unified reinforcement-learning rewards that span sentence-, box-, and segment-level cues, encouraging the model to generate informative CoT rationales while refining mask quality. Using a publicly available 3-billion-parameter vision-language model, i.e., Qwen2.5-VL-3B-Instruct, LENS achieves an average cIoU of 81.2% on the RefCOCO, RefCOCO+, and RefCOCOg benchmarks, outperforming the strong fine-tuned method, i.e., GLaMM, by up to 5.6%. These results demonstrate that RL-driven CoT reasoning serves as a robust prior for text-prompted segmentation and offers a practical path toward more generalizable Segment Anything models. Code is available at https://github.com/hustvl/LENS.",
    "label": "human"
  },
  {
    "text": "We introduce ComputerRL, a framework for autonomous desktop intelligence that enables agents to operate complex digital workspaces skillfully. ComputerRL features the API-GUI paradigm, which unifies programmatic API calls and direct GUI interaction to address the inherent mismatch between machine agents and human-centric desktop environments. Scaling end-to-end RL training is crucial for improvement and generalization across diverse desktop tasks, yet remains challenging due to environmental inefficiency and instability in extended training. To support scalable and robust training, we develop a distributed RL infrastructure capable of orchestrating thousands of parallel virtual desktop environments to accelerate large-scale online RL. Furthermore, we propose Entropulse, a training strategy that alternates reinforcement learning with supervised fine-tuning, effectively mitigating entropy collapse during extended training runs. We employ ComputerRL on open models GLM-4-9B-0414 and Qwen2.5-14B, and evaluate them on the OSWorld benchmark. The AutoGLM-OS-9B based on GLM-4-9B-0414 achieves a new state-of-the-art accuracy of 48.1%, demonstrating significant improvements for general agents in desktop automation. The algorithm and framework are adopted in building AutoGLM (Liu et al., 2024a)",
    "label": "human"
  },
  {
    "text": "Modern 3D generation methods can rapidly create shapes from sparse or single views, but their outputs often lack geometric detail due to computational constraints. We present DetailGen3D, a generative approach specifically designed to enhance these generated 3D shapes. Our key insight is to model the coarse-to-fine transformation directly through data-dependent flows in latent space, avoiding the computational overhead of large-scale 3D generative models. We introduce a token matching strategy that ensures accurate spatial correspondence during refinement, enabling local detail synthesis while preserving global structure. By carefully designing our training data to match the characteristics of synthesized coarse shapes, our method can effectively enhance shapes produced by various 3D generation and reconstruction approaches, from single-view to sparse multi-view inputs. Extensive experiments demonstrate that DetailGen3D achieves high-fidelity geometric detail synthesis while maintaining efficiency in training.",
    "label": "human"
  },
  {
    "text": "Beyond simple text generation, Large Language Models (LLMs) have evolved into agentic systems capable of planning and interacting with external tools to solve complex tasks. This evolution involves fine-tuning LLMs on agent-specific tasks to enhance their proficiency. However, safety concerns are frequently overlooked during this fine-tuning process. In this work, we show that aligned LLMs can become unintentionally misaligned, leading to a higher likelihood of executing harmful tasks and a reduced tendency to refuse them when fine-tuned to execute agentic tasks. To address these safety challenges, we propose Prefix INjection Guard (PING), a simple yet effective method that prepends automatically generated natural language prefixes to agent responses, guiding them to refuse harmful requests while preserving performance on benign tasks. Specifically, we introduce an iterative approach that alternates between (1) generating candidate prefixes and (2) selecting those that optimize both task performance and refusal behavior. Experimental results demonstrate that PING significantly enhances the safety of fine-tuned LLM agents without sacrificing their effectiveness. PING consistently outperforms existing prompting approaches across diverse benchmarks in both web navigation and code generation tasks. Our analysis of internal hidden states via linear probes reveals that prefix tokens are crucial for behavior modification, explaining the performance gains. WARNING: This paper contains contents that are unethical or offensive in nature.",
    "label": "human"
  },
  {
    "text": "Magnetic Resonance Imaging (MRI) is an essential diagnostic tool for assessing knee injuries. However, manual interpretation of MRI slices remains time-consuming and prone to inter-observer variability. This study presents a systematic evaluation of various deep learning architectures combined with explainable AI (xAI) techniques for automated region of interest (ROI) detection in knee MRI scans. We investigate both supervised and self-supervised approaches, including ResNet50, InceptionV3, Vision Transformers (ViT), and multiple U-Net variants augmented with multi-layer perceptron (MLP) classifiers. To enhance interpretability and clinical relevance, we integrate xAI methods such as Grad-CAM and Saliency Maps. Model performance is assessed using AUC for classification and PSNR/SSIM for reconstruction quality, along with qualitative ROI visualizations. Our results demonstrate that ResNet50 consistently excels in classification and ROI identification, outperforming transformer-based models under the constraints of the MRNet dataset. While hybrid U-Net + MLP approaches show potential for leveraging spatial features in reconstruction and interpretability, their classification performance remains lower. Grad-CAM consistently provided the most clinically meaningful explanations across architectures. Overall, CNN-based transfer learning emerges as the most effective approach for this dataset, while future work with larger-scale pretraining may better unlock the potential of transformer models.",
    "label": "human"
  },
  {
    "text": "Data sharing is the fuel of the galloping artificial intelligence economy, providing diverse datasets for training robust models. Trust between data providers and data consumers is widely considered one of the most important factors for enabling data sharing initiatives. Concerns about data sensitivity, privacy breaches, and misuse contribute to reluctance in sharing data across various domains. In recent years, there has been a rise in technological and algorithmic solutions to measure, capture and manage trust, trustworthiness, and reputation in what we collectively refer to as Trust and Reputation Management Systems (TRMSs). Such approaches have been developed and applied to different domains of computer science, such as autonomous vehicles, or IoT networks, but there have not been dedicated approaches to data sharing and its unique characteristics. In this survey, we examine TRMSs from a data-sharing perspective, analyzing how they assess the trustworthiness of both data and entities across different environments. We develop novel taxonomies for system designs, trust evaluation framework, and evaluation metrics for both data and entity, and we systematically analyze the applicability of existing TRMSs in data sharing. Finally, we identify open challenges and propose future research directions to enhance the explainability, comprehensiveness, and accuracy of TRMSs in large-scale data-sharing ecosystems.",
    "label": "human"
  },
  {
    "text": "Recent advances in large language models (LLMs) have significantly improved the performance of dialog systems, yet current approaches often fail to provide accurate guidance of topic due to their inability to discern user confusion in related concepts. To address this, we introduce the Ask-Good-Question (AGQ) framework, which features an improved Concept-Enhanced Item Response Theory (CEIRT) model to better identify users' knowledge levels. Our contributions include applying the CEIRT model along with LLMs to directly generate guiding questions based on the inspiring text, greatly improving information retrieval efficiency during the question & answer process. Through comparisons with other baseline methods, our approach outperforms by significantly enhencing the users' information retrieval experiences.",
    "label": "human"
  },
  {
    "text": "The longest run subsequence (LRS) problem is an NP-hard combinatorial optimization problem belonging to the class of subsequence problems from bioinformatics. In particular, the problem plays a role in genome reassembly. In this paper, we present a solution to the LRS problem using a Biased Random Key Genetic Algorithm (BRKGA). Our approach places particular focus on the computational efficiency of evaluating individuals, which involves converting vectors of gray values into valid solutions to the problem. For comparison purposes, a Max-Min Ant System is developed and implemented. This is in addition to the application of the integer linear programming solver CPLEX for solving all considered problem instances. The computation results show that the proposed BRKGA is currently a state-of-the-art technique for the LRS problem. Nevertheless, the results also show that there is room for improvement, especially in the context of input strings based on large alphabet sizes.",
    "label": "human"
  },
  {
    "text": "Due to regulations like the Right to be Forgotten, there is growing demand for removing training data and its influence from models. Since full retraining is costly, various machine unlearning methods have been proposed. In this paper, we firstly present an efficient knowledge graph (KG) unlearning algorithm. We remark that KG unlearning is nontrivial due to the distinctive structure of KG and the semantic relations between entities. Also, unlearning by estimating the influence of removed components incurs significant computational overhead when applied to large-scale knowledge graphs. To this end, we define an influence function for KG unlearning and propose to approximate the model's sensitivity without expensive computation of first-order and second-order derivatives for parameter updates. Specifically, we use Taylor expansion to estimate the parameter changes caused by data removal. Given that the first-order gradients and second-order derivatives dominate the computational load, we use the Fisher matrices and zeroth-order optimization to approximate the inverse-Hessian vector product without constructing the computational graphs. Our experimental results demonstrate that the proposed method outperforms other state-of-the-art graph unlearning baselines significantly in terms of unlearning efficiency and unlearning quality. Our code is released at https://github.com/NKUShaw/ZOWFKGIF.",
    "label": "human"
  },
  {
    "text": "Speaker de-identification aims to conceal a speaker's identity while preserving intelligibility of the underlying speech. We introduce a benchmark that quantifies residual identity leakage with three complementary error rates: equal error rate, cumulative match characteristic hit rate, and embedding-space similarity measured via canonical correlation analysis and Procrustes analysis. Evaluation results reveal that all state-of-the-art speaker de-identification systems leak identity information. The highest performing system in our evaluation performs only slightly better than random guessing, while the lowest performing system achieves a 45% hit rate within the top 50 candidates based on CMC. These findings highlight persistent privacy risks in current speaker de-identification technologies.",
    "label": "human"
  },
  {
    "text": "We introduce ResPlan, a large-scale dataset of 17,000 detailed, structurally rich, and realistic residential floor plans, created to advance spatial AI research. Each plan includes precise annotations of architectural elements (walls, doors, windows, balconies) and functional spaces (such as kitchens, bedrooms, and bathrooms). ResPlan addresses key limitations of existing datasets such as RPLAN (Wu et al., 2019) and MSD (van Engelenburg et al., 2024) by offering enhanced visual fidelity and greater structural diversity, reflecting realistic and non-idealized residential layouts. Designed as a versatile, general-purpose resource, ResPlan supports a wide range of applications including robotics, reinforcement learning, generative AI, virtual and augmented reality, simulations, and game development. Plans are provided in both geometric and graph-based formats, enabling direct integration into simulation engines and fast 3D conversion. A key contribution is an open-source pipeline for geometry cleaning, alignment, and annotation refinement. Additionally, ResPlan includes structured representations of room connectivity, supporting graph-based spatial reasoning tasks. Finally, we present comparative analyses with existing benchmarks and outline several open benchmark tasks enabled by ResPlan. Ultimately, ResPlan offers a significant advance in scale, realism, and usability, providing a robust foundation for developing and benchmarking next-generation spatial intelligence systems.",
    "label": "human"
  },
  {
    "text": "Autism Spectrum Disorder (ASD) is a complex neurodevelopmental condition marked by disruptions in brain connectivity. Functional MRI (fMRI) offers a non-invasive window into large-scale neural dynamics by measuring blood-oxygen-level-dependent (BOLD) signals across the brain. These signals can be modeled as interactions among Regions of Interest (ROIs), which are grouped into functional communities based on their underlying roles in brain function. Emerging evidence suggests that connectivity patterns within and between these communities are particularly sensitive to ASD-related alterations. Effectively capturing these patterns and identifying interactions that deviate from typical development is essential for improving ASD diagnosis and enabling biomarker discovery. In this work, we introduce ASDFormer, a Transformer-based architecture that incorporates a Mixture of Pooling-Classifier Experts (MoE) to capture neural signatures associated with ASD. By integrating multiple specialized expert branches with attention mechanisms, ASDFormer adaptively emphasizes different brain regions and connectivity patterns relevant to autism. This enables both improved classification performance and more interpretable identification of disorder-related biomarkers. Applied to the ABIDE dataset, ASDFormer achieves state-of-the-art diagnostic accuracy and reveals robust insights into functional connectivity disruptions linked to ASD, highlighting its potential as a tool for biomarker discovery.",
    "label": "human"
  },
  {
    "text": "Generalization in embodied AI is hindered by the \"seeing-to-doing gap,\" which stems from data scarcity and embodiment heterogeneity. To address this, we pioneer \"pointing\" as a unified, embodiment-agnostic intermediate representation, defining four core embodied pointing abilities that bridge high-level vision-language comprehension with low-level action primitives. We introduce Embodied-R1, a 3B Vision-Language Model (VLM) specifically designed for embodied reasoning and pointing. We use a wide range of embodied and general visual reasoning datasets as sources to construct a large-scale dataset, Embodied-Points-200K, which supports key embodied pointing capabilities. We then train Embodied-R1 using a two-stage Reinforced Fine-tuning (RFT) curriculum with a specialized multi-task reward design. Embodied-R1 achieves state-of-the-art performance on 11 embodied spatial and pointing benchmarks. Critically, it demonstrates robust zero-shot generalization by achieving a 56.2% success rate in the SIMPLEREnv and 87.5% across 8 real-world XArm tasks without any task-specific fine-tuning, representing a 62% improvement over strong baselines. Furthermore, the model exhibits high robustness against diverse visual disturbances. Our work shows that a pointing-centric representation, combined with an RFT training paradigm, offers an effective and generalizable pathway to closing the perception-action gap in robotics.",
    "label": "human"
  },
  {
    "text": "With the rapid growth of academic publications, peer review has become an essential yet time-consuming responsibility within the research community. Large Language Models (LLMs) have increasingly been adopted to assist in the generation of review comments; however, current LLM-based review tasks lack a unified evaluation benchmark to rigorously assess the models' ability to produce comprehensive, accurate, and human-aligned assessments, particularly in scenarios involving multimodal content such as figures and tables. To address this gap, we propose \\textbf{MMReview}, a comprehensive benchmark that spans multiple disciplines and modalities. MMReview includes multimodal content and expert-written review comments for 240 papers across 17 research domains within four major academic disciplines: Artificial Intelligence, Natural Sciences, Engineering Sciences, and Social Sciences. We design a total of 13 tasks grouped into four core categories, aimed at evaluating the performance of LLMs and Multimodal LLMs (MLLMs) in step-wise review generation, outcome formulation, alignment with human preferences, and robustness to adversarial input manipulation. Extensive experiments conducted on 16 open-source models and 5 advanced closed-source models demonstrate the thoroughness of the benchmark. We envision MMReview as a critical step toward establishing a standardized foundation for the development of automated peer review systems.",
    "label": "human"
  },
  {
    "text": "Long-context modeling is critical for a wide range of real-world tasks, including long-context question answering, summarization, and complex reasoning tasks. Recent studies have explored fine-tuning Large Language Models (LLMs) with synthetic data to enhance their long-context capabilities. However, the effectiveness of such approaches is often limited by the low diversity and factual inconsistencies in the generated data. To address these challenges, we propose LongMab-PO, a novel framework that leverages a Multi-Armed Bandit (MAB) rollout strategy to identify the most informative chunks from the given long context for sampling high-quality and diverse responses and constructing preference data pairs for Direct Preference Optimization (DPO) training. Specifically, we treat context chunks as arms of MAB, select chunks based on their expected reward scores to input into LLMs to generate responses, and iteratively update these scores based on reward feedback. This exploration and exploitation process enables the model to focus on the most relevant context segments, thereby generating and collecting high-quality and diverse responses. Finally, we collect these generated responses from the rollout process and apply the DPO method to further optimize the LLM. Experimental results show that LongMab-PO significantly improves the diversity and quality of preference data pairs, achieving state-of-the-art performance on long-context reasoning benchmarks. All code and data will be released on https://github.com/NEUIR/LongMab-PO.",
    "label": "human"
  },
  {
    "text": "Audio comprehension-including speech, non-speech sounds, and music-is essential for achieving human-level intelligence. Consequently, AI agents must demonstrate holistic audio understanding to qualify as generally intelligent. However, evaluating auditory intelligence comprehensively remains challenging. To address this gap, we introduce MMAU-Pro, the most comprehensive and rigorously curated benchmark for assessing audio intelligence in AI systems. MMAU-Pro contains 5,305 instances, where each instance has one or more audios paired with human expert-generated question-answer pairs, spanning speech, sound, music, and their combinations. Unlike existing benchmarks, MMAU-Pro evaluates auditory intelligence across 49 unique skills and multiple complex dimensions, including long-form audio comprehension, spatial audio reasoning, multi-audio understanding, among others. All questions are meticulously designed to require deliberate multi-hop reasoning, including both multiple-choice and open-ended response formats. Importantly, audio data is sourced directly ``from the wild\" rather than from existing datasets with known distributions. We evaluate 22 leading open-source and proprietary multimodal AI models, revealing significant limitations: even state-of-the-art models such as Gemini 2.5 Flash and Audio Flamingo 3 achieve only 59.2% and 51.7% accuracy, respectively, approaching random performance in multiple categories. Our extensive analysis highlights specific shortcomings and provides novel insights, offering actionable perspectives for the community to enhance future AI systems' progression toward audio general intelligence. The benchmark and code is available at https://sonalkum.github.io/mmau-pro.",
    "label": "human"
  },
  {
    "text": "As artificial intelligence (AI) systems become increasingly integral to organizational processes, they introduce new forms of fraud that are often subtle, systemic, and concealed within technical complexity. This paper introduces the AI-Fraud Diamond, an extension of the traditional Fraud Triangle that adds technical opacity as a fourth condition alongside pressure, opportunity, and rationalization. Unlike traditional fraud, AI-enabled deception may not involve clear human intent but can arise from system-level features such as opaque model behavior, flawed training data, or unregulated deployment practices. The paper develops a taxonomy of AI-fraud across five categories: input data manipulation, model exploitation, algorithmic decision manipulation, synthetic misinformation, and ethics-based fraud. To assess the relevance and applicability of the AI-Fraud Diamond, the study draws on expert interviews with auditors from two of the Big Four consulting firms. The findings underscore the challenges auditors face when addressing fraud in opaque and automated environments, including limited technical expertise, insufficient cross-disciplinary collaboration, and constrained access to internal system processes. These conditions hinder fraud detection and reduce accountability. The paper argues for a shift in audit methodology-from outcome-based checks to a more diagnostic approach focused on identifying systemic vulnerabilities. Ultimately, the work lays a foundation for future empirical research and audit innovation in a rapidly evolving AI governance landscape.",
    "label": "human"
  },
  {
    "text": "The Human-Robot Interaction (HRI) community often highlights the social context of an interaction as a key consideration when designing, implementing, and evaluating robot behavior. Unfortunately, researchers use the term \"social context\" in varied ways. This can lead to miscommunication, making it challenging to draw connections between related work on understanding and modeling the social contexts of human-robot interactions. To address this gap, we survey the HRI literature for existing definitions and uses of the term \"social context\". Then, we propose a conceptual model for describing the social context of a human-robot interaction. We apply this model to existing work, and we discuss a range of attributes of social contexts that can help researchers plan for interactions, develop behavior models for robots, and gain insights after interactions have taken place. We conclude with a discussion of open research questions in relation to understanding and modeling the social contexts of human-robot interactions.",
    "label": "human"
  },
  {
    "text": "This contribution is concerned with the following issue: can pretrained large language models (LLMs) be refined and customized to the point where they become virtual assistants helping experts with the effective use of a simulation tool? In this case study, the ``simulation tool'' considered is PyChrono, an open source multi-physics dynamics engine for multibody systems. We present a framework for refining and customizing both open- and closed-source LLMs to harness the power of AI in generating scripts that perform PyChrono virtual experiments. We refine and customize several classes of LLMs through a process that leads to a quantifiable improvement in the quality of the generated PyChrono simulation scripts. These scripts can range from simple single-pendulum simulations to complex virtual experiments involving full vehicles on deformable terrain. While the generated scripts are rarely perfect, they often serve as strong starting points for the user to modify and improve on. Additionally, the LLM can answer specific API questions about the simulator, or recommend modeling approaches. The framework discussed is general and can be applied to lower the entry barrier for simulation tools associated with other application domains.",
    "label": "human"
  },
  {
    "text": "This paper proposes a Vector Autoregression augmented with nonlinear factors that are modeled nonparametrically using regression trees. There are four main advantages of our model. First, modeling potential nonlinearities nonparametrically lessens the risk of mis-specification. Second, the use of factor methods ensures that departures from linearity are modeled parsimoniously. In particular, they exhibit functional pooling where a small number of nonlinear factors are used to model common nonlinearities across variables. Third, Bayesian computation using MCMC is straightforward even in very high dimensional models, allowing for efficient, equation by equation estimation, thus avoiding computational bottlenecks that arise in popular alternatives such as the time varying parameter VAR. Fourth, existing methods for identifying structural economic shocks in linear factor models can be adapted for the nonlinear case in a straightforward fashion using our model. Exercises involving artificial and macroeconomic data illustrate the properties of our model and its usefulness for forecasting and structural economic analysis.",
    "label": "human"
  },
  {
    "text": "We investigate to what extent Multimodal Large Language Models (MLLMs) can accurately identify the orientation of input images rotated 0{\\deg}, 90{\\deg}, 180{\\deg}, and 270{\\deg}. This task demands robust visual reasoning capabilities to detect rotational cues and contextualize spatial relationships within images, regardless of their orientation. To evaluate MLLMs on these abilities, we introduce RotBench -- a 350-image manually-filtered benchmark comprising lifestyle, portrait, and landscape images. Despite the relatively simple nature of this task, we show that several state-of-the-art open and proprietary MLLMs, including GPT-5, o3, and Gemini-2.5-Pro, do not reliably identify rotation in input images. Providing models with auxiliary information -- including captions, depth maps, and more -- or using chain-of-thought prompting offers only small and inconsistent improvements. Our results indicate that most models are able to reliably identify right-side-up (0{\\deg}) images, while certain models are able to identify upside-down (180{\\deg}) images. None can reliably distinguish between 90{\\deg} and 270{\\deg}. Simultaneously showing the image rotated in different orientations leads to moderate performance gains for reasoning models, while a modified setup using voting improves the performance of weaker models. We further show that fine-tuning does not improve models' ability to distinguish 90{\\deg} and 270{\\deg} rotations, despite substantially improving the identification of 180{\\deg} images. Together, these results reveal a significant gap between MLLMs' spatial reasoning capabilities and human perception in identifying rotation.",
    "label": "human"
  },
  {
    "text": "As Artificial Intelligence (AI) becomes increasingly integrated into daily life, there is a growing need to equip the next generation with the ability to apply, interact with, evaluate, and collaborate with AI systems responsibly. Prior research highlights the urgent demand from K-12 educators to teach students the ethical and effective use of AI for learning. To address this need, we designed an Large-Language Model (LLM)-based module to teach prompting literacy. This includes scenario-based deliberate practice activities with direct interaction with intelligent LLM agents, aiming to foster secondary school students' responsible engagement with AI chatbots. We conducted two iterations of classroom deployment in 11 authentic secondary education classrooms, and evaluated 1) AI-based auto-grader's capability; 2) students' prompting performance and confidence changes towards using AI for learning; and 3) the quality of learning and assessment materials. Results indicated that the AI-based auto-grader could grade student-written prompts with satisfactory quality. In addition, the instructional materials supported students in improving their prompting skills through practice and led to positive shifts in their perceptions of using AI for learning. Furthermore, data from Study 1 informed assessment revisions in Study 2. Analyses of item difficulty and discrimination in Study 2 showed that True/False and open-ended questions could measure prompting literacy more effectively than multiple-choice questions for our target learners. These promising outcomes highlight the potential for broader deployment and highlight the need for broader studies to assess learning effectiveness and assessment design.",
    "label": "human"
  },
  {
    "text": "The latest developments in AI focus on agentic systems where artificial and human agents cooperate to realize global goals. An example is collaborative learning, which aims to train a global model based on data from individual agents. A major challenge in designing such systems is to guarantee safety and alignment with human values, particularly a fair distribution of rewards upon achieving the global goal. Cooperative game theory offers useful abstractions of cooperating agents via value functions, which assign value to each coalition, and via reward functions. With these, the idea of fair allocation can be formalized by specifying fairness axioms and designing concrete mechanisms. Classical cooperative game theory, exemplified by the Shapley value, does not fully capture scenarios like collaborative learning, as it assumes nonreplicable resources, whereas data and models can be replicated. Infinite replicability requires a generalized notion of fairness, formalized through new axioms and mechanisms. These must address imbalances in reciprocal benefits among participants, which can lead to strategic exploitation and unfair allocations. The main contribution of this paper is a mechanism and a proof that it fulfills the property of mutual fairness, formalized by the Balanced Reciprocity Axiom. It ensures that, for every pair of players, each benefits equally from the participation of the other.",
    "label": "human"
  },
  {
    "text": "Large Language Models (LLMs) require sophisticated prompting, yet current practices face challenges in structure, data integration, format sensitivity, and tooling. Existing methods lack comprehensive solutions for organizing complex prompts involving diverse data types (documents, tables, images) or managing presentation variations systematically. To address these gaps, we introduce POML (Prompt Orchestration Markup Language). POML employs component-based markup for logical structure (roles, tasks, examples), specialized tags for seamless data integration, and a CSS-like styling system to decouple content from presentation, reducing formatting sensitivity. It includes templating for dynamic prompts and a comprehensive developer toolkit (IDE support, SDKs) to improve version control and collaboration. We validate POML through two case studies demonstrating its impact on complex application integration (PomLink) and accuracy performance (TableQA), as well as a user study assessing its effectiveness in real-world development scenarios.",
    "label": "human"
  },
  {
    "text": "The rise of autonomous, AI-driven agents in economic settings raises critical questions about their emergent strategic behavior. This paper investigates these dynamics in the cooperative context of a multi-echelon supply chain, a system famously prone to instabilities like the bullwhip effect. We conduct computational experiments with generative AI agents, powered by Large Language Models (LLMs), within a controlled supply chain simulation designed to isolate their behavioral tendencies. Our central finding is the \"collaboration paradox\": a novel, catastrophic failure mode where theoretically superior collaborative AI agents, designed with Vendor-Managed Inventory (VMI) principles, perform even worse than non-AI baselines. We demonstrate that this paradox arises from an operational flaw where agents hoard inventory, starving the system. We then show that resilience is only achieved through a synthesis of two distinct layers: high-level, AI-driven proactive policy-setting to establish robust operational targets, and a low-level, collaborative execution protocol with proactive downstream replenishment to maintain stability. Our final framework, which implements this synthesis, can autonomously generate, evaluate, and quantify a portfolio of viable strategic choices. The work provides a crucial insight into the emergent behaviors of collaborative AI agents and offers a blueprint for designing stable, effective AI-driven systems for business analytics.",
    "label": "human"
  },
  {
    "text": "This work revisits and extends synthetic query generation pipelines for Neural Information Retrieval (NIR) by leveraging the InPars Toolkit, a reproducible, end-to-end framework for generating training data using large language models (LLMs). We first assess the reproducibility of the original InPars, InPars-V2, and Promptagator pipelines on the SciFact benchmark and validate their effectiveness using open-source reranker and generator models. Building on this foundation, we introduce two key extensions to the pipeline: (1) fine-tuning a query generator LLM via Contrastive Preference Optimization (CPO) to improve the signal quality in generated queries, and (2) replacing static prompt templates with dynamic, Chain-of-Thought (CoT) optimized prompts using the DSPy framework. Our results show that both extensions reduce the need for aggressive filtering while improving retrieval performance. All code, models, and synthetic datasets are publicly released to support further research at: \\href{https://github.com/danilotpnta/IR2-project}{this https URL}.",
    "label": "human"
  },
  {
    "text": "A policy in deep reinforcement learning (RL), either deterministic or stochastic, is commonly parameterized as a Gaussian distribution alone, limiting the learned behavior to be unimodal. However, the nature of many practical decision-making problems favors a multimodal policy that facilitates robust exploration of the environment and thus to address learning challenges arising from sparse rewards, complex dynamics, or the need for strategic adaptation to varying contexts. This issue is exacerbated in continuous control domains where exploration usually takes place in the vicinity of the predicted optimal action, either through an additive Gaussian noise or the sampling process of a stochastic policy. In this paper, we introduce Categorical Policies to model multimodal behavior modes with an intermediate categorical distribution, and then generate output action that is conditioned on the sampled mode. We explore two sampling schemes that ensure differentiable discrete latent structure while maintaining efficient gradient-based optimization. By utilizing a latent categorical distribution to select the behavior mode, our approach naturally expresses multimodality while remaining fully differentiable via the sampling tricks. We evaluate our multimodal policy on a set of DeepMind Control Suite environments, demonstrating that through better exploration, our learned policies converge faster and outperform standard Gaussian policies. Our results indicate that the Categorical distribution serves as a powerful tool for structured exploration and multimodal behavior representation in continuous control.",
    "label": "human"
  },
  {
    "text": "Recent advances in large language models (LLMs) have sparked interest in their application to IoT and automation systems, particularly for facilitating device management through natural language instructions. However, existing centralized approaches face significant scalability challenges when managing and coordinating the collaboration between IoT devices of diverse capabilities in large-scale heterogeneous IoT systems. This paper introduces LLMind 2.0, a distributed IoT automation framework that addresses the scalability challenges through lightweight LLM-empowered device agents via natural language-based machine-to-machine (M2M) communication. Unlike previous LLM-controlled automation systems that rely on a centralized coordinator to generate device-specific code to be executed on individual devices, LLMind 2.0 distributes intelligence across individual devices through lightweight LLMs embedded in IoT devices. The central coordinator translates human instructions into simple subtasks described in natural human language, which are then processed by device-specific agents to generate device-specific code locally at the associated devices. This approach transcends device heterogeneity barriers by using natural language as a unified communication medium, enabling seamless collaboration between devices from different manufacturers. The system incorporates several key innovations: a Retrieval-Augmented Generation (RAG) mechanism for accurate subtask-to-API mapping, fine-tuned lightweight LLMs for reliable code generation, and a finite state machine-based task execution framework. Experimental validation in multi-robot warehouse scenarios and real-world WiFi network deployments demonstrates significant improvements in scalability, reliability, and privacy protection compared to the centralized approach.",
    "label": "human"
  },
  {
    "text": "Time-series data is central to decision-making in financial markets, yet building high-performing, interpretable, and auditable models remains a major challenge. While Automated Machine Learning (AutoML) frameworks streamline model development, they often lack adaptability and responsiveness to domain-specific needs and evolving objectives. Concurrently, Large Language Models (LLMs) have enabled agentic systems capable of reasoning, memory management, and dynamic code generation, offering a path toward more flexible workflow automation. In this paper, we introduce \\textsf{TS-Agent}, a modular agentic framework designed to automate and enhance time-series modeling workflows for financial applications. The agent formalizes the pipeline as a structured, iterative decision process across three stages: model selection, code refinement, and fine-tuning, guided by contextual reasoning and experimental feedback. Central to our architecture is a planner agent equipped with structured knowledge banks, curated libraries of models and refinement strategies, which guide exploration, while improving interpretability and reducing error propagation. \\textsf{TS-Agent} supports adaptive learning, robust debugging, and transparent auditing, key requirements for high-stakes environments such as financial services. Empirical evaluations on diverse financial forecasting and synthetic data generation tasks demonstrate that \\textsf{TS-Agent} consistently outperforms state-of-the-art AutoML and agentic baselines, achieving superior accuracy, robustness, and decision traceability.",
    "label": "human"
  },
  {
    "text": "Intelligence is fundamentally non-ergodic: it emerges not from uniform sampling or optimization from scratch, but from the structured reuse of prior inference trajectories. We introduce Memory-Amortized Inference (MAI) as a formal framework in which cognition is modeled as inference over latent cycles in memory, rather than recomputation through gradient descent. MAI systems encode inductive biases via structural reuse, minimizing entropy and enabling context-aware, structure-preserving inference. This approach reframes cognitive systems not as ergodic samplers, but as navigators over constrained latent manifolds, guided by persistent topological memory. Through the lens of delta-homology, we show that MAI provides a principled foundation for Mountcastle's Universal Cortical Algorithm, modeling each cortical column as a local inference operator over cycle-consistent memory states. Furthermore, we establish a time-reversal duality between MAI and reinforcement learning: whereas RL propagates value forward from reward, MAI reconstructs latent causes backward from memory. This inversion paves a path toward energy-efficient inference and addresses the computational bottlenecks facing modern AI. MAI thus offers a unified, biologically grounded theory of intelligence based on structure, reuse, and memory. We also briefly discuss the profound implications of MAI for achieving artificial general intelligence (AGI).",
    "label": "human"
  },
  {
    "text": "We present a study that translates the Force Concept Inventory (FCI) using OpenAI GPT-4o and assess the specific difficulties of translating a scientific-focused topic using Large Language Models (LLMs). The FCI is a physics exam meant to evaluate outcomes of a student cohort before and after instruction in Newtonian physics. We examine the problem-solving ability of the LLM in both the translated document and the translation back into English, detailing the language-dependent issues that complicate the translation. While ChatGPT performs remarkably well on answering the questions in both the translated language as well as the back-translation into English, problems arise with language-specific nuances and formatting. Pitfalls include words or phrases that lack one-to-one matching terms in another language, especially discipline-specific scientific terms, or outright mistranslations. Depending on the context, these translations can result in a critical change in the physical meaning of the problem. Additionally, issues with question numbering and lettering are found in some languages. The issues around the translations of numbering and lettering provide insight into the abilities of the LLM and suggest that it is not simply relying upon FCI questions that may have been part of the LLM training data to provide answers. These findings underscore that while LLMs can accelerate multilingual access to educational tools, careful review is still needed to ensure fidelity and clarity in translated assessments. LLMs provide a new opportunity to expand educational tools and assessments. At the same time, there are unique challenges using LLMs to facilitate translations that this case study examines in detail.",
    "label": "human"
  },
  {
    "text": "Modern GPUs are equipped with large amounts of high-bandwidth memory, enabling them to support mini-batch sizes of up to tens of thousands of training samples. However, most existing optimizers struggle to perform effectively at such a large batch size. As batch size increases, gradient noise decreases due to averaging over many samples, limiting the ability of first-order methods to escape sharp or suboptimal minima and reach the global minimum. Meanwhile, second-order methods like the natural gradient with Kronecker-Factored Approximate Curvature (KFAC) often require excessively high damping to remain stable at large batch sizes. This high damping effectively washes out the curvature information that gives these methods their advantage, reducing their performance to that of simple gradient descent. In this paper, we introduce Fisher-Orthogonal Projection (FOP), a novel technique that restores the effectiveness of the second-order method at very large batch sizes, enabling scalable training with improved generalization and faster convergence. FOP constructs a variance-aware update direction by leveraging gradients from two sub-batches, enhancing the average gradient with a component of the gradient difference that is orthogonal to the average under the Fisher-metric.",
    "label": "human"
  },
  {
    "text": "LLMs have recently been used to generate Python programs representing generalized plans in PDDL planning, i.e., plans that generalize across the tasks of a given PDDL domain. Previous work proposed a framework consisting of three steps: the LLM first generates a summary and then a strategy for the domain, both in natural language, and then implements that strategy as a Python program, that gets debugged on example planning tasks. In that work, only one strategy is generated and passed directly to the program generation. If the strategy is incorrect, its implementation will therefore result in an incorrect generalized plan. Here, we introduce an approach that generates the strategy in the form of pseudocode and enables automatic debugging of the pseudocode, hence allowing us to identify and fix errors prior to the generation of the generalized plan itself. Additionally, we extend the Python debugging phase with a reflection step prompting the LLM to pinpoint the reason for the observed plan failure. Finally, we take inspiration from LLM code generation to produce several program variants and pick the best one. Running experiments on 17 benchmark domains, we show that these extensions substantially improve (and never deteriorate) the quality of the generalized plans. In 12 of the domains, our best Python programs solve all tasks that can be generated with the respective instance generator.",
    "label": "human"
  },
  {
    "text": "Reinforcement learning (RL) has demonstrated great potential in robotic operations. However, its data-intensive nature and reliance on the Markov Decision Process (MDP) assumption limit its practical deployment in real-world scenarios involving complex dynamics and long-term temporal dependencies, such as multi-robot manipulation. Decision Transformers (DTs) have emerged as a promising offline alternative by leveraging causal transformers for sequence modeling in RL tasks. However, their applications to multi-robot manipulations still remain underexplored. To address this gap, we propose a novel framework, Symbolically-Guided Decision Transformer (SGDT), which integrates a neuro-symbolic mechanism with a causal transformer to enable deployable multi-robot collaboration. In the proposed SGDT framework, a neuro-symbolic planner generates a high-level task-oriented plan composed of symbolic subgoals. Guided by these subgoals, a goal-conditioned decision transformer (GCDT) performs low-level sequential decision-making for multi-robot manipulation. This hierarchical architecture enables structured, interpretable, and generalizable decision making in complex multi-robot collaboration tasks. We evaluate the performance of SGDT across a range of task scenarios, including zero-shot and few-shot scenarios. To our knowledge, this is the first work to explore DT-based technology for multi-robot manipulation.",
    "label": "human"
  },
  {
    "text": "The Circle of Willis (CoW), vital for ensuring consistent blood flow to the brain, is closely linked to ischemic stroke. Accurate assessment of the CoW is important for identifying individuals at risk and guiding appropriate clinical management. Among existing imaging methods, Transcranial Color-coded Doppler (TCCD) offers unique advantages due to its radiation-free nature, affordability, and accessibility. However, reliable TCCD assessments depend heavily on operator expertise for identifying anatomical landmarks and performing accurate angle correction, which limits its widespread adoption. To address this challenge, we propose an AI-powered, real-time CoW auto-segmentation system capable of efficiently capturing cerebral arteries. No prior studies have explored AI-driven cerebrovascular segmentation using TCCD. In this work, we introduce a novel Attention-Augmented Wavelet YOLO (AAW-YOLO) network tailored for TCCD data, designed to provide real-time guidance for brain vessel segmentation in the CoW. We prospectively collected TCCD data comprising 738 annotated frames and 3,419 labeled artery instances to establish a high-quality dataset for model training and evaluation. The proposed AAW-YOLO demonstrated strong performance in segmenting both ipsilateral and contralateral CoW vessels, achieving an average Dice score of 0.901, IoU of 0.823, precision of 0.882, recall of 0.926, and mAP of 0.953, with a per-frame inference speed of 14.199 ms. This system offers a practical solution to reduce reliance on operator experience in TCCD-based cerebrovascular screening, with potential applications in routine clinical workflows and resource-constrained settings. Future research will explore bilateral modeling and larger-scale validation.",
    "label": "human"
  },
  {
    "text": "The Id-Pattern system within the RED.AI project (Reabilita\\c{c}\\~ao Estrutural Digital atrav\\'es da AI) consists of an agentic system designed to assist in the identification of stone deterioration patterns. Traditional methodologies, based on direct observation by expert teams, are accurate but costly in terms of time and resources. The system developed here introduces and evaluates a multi-agent artificial intelligence (AI) system, designed to simulate collaboration between experts and automate the diagnosis of stone pathologies from visual evidence. The approach is based on a cognitive architecture that orchestrates a team of specialized AI agents which, in this specific case, are limited to five: a lithologist, a pathologist, an environmental expert, a conservator-restorer, and a diagnostic coordinator. To evaluate the system we selected 28 difficult images involving multiple deterioration patterns. Our first results showed a huge boost on all metrics of our system compared to the foundational model.",
    "label": "human"
  },
  {
    "text": "This paper explores the problem of fair assignment on Multi-Stage graphs. A multi-stage graph consists of nodes partitioned into $K$ disjoint sets (stages) structured as a sequence of weighted bipartite graphs formed across adjacent stages. The goal is to assign node-disjoint paths to $n$ agents starting from the first stage and ending in the last stage. We show that an efficient assignment that minimizes the overall sum of costs of all the agents' paths may be highly unfair and lead to significant cost disparities (envy) among the agents. We further show that finding an envy-minimizing assignment on a multi-stage graph is NP-hard. We propose the C-Balance algorithm, which guarantees envy that is bounded by $2M$ in the case of two agents, where $M$ is the maximum edge weight. We demonstrate the algorithm's tightness by presenting an instance where the envy is $2M$. We further show that the cost of fairness ($CoF$), defined as the ratio of the cost of the assignment given by the fair algorithm to that of the minimum cost assignment, is bounded by $2$ for C-Balance. We then extend this approach to $n$ agents by proposing the DC-Balance algorithm that makes iterative calls to C-Balance. We show the convergence of DC-Balance, resulting in envy that is arbitrarily close to $2M$. We derive $CoF$ bounds for DC-Balance and provide insights about its dependency on the instance-specific parameters and the desired degree of envy. We experimentally show that our algorithm runs several orders of magnitude faster than a suitably formulated ILP.",
    "label": "human"
  },
  {
    "text": "Current e-commerce multimodal retrieval systems face two key limitations: they optimize for specific tasks with fixed modality pairings, and lack comprehensive benchmarks for evaluating unified retrieval approaches. To address these challenges, we introduce UniECS, a unified multimodal e-commerce search framework that handles all retrieval scenarios across image, text, and their combinations. Our work makes three key contributions. First, we propose a flexible architecture with a novel gated multimodal encoder that uses adaptive fusion mechanisms. This encoder integrates different modality representations while handling missing modalities. Second, we develop a comprehensive training strategy to optimize learning. It combines cross-modal alignment loss (CMAL), cohesive local alignment loss (CLAL), intra-modal contrastive loss (IMCL), and adaptive loss weighting. Third, we create M-BEER, a carefully curated multimodal benchmark containing 50K product pairs for e-commerce search evaluation. Extensive experiments demonstrate that UniECS consistently outperforms existing methods across four e-commerce benchmarks with fine-tuning or zero-shot evaluation. On our M-BEER bench, UniECS achieves substantial improvements in cross-modal tasks (up to 28\\% gain in R@10 for text-to-image retrieval) while maintaining parameter efficiency (0.2B parameters) compared to larger models like GME-Qwen2VL (2B) and MM-Embed (8B). Furthermore, we deploy UniECS in the e-commerce search platform of Kuaishou Inc. across two search scenarios, achieving notable improvements in Click-Through Rate (+2.74\\%) and Revenue (+8.33\\%). The comprehensive evaluation demonstrates the effectiveness of our approach in both experimental and real-world settings. Corresponding codes, models and datasets will be made publicly available at https://github.com/qzp2018/UniECS.",
    "label": "human"
  },
  {
    "text": "This paper investigates a reconfigurable intelligent surface (RIS)-assisted integrated sensing and communication (ISAC) system and proposes a joint communication and sensing beamforming design based on non-orthogonal multiple access (NOMA) technology. The system employs a dual-functional base station (DFBS) to simultaneously serve multiple users and sense multiple targets with the aid of RIS. To maximize the sum-rate of users, we jointly optimize the DFBS's active beamforming, the RIS's reflection coefficients, and the radar receive filters. The optimization is performed under constraints including the radar signal-to-noise ratio thresholds, the user signal-to-interference-plus-noise ratio requirements, the phase shifts of the RIS, the total transmit power, the receive filters, and the successive interference cancellation decoding order. To tackle the complex interdependencies and non-convex nature of the optimization problem, we introduce an effective iterative algorithm based on the alternating optimization framework. Simulation results demonstrate that the proposed algorithm outperforms baseline algorithms, highlighting its distinct advantages in the considered RIS-empowered NOMA-ISAC systems.",
    "label": "human"
  },
  {
    "text": "Pruning is a core technique for compressing neural networks to improve computational efficiency. This process is typically approached in two ways: one-shot pruning, which involves a single pass of training and pruning, and iterative pruning, where pruning is performed over multiple cycles for potentially finer network refinement. Although iterative pruning has historically seen broader adoption, this preference is often assumed rather than rigorously tested. Our study presents one of the first systematic and comprehensive comparisons of these methods, providing rigorous definitions, benchmarking both across structured and unstructured settings, and applying different pruning criteria and modalities. We find that each method has specific advantages: one-shot pruning proves more effective at lower pruning ratios, while iterative pruning performs better at higher ratios. Building on these findings, we advocate for patience-based pruning and introduce a hybrid approach that can outperform traditional methods in certain scenarios, providing valuable insights for practitioners selecting a pruning strategy tailored to their goals and constraints. Source code is available at https://github.com/janumiko/pruning-benchmark.",
    "label": "human"
  },
  {
    "text": "This study explores the integration of Building Information Modeling (BIM) with Natural Language Processing (NLP) to automate the extraction of requirements from unstructured French Building Technical Specification (BTS) documents within the construction industry. Employing Named Entity Recognition (NER) and Relation Extraction (RE) techniques, the study leverages the transformer-based model CamemBERT and applies transfer learning with the French language model Fr\\_core\\_news\\_lg, both pre-trained on a large French corpus in the general domain. To benchmark these models, additional approaches ranging from rule-based to deep learning-based methods are developed. For RE, four different supervised models, including Random Forest, are implemented using a custom feature vector. A hand-crafted annotated dataset is used to compare the effectiveness of NER approaches and RE models. Results indicate that CamemBERT and Fr\\_core\\_news\\_lg exhibited superior performance in NER, achieving F1-scores over 90\\%, while Random Forest proved most effective in RE, with an F1 score above 80\\%. The outcomes are intended to be represented as a knowledge graph in future work to further enhance automatic verification systems.",
    "label": "human"
  },
  {
    "text": "The structure of biological neural circuits-modular, hierarchical, and sparsely interconnected-reflects an efficient trade-off between wiring cost, functional specialization, and robustness. These principles offer valuable insights for artificial neural network (ANN) design, especially as networks grow in depth and scale. Sparsity, in particular, has been widely explored for reducing memory and computation, improving speed, and enhancing generalization. Motivated by systems neuroscience findings, we explore how patterns of functional connectivity in the mouse visual cortex-specifically, ensemble-to-ensemble communication, can inform ANN design. We introduce G2GNet, a novel architecture that imposes sparse, modular connectivity across feedforward layers. Despite having significantly fewer parameters than fully connected models, G2GNet achieves superior accuracy on standard vision benchmarks. To our knowledge, this is the first architecture to incorporate biologically observed functional connectivity patterns as a structural bias in ANN design. We complement this static bias with a dynamic sparse training (DST) mechanism that prunes and regrows edges during training. We also propose a Hebbian-inspired rewiring rule based on activation correlations, drawing on principles of biological plasticity. G2GNet achieves up to 75% sparsity while improving accuracy by up to 4.3% on benchmarks, including Fashion-MNIST, CIFAR-10, and CIFAR-100, outperforming dense baselines with far fewer computations.",
    "label": "human"
  },
  {
    "text": "Information overload and the rapid pace of scientific advancement make it increasingly difficult to evaluate and allocate resources to new research proposals. Is there a structure to scientific discovery that could inform such decisions? We present statistical evidence for such structure, by training a classifier that successfully predicts high-citation research papers between 2010-2024 in the Computer Science, Physics, and PubMed domains.",
    "label": "human"
  },
  {
    "text": "Retrieval-Augmented Generation (RAG) technology has been widely applied in recent years. However, despite the emergence of various RAG frameworks, a single RAG framework still cannot adapt well to a broad range of downstream tasks. Therefore, how to leverage the advantages of multiple RAG systems has become an area worth exploring. To address this issue, we have conducted a comprehensive and systematic investigation into ensemble methods based on RAG systems. Specifically, we have analyzed the RAG ensemble framework from both theoretical and mechanistic analysis perspectives. From the theoretical analysis, we provide the first explanation of the RAG ensemble framework from the perspective of information entropy. In terms of mechanism analysis, we have explored the RAG ensemble framework from both the pipeline and module levels. We carefully select four different pipelines (Branching, Iterative, Loop, and Agentic) and three different modules (Generator, Retriever, and Reranker) to solve seven different research questions. The experiments show that aggregating multiple RAG systems is both generalizable and robust, whether at the pipeline level or the module level. Our work lays the foundation for similar research on the multi-RAG system ensemble.",
    "label": "human"
  },
  {
    "text": "Achieving top-notch performance in Intelligent Transportation detection is a critical research area. However, many challenges still need to be addressed when it comes to detecting in a cross-domain scenario. In this paper, we propose a Self-Aware Adaptive Alignment (SA3), by leveraging an efficient alignment mechanism and recognition strategy. Our proposed method employs a specified attention-based alignment module trained on source and target domain datasets to guide the image-level features alignment process, enabling the local-global adaptive alignment between the source domain and target domain. Features from both domains, whose channel importance is re-weighted, are fed into the region proposal network, which facilitates the acquisition of salient region features. Also, we introduce an instance-to-image level alignment module specific to the target domain to adaptively mitigate the domain gap. To evaluate the proposed method, extensive experiments have been conducted on popular cross-domain object detection benchmarks. Experimental results show that SA3 achieves superior results to the previous state-of-the-art methods.",
    "label": "human"
  },
  {
    "text": "Evaluating Natural Language Generation (NLG) is crucial for the practical adoption of AI, but has been a longstanding research challenge. While human evaluation is considered the de-facto standard, it is expensive and lacks scalability. Practical applications have driven the development of various automatic evaluation metrics (AEM), designed to compare the model output with human-written references, generating a score which approximates human judgment. Over time, AEMs have evolved from simple lexical comparisons, to semantic similarity models and, more recently, to LLM-based evaluators. However, it seems that no single metric has emerged as a definitive solution, resulting in studies using different ones without fully considering the implications. This paper aims to show this by conducting a thorough examination of the methodologies of existing metrics, their documented strengths and limitations, validation methods, and correlations with human judgment. We identify several key challenges: metrics often capture only specific aspects of text quality, their effectiveness varies by task and dataset, validation practices remain unstructured, and correlations with human judgment are inconsistent. Importantly, we find that these challenges persist in the most recent type of metric, LLM-as-a-Judge, as well as in the evaluation of Retrieval Augmented Generation (RAG), an increasingly relevant task in academia and industry. Our findings challenge the quest for the 'perfect metric'. We propose selecting metrics based on task-specific needs and leveraging complementary evaluations and advocate that new metrics should focus on enhanced validation methodologies.",
    "label": "human"
  },
  {
    "text": "Spiking neural networks (SNNs) offer energy efficiency over artificial neural networks (ANNs) but suffer from high latency and computational overhead due to their multi-timestep operational nature. While various dynamic computation methods have been developed to mitigate this by targeting spatial, temporal, or architecture-specific redundancies, they remain fragmented. While the principles of adaptive computation time (ACT) offer a robust foundation for a unified approach, its application to SNN-based vision Transformers (ViTs) is hindered by two core issues: the violation of its temporal similarity prerequisite and a static architecture fundamentally unsuited for its principles. To address these challenges, we propose STAS (Spatio-Temporal Adaptive computation time for Spiking transformers), a framework that co-designs the static architecture and dynamic computation policy. STAS introduces an integrated spike patch splitting (I-SPS) module to establish temporal stability by creating a unified input representation, thereby solving the architectural problem of temporal dissimilarity. This stability, in turn, allows our adaptive spiking self-attention (A-SSA) module to perform two-dimensional token pruning across both spatial and temporal axes. Implemented on spiking Transformer architectures and validated on CIFAR-10, CIFAR-100, and ImageNet, STAS reduces energy consumption by up to 45.9%, 43.8%, and 30.1%, respectively, while simultaneously improving accuracy over SOTA models.",
    "label": "human"
  },
  {
    "text": "As AI systems increasingly rely on training data, assessing dataset trustworthiness has become critical, particularly for properties like fairness or bias that emerge at the dataset level. Prior work has used Subjective Logic to assess trustworthiness of individual data, but not to evaluate trustworthiness properties that emerge only at the level of the dataset as a whole. This paper introduces the first formal framework for assessing the trustworthiness of AI training datasets, enabling uncertainty-aware evaluations of global properties such as bias. Built on Subjective Logic, our approach supports trust propositions and quantifies uncertainty in scenarios where evidence is incomplete, distributed, and/or conflicting. We instantiate this framework on the trustworthiness property of bias, and we experimentally evaluate it based on a traffic sign recognition dataset. The results demonstrate that our method captures class imbalance and remains interpretable and robust in both centralized and federated contexts.",
    "label": "human"
  },
  {
    "text": "Quantified formulas pose a significant challenge for Satisfiability Modulo Theories (SMT) solvers due to their inherent undecidability. Existing instantiation techniques, such as e-matching, syntax-guided, model-based, conflict-based, and enumerative methods, often complement each other. This paper introduces a novel instantiation approach that dynamically learns from these techniques during solving. By treating observed instantiations as samples from a latent language, we use probabilistic context-free grammars to generate new, similar terms. Our method not only mimics successful past instantiations but also explores diversity by optionally inverting learned term probabilities, aiming to balance exploitation and exploration in quantifier reasoning.",
    "label": "human"
  },
  {
    "text": "Controlling the length of text produced by large language models (LLMs) remains challenging: models frequently overshoot or undershoot explicit length instructions because they cannot reliably keep an internal token count. We present a prompt-based, one-shot strategy that compels an off-the-shelf LLM to generate exactly a desired number of tokens - words (English) or characters (Chinese) - without any fine-tuning or iterative sampling. The prompt appends countdown markers and explicit counting rules so that the model \"writes while counting.\" We evaluate on four settings: open-ended generation (1-1000 tokens), XSUM summarization, MT-Bench-LI instruction following, and the LIFEBENCH equal-length track. On MT-Bench-LI, strict length compliance with GPT-4.1 leaps from below 30% under naive prompts to above 95% with our countdown prompt, surpassing the popular draft-then-revise baseline, while judged answer quality is preserved. These results show that precise length control can be achieved through prompt engineering alone, offering a lightweight alternative to training- or decoding-based methods.",
    "label": "human"
  },
  {
    "text": "Rapid scaling of deep learning models has enabled performance gains across domains, yet it introduced several challenges. Federated Learning (FL) has emerged as a promising framework to address these concerns by enabling decentralized training. Nevertheless, communication efficiency remains a key bottleneck in FL, particularly under heterogeneous and dynamic client participation. Existing methods, such as FedAvg and FedProx, or other approaches, including client selection strategies, attempt to mitigate communication costs. However, the problem of choosing the number of clients in a training round remains extremely underexplored. We introduce Intelligent Selection of Participants (ISP), an adaptive mechanism that dynamically determines the optimal number of clients per round to enhance communication efficiency without compromising model accuracy. We validate the effectiveness of ISP across diverse setups, including vision transformers, real-world ECG classification, and training with gradient compression. Our results show consistent communication savings of up to 30\\% without losing the final quality. Applying ISP to different real-world ECG classification setups highlighted the selection of the number of clients as a separate task of federated learning.",
    "label": "human"
  },
  {
    "text": "We introduce Med-CTX, a fully transformer based multimodal framework for explainable breast cancer ultrasound segmentation. We integrate clinical radiology reports to boost both performance and interpretability. Med-CTX achieves exact lesion delineation by using a dual-branch visual encoder that combines ViT and Swin transformers, as well as uncertainty aware fusion. Clinical language structured with BI-RADS semantics is encoded by BioClinicalBERT and combined with visual features utilising cross-modal attention, allowing the model to provide clinically grounded, model generated explanations. Our methodology generates segmentation masks, uncertainty maps, and diagnostic rationales all at once, increasing confidence and transparency in computer assisted diagnosis. On the BUS-BRA dataset, Med-CTX achieves a Dice score of 99% and an IoU of 95%, beating existing baselines U-Net, ViT, and Swin. Clinical text plays a key role in segmentation accuracy and explanation quality, as evidenced by ablation studies that show a -5.4% decline in Dice score and -31% in CIDEr. Med-CTX achieves good multimodal alignment (CLIP score: 85%) and increased confi dence calibration (ECE: 3.2%), setting a new bar for trustworthy, multimodal medical architecture.",
    "label": "human"
  },
  {
    "text": "The rapid development of large language models (LLMs) has significantly propelled the development of artificial intelligence (AI) agents, which are increasingly evolving into diverse autonomous entities, advancing the LLM-based multi-agent systems (LaMAS). However, current agentic ecosystems remain fragmented and closed. Establishing an interconnected and scalable paradigm for Agentic AI has become a critical prerequisite. Although Agentic Web proposes an open architecture to break the ecosystem barriers, its implementation still faces core challenges such as privacy protection, data management, and value measurement. Existing centralized or semi-centralized paradigms suffer from inherent limitations, making them inadequate for supporting large-scale, heterogeneous, and cross-domain autonomous interactions. To address these challenges, this paper introduces the blockchain-enabled trustworthy Agentic Web (BetaWeb). By leveraging the inherent strengths of blockchain, BetaWeb not only offers a trustworthy and scalable infrastructure for LaMAS but also has the potential to advance the Web paradigm from Web3 (centered on data ownership) towards Web3.5, which emphasizes ownership of agent capabilities and the monetization of intelligence. Beyond a systematic examination of the BetaWeb framework, this paper presents a five-stage evolutionary roadmap, outlining the path of LaMAS from passive execution to advanced collaboration and autonomous governance. We also conduct a comparative analysis of existing products and discuss key challenges of BetaWeb from multiple perspectives. Ultimately, we argue that deep integration between blockchain and LaMAS can lay the foundation for a resilient, trustworthy, and sustainably incentivized digital ecosystem. A summary of the enabling technologies for each stage is available at https://github.com/MatZaharia/BetaWeb.",
    "label": "human"
  },
  {
    "text": "Controllable text-to-audio generation aims to synthesize audio from textual descriptions while satisfying user-specified constraints, including event types, temporal sequences, and onset and offset timestamps. This enables precise control over both the content and temporal structure of the generated audio. Despite recent progress, existing methods still face inherent trade-offs among accurate temporal localization, open-vocabulary scalability, and practical efficiency. To address these challenges, we propose DegDiT, a novel dynamic event graph-guided diffusion transformer framework for open-vocabulary controllable audio generation. DegDiT encodes the events in the description as structured dynamic graphs. The nodes in each graph are designed to represent three aspects: semantic features, temporal attributes, and inter-event connections. A graph transformer is employed to integrate these nodes and produce contextualized event embeddings that serve as guidance for the diffusion model. To ensure high-quality and diverse training data, we introduce a quality-balanced data selection pipeline that combines hierarchical event annotation with multi-criteria quality scoring, resulting in a curated dataset with semantic diversity. Furthermore, we present consensus preference optimization, facilitating audio generation through consensus among multiple reward signals. Extensive experiments on AudioCondition, DESED, and AudioTime datasets demonstrate that DegDiT achieves state-of-the-art performances across a variety of objective and subjective evaluation metrics.",
    "label": "human"
  },
  {
    "text": "Dynamic contrast-enhanced (DCE) MRI is essential for breast cancer diagnosis and treatment. However, its reliance on contrast agents introduces safety concerns, contraindications, increased cost, and workflow complexity. To this end, we present pre-contrast conditioned denoising diffusion probabilistic models to synthesize DCE-MRI, introducing, evaluating, and comparing a total of 22 generative model variants in both single-breast and full breast settings. Towards enhancing lesion fidelity, we introduce both tumor-aware loss functions and explicit tumor segmentation mask conditioning. Using a public multicenter dataset and comparing to respective pre-contrast baselines, we observe that subtraction image-based models consistently outperform post-contrast-based models across five complementary evaluation metrics. Apart from assessing the entire image, we also separately evaluate the region of interest, where both tumor-aware losses and segmentation mask inputs improve evaluation metrics. The latter notably enhance qualitative results capturing contrast uptake, albeit assuming access to tumor localization inputs that are not guaranteed to be available in screening settings. A reader study involving 2 radiologists and 4 MRI technologists confirms the high realism of the synthetic images, indicating an emerging clinical potential of generative contrast-enhancement. We share our codebase at https://github.com/sebastibar/conditional-diffusion-breast-MRI.",
    "label": "human"
  },
  {
    "text": "This paper reports on the implementation and evaluation of a Model Context Protocol (MCP) server for DraCor, enabling Large Language Models (LLM) to autonomously interact with the DraCor API. We conducted experiments focusing on tool selection and application by the LLM, employing a qualitative approach that includes systematic observation of prompts to understand how LLMs behave when using MCP tools, evaluating \"Tool Correctness\", \"Tool-Calling Efficiency\", and \"Tool-Use Reliability\". Our findings highlight the importance of \"Docstring Engineering\", defined as reflexively crafting tool documentation to optimize LLM-tool interaction. Our experiments demonstrate both the promise of agentic AI for research in Computational Literary Studies and the essential infrastructure development needs for reliable Digital Humanities infrastructures.",
    "label": "human"
  },
  {
    "text": "Long-term time series forecasting (LTSF) is a fundamental task with wide-ranging applications. Although Transformer-based models have made significant breakthroughs in forecasting, their effectiveness for time series forecasting remains debatable. In this paper, we revisit the significance of self-attention and propose a simple yet effective mechanism, Periodic-Nested Group Attention, namely PENGUIN. Our approach highlights the importance of explicitly modeling periodic patterns and incorporating relative attention bias for effective time series modeling. To this end, we introduce a periodic-nested relative attention bias that captures periodic structures directly. To handle multiple coexisting periodicities (e.g., daily and weekly cycles), we design a grouped attention mechanism, where each group targets a specific periodicity using a multi-query attention mechanism. Extensive experiments across diverse benchmarks demonstrate that PENGUIN consistently outperforms both MLP-based and Transformer-based models.",
    "label": "human"
  },
  {
    "text": "An ideal time series classification (TSC) should be able to capture invariant representations, but achieving reliable performance on out-of-distribution (OOD) data remains a core obstacle. This obstacle arises from the way models inherently entangle domain-specific and label-relevant features, resulting in spurious correlations. While feature disentanglement aims to solve this, current methods are largely unguided, lacking the semantic direction required to isolate truly universal features. To address this, we propose an end-to-end Energy-Regularized Information for Shift-Robustness (\\textbf{ERIS}) framework to enable guided and reliable feature disentanglement. The core idea is that effective disentanglement requires not only mathematical constraints but also semantic guidance to anchor the separation process. ERIS incorporates three key mechanisms to achieve this goal. Specifically, we first introduce an energy-guided calibration mechanism, which provides crucial semantic guidance for the separation, enabling the model to self-calibrate. Additionally, a weight-level orthogonality strategy enforces structural independence between domain-specific and label-relevant features, thereby mitigating their interference. Moreover, an auxiliary adversarial training mechanism enhances robustness by injecting structured perturbations. Experiments demonstrate that ERIS improves upon state-of-the-art baselines by an average of 4.04% accuracy across four benchmarks.",
    "label": "human"
  },
  {
    "text": "Accurate compensation of brain shift is critical for maintaining the reliability of neuronavigation during neurosurgery. While keypoint-based registration methods offer robustness to large deformations and topological changes, they typically rely on simple geometric interpolators that ignore tissue biomechanics to create dense displacement fields. In this work, we propose a novel deep learning framework that estimates dense, physically plausible brain deformations from sparse matched keypoints. We first generate a large dataset of synthetic brain deformations using biomechanical simulations. Then, a residual 3D U-Net is trained to refine standard interpolation estimates into biomechanically guided deformations. Experiments on a large set of simulated displacement fields demonstrate that our method significantly outperforms classical interpolators, reducing by half the mean square error while introducing negligible computational overhead at inference time. Code available at: \\href{https://github.com/tiago-assis/Deep-Biomechanical-Interpolator}{https://github.com/tiago-assis/Deep-Biomechanical-Interpolator}.",
    "label": "human"
  },
  {
    "text": "Background: The aim of this study was to develop and evaluate a deep learning-based automated segmentation method for hepatic anatomy (i.e., parenchyma, tumors, portal vein, hepatic vein and biliary tree) from the hepatobiliary phase of gadoxetic acid-enhanced MRI. This method should ease the clinical workflow of preoperative planning.   Methods: Manual segmentation was performed on hepatobiliary phase MRI scans from 90 consecutive patients who underwent liver surgery between January 2020 and October 2023. A deep learning network (nnU-Net v1) was trained on 72 patients with an extra focus on thin structures and topography preservation. Performance was evaluated on an 18-patient test set by comparing automated and manual segmentations using Dice similarity coefficient (DSC). Following clinical integration, 10 segmentations (assessment dataset) were generated using the network and manually refined for clinical use to quantify required adjustments using DSC.   Results: In the test set, DSCs were 0.97+/-0.01 for liver parenchyma, 0.80+/-0.04 for hepatic vein, 0.79+/-0.07 for biliary tree, 0.77+/-0.17 for tumors, and 0.74+/-0.06 for portal vein. Average tumor detection rate was 76.6+/-24.1%, with a median of one false-positive per patient. The assessment dataset showed minor adjustments were required for clinical use of the 3D models, with high DSCs for parenchyma (1.00+/-0.00), portal vein (0.98+/-0.01) and hepatic vein (0.95+/-0.07). Tumor segmentation exhibited greater variability (DSC 0.80+/-0.27). During prospective clinical use, the model detected three additional tumors initially missed by radiologists.   Conclusions: The proposed nnU-Net-based segmentation method enables accurate and automated delineation of hepatic anatomy. This enables 3D planning to be applied efficiently as a standard-of-care for every patient undergoing liver surgery.",
    "label": "human"
  },
  {
    "text": "Current code generation benchmarks focus primarily on functional correctness while overlooking two critical aspects of real-world programming: algorithmic efficiency and code quality. We introduce COMPASS (COdility's Multi-dimensional Programming ASSessment), a comprehensive evaluation framework that assesses code generation across three dimensions: correctness, efficiency, and quality. COMPASS consists of 50 competitive programming problems from real Codility competitions, providing authentic human baselines from 393,150 submissions. Unlike existing benchmarks that treat algorithmically inefficient solutions identically to optimal ones provided they pass test cases, COMPASS systematically evaluates runtime efficiency and code quality using industry-standard analysis tools. Our evaluation of three leading reasoning-enhanced models, Anthropic Claude Opus 4, Google Gemini 2.5 Pro, and OpenAI O4-Mini-High, reveals that models achieving high correctness scores do not necessarily produce efficient algorithms or maintainable code. These findings highlight the importance of evaluating more than just correctness to truly understand the real-world capabilities of code generation models. COMPASS serves as a guiding framework, charting a path for future research toward AI systems that are robust, reliable, and ready for production use.",
    "label": "human"
  },
  {
    "text": "Reinforcement Learning with Verifiable Reward (RLVR) has emerged as a powerful paradigm for unlocking reasoning capabilities in large language models, yet its full potential is hindered by two under-explored dimensions: Depth-the hardest problem a model can sample; Breadth-the number of instances consumed in a single iteration. We dissect the popular GRPO algorithm and reveal a systematic bias: the cumulative-advantage disproportionately weights samples with medium accuracy, while down-weighting the low-accuracy instances that are crucial for pushing reasoning boundaries. To rectify the depth neglect, we introduce Difficulty Adaptive Rollout Sampling (DARS), which re-weights hard problems through targeted multi-stage rollouts, thereby increasing the number of positive rollouts for hard problems. Empirically, naively enlarging rollout size only accelerates convergence and even hurts Pass@K. Our DARS, in contrast, delivers consistent Pass@K gains without extra inference cost at convergence. Just as we adaptively expanded the depth of exploration, we now ask whether aggressively scaling the breadth of training data can further amplify reasoning gains. To this end, we intensely scale batch size and replace PPO's mini-batch iterations with full-batch updates over multiple epochs. Increasing breadth significantly enhances Pass@1 performance. Large-breadth training sustains high token-level entropy, indicating continued exploration and reduced gradient noise. We further present DARS-B, which augments DARS with large breadth, and demonstrate simultaneous gains in Pass@K and Pass@1. The results confirm that breadth and adaptive exploration across depth operate as orthogonal dimensions in RLVR, which are key to unleashing the reasoning power of RLVR.",
    "label": "human"
  },
  {
    "text": "Medical Decision-Making (MDM) is a complex process requiring substantial domain-specific expertise to effectively synthesize heterogeneous and complicated clinical information. While recent advancements in Large Language Models (LLMs) show promise in supporting MDM, single-LLM approaches are limited by their parametric knowledge constraints and static training corpora, failing to robustly integrate the clinical information. To address this challenge, we propose the Expertise-aware Multi-LLM Recruitment and Collaboration (EMRC) framework to enhance the accuracy and reliability of MDM systems. It operates in two stages: (i) expertise-aware agent recruitment and (ii) confidence- and adversarial-driven multi-agent collaboration. Specifically, in the first stage, we use a publicly available corpus to construct an LLM expertise table for capturing expertise-specific strengths of multiple LLMs across medical department categories and query difficulty levels. This table enables the subsequent dynamic selection of the optimal LLMs to act as medical expert agents for each medical query during the inference phase. In the second stage, we employ selected agents to generate responses with self-assessed confidence scores, which are then integrated through the confidence fusion and adversarial validation to improve diagnostic reliability. We evaluate our EMRC framework on three public MDM datasets, where the results demonstrate that our EMRC outperforms state-of-the-art single- and multi-LLM methods, achieving superior diagnostic performance. For instance, on the MMLU-Pro-Health dataset, our EMRC achieves 74.45% accuracy, representing a 2.69% improvement over the best-performing closed-source model GPT- 4-0613, which demonstrates the effectiveness of our expertise-aware agent recruitment strategy and the agent complementarity in leveraging each LLM's specialized capabilities.",
    "label": "human"
  },
  {
    "text": "Large Vision-Language Models (LVLMs) demonstrate strong performance on single-image tasks. However, we observe that their performance degrades significantly when handling multi-image inputs. This occurs because visual cues from different images become entangled in the model's output. We refer to this phenomenon as cross-image information leakage. To address this issue, we propose FOCUS, a training-free and architecture-agnostic decoding strategy that mitigates cross-image information leakage during inference. FOCUS sequentially masks all but one image with random noise, guiding the model to focus on the single clean image. We repeat this process across all target images to obtain logits under partially masked contexts. These logits are aggregated and then contrastively refined using a noise-only reference input, which suppresses the leakage and yields more accurate outputs. FOCUS consistently improves performance across four multi-image benchmarks and diverse LVLM families. This demonstrates that FOCUS offers a general and practical solution for enhancing multi-image reasoning without additional training or architectural modifications.",
    "label": "human"
  },
  {
    "text": "Federated Learning (FL) is an emerging distributed machine learning paradigm enabling multiple clients to train a global model collaboratively without sharing their raw data. While FL enhances data privacy by design, it remains vulnerable to various security and privacy threats. This survey provides a comprehensive overview of more than 200 papers regarding the state-of-the-art attacks and defense mechanisms developed to address these challenges, categorizing them into security-enhancing and privacy-preserving techniques. Security-enhancing methods aim to improve FL robustness against malicious behaviors such as byzantine attacks, poisoning, and Sybil attacks. At the same time, privacy-preserving techniques focus on protecting sensitive data through cryptographic approaches, differential privacy, and secure aggregation. We critically analyze the strengths and limitations of existing methods, highlight the trade-offs between privacy, security, and model performance, and discuss the implications of non-IID data distributions on the effectiveness of these defenses. Furthermore, we identify open research challenges and future directions, including the need for scalable, adaptive, and energy-efficient solutions operating in dynamic and heterogeneous FL environments. Our survey aims to guide researchers and practitioners in developing robust and privacy-preserving FL systems, fostering advancements safeguarding collaborative learning frameworks' integrity and confidentiality.",
    "label": "human"
  },
  {
    "text": "Understanding what knowledge is implicitly encoded in deep learning models is essential for improving the interpretability of AI systems. This paper examines common methods to explain the knowledge encoded in word embeddings, which are core elements of large language models (LLMs). These methods typically involve mapping embeddings onto collections of human-interpretable semantic features, known as feature norms. Prior work assumes that accurately predicting these semantic features from the word embeddings implies that the embeddings contain the corresponding knowledge. We challenge this assumption by demonstrating that prediction accuracy alone does not reliably indicate genuine feature-based interpretability.   We show that these methods can successfully predict even random information, concluding that the results are predominantly determined by an algorithmic upper bound rather than meaningful semantic representation in the word embeddings. Consequently, comparisons between datasets based solely on prediction performance do not reliably indicate which dataset is better captured by the word embeddings. Our analysis illustrates that such mappings primarily reflect geometric similarity within vector spaces rather than indicating the genuine emergence of semantic properties.",
    "label": "human"
  },
  {
    "text": "The growing demand for continuous physiological monitoring and human-machine interaction in real-world settings calls for wearable platforms that are flexible, low-power, and capable of on-device intelligence. This work presents BioGAP-Ultra, an advanced multimodal biosensing platform that supports synchronized acquisition of diverse electrophysiological and hemodynamic signals such as EEG, EMG, ECG, and PPG while enabling embedded AI processing at state-of-the-art energy efficiency. BioGAP-Ultra is a major extension of our previous design, BioGAP [1], aimed at meeting the rapidly growing requirements of wearable biosensing applications. It features (i) increased on-device storage (x2 SRAM, x4 FLASH), (ii) improved wireless connectivity (1.4 Mbit/s bandwidth, x4 higher than BioGAP), (iii) enhanced number of signal modalities (from 3 to 5) and analog input channels (x2). Further, it is complemented by a complete real-time visualization and analysis software suite, providing access to raw data and real-time configurability on a mobile phone. Electrical characterization and multiple case studies confirm the platform's robustness, configurability, and suitability for real-world multimodal biosignal acquisition and edge intelligence. Finally, we demonstrate the system's versatility through integration into various wearable form factors: an EEG-PPG headband consuming 32.8 mW, an EMG sleeve at 26.7 mW, and an ECG-PPG chest band requiring only 9.3 mW, tailored for diverse biosignal applications. All hardware and software design files are also released open-source with a permissive license.",
    "label": "human"
  },
  {
    "text": "Large language model (LLM) agents-especially smaller, open-source models-often produce causally invalid or incoherent actions in collaborative tasks due to their reliance on surface-level correlations rather than grounded causal reasoning. This limitation undermines their performance in terms of coordination and planning in dynamic environments. We address this challenge with CausalPlan, a two-phase framework that integrates explicit structural causal reasoning into the LLM planning process. At the core of CausalPlan is the Structural Causal Action (SCA) model, which learns a causal graph from agent trajectories to capture how prior actions and current environment states influence future decisions. This structure is then used to guide action selection by assigning causal scores to LLM-generated proposals, reweighting them accordingly, or falling back to causally grounded alternatives when needed. By embedding this causal knowledge directly into the decision loop, CausalPlan constrains planning to intervention-consistent behaviours without requiring fine-tuning of the LLM itself. We evaluate CausalPlan on the Overcooked-AI benchmark across five multi-agent coordination tasks and four LLMs of varying sizes: Gemma-7B, Llama-8B, Qwen-14B, and Llama-70B. Experimental results show that CausalPlan consistently reduces invalid actions and improves collaboration in both AI-AI and human-AI settings, outperforming strong reinforcement learning baselines. Our findings highlight the value of causality-driven planning for deploying efficient, interpretable, and generalisable multi-agent LLM systems.",
    "label": "human"
  },
  {
    "text": "This paper evaluates the capabilities of 28 large language models (LLMs) to reason with 20 defeasible reasoning patterns involving generic generalizations (e.g., 'Birds fly', 'Ravens are black') central to non-monotonic logic. Generics are of special interest to linguists, philosophers, logicians, and cognitive scientists because of their complex exception-permitting behaviour and their centrality to default reasoning, cognition, and concept acquisition. We find that while several frontier models handle many default reasoning problems well, performance varies widely across models and prompting styles. Few-shot prompting modestly improves performance for some models, but chain-of-thought (CoT) prompting often leads to serious performance degradation (mean accuracy drop -11.14%, SD 15.74% in models performing above 75% accuracy in zero-shot condition, temperature 0). Most models either struggle to distinguish between defeasible and deductive inference or misinterpret generics as universal statements. These findings underscore both the promise and limits of current LLMs for default reasoning.",
    "label": "human"
  },
  {
    "text": "As AI systems become more capable, integrated, and widespread, understanding the associated risks becomes increasingly important. This paper maps the full spectrum of AI risks, from current harms affecting individual users to existential threats that could endanger humanity's survival. We organize these risks into three main causal categories. Misuse risks, which occur when people deliberately use AI for harmful purposes - creating bioweapons, launching cyberattacks, adversarial AI attacks or deploying lethal autonomous weapons. Misalignment risks happen when AI systems pursue outcomes that conflict with human values, irrespective of developer intentions. This includes risks arising through specification gaming (reward hacking), scheming and power-seeking tendencies in pursuit of long-term strategic goals. Systemic risks, which arise when AI integrates into complex social systems in ways that gradually undermine human agency - concentrating power, accelerating political and economic disempowerment, creating overdependence that leads to human enfeeblement, or irreversibly locking in current values curtailing future moral progress. Beyond these core categories, we identify risk amplifiers - competitive pressures, accidents, corporate indifference, and coordination failures - that make all risks more likely and severe. Throughout, we connect today's existing risks and empirically observable AI behaviors to plausible future outcomes, demonstrating how existing trends could escalate to catastrophic outcomes. Our goal is to help readers understand the complete landscape of AI risks. Good futures are possible, but they don't happen by default. Navigating these challenges will require unprecedented coordination, but an extraordinary future awaits if we do.",
    "label": "human"
  },
  {
    "text": "We contribute a theoretical and operational framework for neurosymbolic AI called DeepLog. DeepLog introduces building blocks and primitives for neurosymbolic AI that make abstraction of commonly used representations and computational mechanisms used in neurosymbolic AI. DeepLog can represent and emulate a wide range of neurosymbolic systems. It consists of two key components. The first is the DeepLog language for specifying neurosymbolic models and inference tasks. This language consists of an annotated neural extension of grounded first-order logic, and makes abstraction of the type of logic, e.g. boolean, fuzzy or probabilistic, and whether logic is used in the architecture or in the loss function. The second DeepLog component is situated at the computational level and uses extended algebraic circuits as computational graphs. Together these two components are to be considered as a neurosymbolic abstract machine, with the DeepLog language as the intermediate level of abstraction and the circuits level as the computational one. DeepLog is implemented in software, relies on the latest insights in implementing algebraic circuits on GPUs, and is declarative in that it is easy to obtain different neurosymbolic models by making different choices for the underlying algebraic structures and logics. The generality and efficiency of the DeepLog neurosymbolic machine is demonstrated through an experimental comparison between 1) different fuzzy and probabilistic logics, 2) between using logic in the architecture or in the loss function, and 3) between a standalone CPU-based implementation of a neurosymbolic AI system and a DeepLog GPU-based one.",
    "label": "human"
  },
  {
    "text": "The aspiration for artificial general intelligence, fueled by the rapid progress of multimodal models, demands human-comparable performance across diverse environments. We propose HumanPCR, an evaluation suite for probing MLLMs' capacity about human-related visual contexts across three hierarchical levels: Perception, Comprehension, and Reasoning (denoted by Human-P, Human-C, and Human-R, respectively). Human-P and Human-C feature over 6,000 human-verified multiple choice questions, assessing massive tasks of 9 dimensions, including but not limited to essential skills frequently overlooked by existing benchmarks. Human-R offers a challenging manually curated video reasoning test that requires integrating multiple visual evidences, proactively extracting context beyond question cues, and applying human-like expertise. Each question includes human-annotated Chain-of-Thought (CoT) rationales with key visual evidence to support further research. Extensive evaluations on over 30 state-of-the-art models exhibit significant challenges in human-centric visual understanding, particularly in tasks involving detailed space perception, temporal understanding, and mind modeling. Moreover, analysis of Human-R reveals the struggle of models in extracting essential proactive visual evidence from diverse human scenes and their faulty reliance on query-guided retrieval. Even with advanced techniques like scaling visual contexts and test-time thinking yield only limited benefits. We hope HumanPCR and our findings will advance the development, evaluation, and human-centric application of multimodal models.",
    "label": "human"
  },
  {
    "text": "Cancerous tissues beneath healthy tissues were experimentally identified by using circularly polarized light scattering. This method enabled the changes in the size of the cell nuclei within the penetration depth in tissue to be investigated. Artificial unexposed cancerous tissues were prepared that consisted of healthy/cancerous/healthy layers with various thicknesses of the topmost healthy layer and the cancerous layer. A polarization imaging camera with a quarter-wave plate was used to create distribution images of the circular polarization of the scattered light, which indicated the presence and depth of cancerous tissues that had crept under the epithelium. These findings will lead to the development of a non-invasive optical diagnostic method for early-stage cervical cancer.",
    "label": "human"
  },
  {
    "text": "Large Language Models (LLMs) have shown promising results across various tasks, yet their reasoning capabilities remain a fundamental challenge. Developing AI systems with strong reasoning capabilities is regarded as a crucial milestone in the pursuit of Artificial General Intelligence (AGI) and has garnered considerable attention from both academia and industry. Various techniques have been explored to enhance the reasoning capabilities of LLMs, with neuro-symbolic approaches being a particularly promising way. This paper comprehensively reviews recent developments in neuro-symbolic approaches for enhancing LLM reasoning. We first present a formalization of reasoning tasks and give a brief introduction to the neurosymbolic learning paradigm. Then, we discuss neuro-symbolic methods for improving the reasoning capabilities of LLMs from three perspectives: Symbolic->LLM, LLM->Symbolic, and LLM+Symbolic. Finally, we discuss several key challenges and promising future directions. We have also released a GitHub repository including papers and resources related to this survey: https://github.com/LAMDASZ-ML/Awesome-LLM-Reasoning-with-NeSy.",
    "label": "human"
  },
  {
    "text": "To maintain the company's talent pool, recruiters need to continuously search for resumes from third-party websites (e.g., LinkedIn, Indeed). However, fetched resumes are often incomplete and inaccurate. To improve the quality of third-party resumes and enrich the company's talent pool, it is essential to conduct duplication detection between the fetched resumes and those already in the company's talent pool. Such duplication detection is challenging due to the semantic complexity, structural heterogeneity, and information incompleteness of resume texts. To this end, we propose MHSNet, an multi-level identity verification framework that fine-tunes BGE-M3 using contrastive learning. With the fine-tuned , Mixture-of-Experts (MoE) generates multi-level sparse and dense representations for resumes, enabling the computation of corresponding multi-level semantic similarities. Moreover, the state-aware Mixture-of-Experts (MoE) is employed in MHSNet to handle diverse incomplete resumes. Experimental results verify the effectiveness of MHSNet",
    "label": "human"
  },
  {
    "text": "Knowledge Graphs are used for various purposes, including business applications, biomedical analyses, or digital twins in industry 4.0. In this paper, we investigate knowledge graphs describing household actions, which are beneficial for controlling household robots and analyzing video footage. In the latter case, the information extracted from videos is notoriously incomplete, and completing the knowledge graph for enhancing the situational picture is essential. In this paper, we show that, while a standard link prediction problem, situational knowledge graphs have special characteristics that render many link prediction algorithms not fit for the job, and unable to outperform even simple baselines.",
    "label": "human"
  },
  {
    "text": "Spiking Neural Networks (SNNs) are promising brain-inspired models known for low power consumption and superior potential for temporal processing, but identifying suitable learning mechanisms remains a challenge. Despite the presence of multiple coexisting learning strategies in the brain, current SNN training methods typically rely on a single form of synaptic plasticity, which limits their adaptability and representational capability. In this paper, we propose a biologically inspired training framework that incorporates multiple synergistic plasticity mechanisms for more effective SNN training. Our method enables diverse learning algorithms to cooperatively modulate the accumulation of information, while allowing each mechanism to preserve its own relatively independent update dynamics. We evaluated our approach on both static image and dynamic neuromorphic datasets to demonstrate that our framework significantly improves performance and robustness compared to conventional learning mechanism models. This work provides a general and extensible foundation for developing more powerful SNNs guided by multi-strategy brain-inspired learning.",
    "label": "human"
  },
  {
    "text": "Explainable Artificial Intelligence (XAI) methods, such as Local Interpretable Model-Agnostic Explanations (LIME), have advanced the interpretability of black-box machine learning models by approximating their behavior locally using interpretable surrogate models. However, LIME's inherent randomness in perturbation and sampling can lead to locality and instability issues, especially in scenarios with limited training data. In such cases, data scarcity can result in the generation of unrealistic variations and samples that deviate from the true data manifold. Consequently, the surrogate model may fail to accurately approximate the complex decision boundary of the original model. To address these challenges, we propose a novel Instance-based Transfer Learning LIME framework (ITL-LIME) that enhances explanation fidelity and stability in data-constrained environments. ITL-LIME introduces instance transfer learning into the LIME framework by leveraging relevant real instances from a related source domain to aid the explanation process in the target domain. Specifically, we employ clustering to partition the source domain into clusters with representative prototypes. Instead of generating random perturbations, our method retrieves pertinent real source instances from the source cluster whose prototype is most similar to the target instance. These are then combined with the target instance's neighboring real instances. To define a compact locality, we further construct a contrastive learning-based encoder as a weighting mechanism to assign weights to the instances from the combined set based on their proximity to the target instance. Finally, these weighted source and target instances are used to train the surrogate model for explanation purposes.",
    "label": "human"
  },
  {
    "text": "Tracking single fluorescent molecules has offered resolution into dynamic molecular processes at the single-molecule level. This perspective traces the evolution of single-molecule tracking, highlighting key developments across various methodological branches within fluorescence microscopy. We compare the strengths and limitations of each approach, ranging from conventional widefield offline tracking to real-time confocal tracking. In the final section, we explore emerging efforts to advance physics-inspired tracking techniques, a possibility for parallelization and artificial intelligence, and discuss challenges and opportunities they present toward achieving higher spatiotemporal resolution and greater computational and data efficiency in next-generation single-molecule studies.",
    "label": "human"
  },
  {
    "text": "Source code is usually formatted with elements like indentation and newlines to improve readability for human developers. However, these visual aids do not seem to be beneficial for large language models (LLMs) in the same way since the code is processed as a linear sequence of tokens. Furthermore, these additional tokens can lead to increased computational costs and longer response times for LLMs. If such formatting elements are non-essential to LLMs, we can reduce such costs by removing them from the code. To figure out the role played by formatting elements, we conduct a comprehensive empirical study to evaluate the impact of code formatting on LLM performance and efficiency. Through large-scale experiments on Fill-in-the-Middle Code Completion tasks across four programming languages (Java, Python, C++, C\\#) and ten LLMs-including both commercial and open-source models-we systematically analyze token count and performance when formatting elements are removed. Key findings indicate that LLMs can maintain performance across formatted code and unformatted code, achieving an average input token reduction of 24.5\\% with negligible output token reductions. This makes code format removal a practical optimization strategy for improving LLM efficiency. Further exploration reveals that both prompting and fine-tuning LLMs can lead to significant reductions (up to 36.1\\%) in output code length without compromising correctness. To facilitate practical applications, we develop a bidirectional code transformation tool for format processing, which can be seamlessly integrated into existing LLM inference workflows, ensuring both human readability and LLM efficiency.",
    "label": "human"
  },
  {
    "text": "We propose an improved algorithm by identifying and encouraging cooperative behavior in multi-agent environments. First, we analyze the shortcomings of existing algorithms in addressing multi-agent reinforcement learning problems. Then, based on the existing algorithm MADDPG, we introduce a new parameter to increase the reward that an agent can obtain when cooperative behavior among agents is identified. Finally, we compare our improved algorithm with MADDPG in environments from PettingZoo. The results show that the new algorithm helps agents achieve both higher team rewards and individual rewards.",
    "label": "human"
  },
  {
    "text": "Methods for query answering over incomplete knowledge graphs retrieve entities that are likely to be answers, which is particularly useful when such answers cannot be reached by direct graph traversal due to missing edges. However, existing approaches have focused on queries formalized using first-order-logic. In practice, many real-world queries involve constraints that are inherently vague or context-dependent, such as preferences for attributes or related categories. Addressing this gap, we introduce the problem of query answering with soft constraints. We propose a Neural Query Reranker (NQR) designed to adjust query answer scores by incorporating soft constraints without disrupting the original answers to a query. NQR operates interactively, refining answers based on incremental examples of preferred and non-preferred entities. We extend existing QA benchmarks by generating datasets with soft constraints. Our experiments demonstrate that NQR can capture soft constraints while maintaining robust query answering performance.",
    "label": "human"
  },
  {
    "text": "Combined Algorithm Selection and Hyperparameter Optimization (CASH) has been fundamental to traditional AutoML systems. However, with the advancements of pre-trained models, modern ML workflows go beyond hyperparameter optimization and often require fine-tuning, ensembling, and other adaptation techniques. While the core challenge of identifying the best-performing model for a downstream task remains, the increasing heterogeneity of ML pipelines demands novel AutoML approaches. This work extends the CASH framework to select and adapt modern ML pipelines. We propose PS-PFN to efficiently explore and exploit adapting ML pipelines by extending Posterior Sampling (PS) to the max k-armed bandit problem setup. PS-PFN leverages prior-data fitted networks (PFNs) to efficiently estimate the posterior distribution of the maximal value via in-context learning. We show how to extend this method to consider varying costs of pulling arms and to use different PFNs to model reward distributions individually per arm. Experimental results on one novel and two existing standard benchmark tasks demonstrate the superior performance of PS-PFN compared to other bandit and AutoML strategies. We make our code and data available at https://github.com/amirbalef/CASHPlus.",
    "label": "human"
  },
  {
    "text": "Current Large Language Models (LLMs) are usually post-trained on large-scale carefully curated datasets (data & training scaling) and doing reasoning in test time (inference time scaling). In this work, we present a new scaling paradigm, Input Time Scaling, to complement previous scaling methods by putting resources on queries (input time). During training and testing, we combine meta-knowledge from LLMs to refine inputs with different strategies. We also find a new phenomenon, training-testing co-design there. We need to apply query strategies during both training and testing. Only applying strategies on training or testing would seriously degrade the performance. We are also surprised to find that seemingly low data quality datasets can gain high performance. Adding irrelevant information to the queries, randomly selecting examples from a minimally filtered dataset, can even perform the best. These findings contradict the widely held inductive bias, \"garbage in, garbage out\". Curating datasets with seemingly high-quality data can even potentially limit the performance ceiling. In addition, models trained on more data with similar quality (15k VS 1k) perform worse, simple dataset size scaling should also be carefully inspected. The good news is that our findings are compatible with the Less is More phenomenon. A small set of examples is enough to evoke high-level reasoning ability. With experiments on models trained on Qwen2.5-32B-Instruct, we are able to reach SOTA performance among 32B models on AIME24(76.7%) and AIME25(76.7%) pass@1. We can further achieve AIME24(76.7%) and AIME25(80%) with a majority vote of three models. Starting from DeepSeek-R1-Distill-Qwen-32B, the best result would be 86.7% on AIME24 and 76.7% on AIME25. To facilitate reproducibility and further research, we are working on open-source our datasets, data pipelines, evaluation results, and checkpoints.",
    "label": "human"
  },
  {
    "text": "Training modern neural networks on large datasets is computationally and environmentally costly. We introduce GRAFT, a scalable in-training subset selection method that (i) extracts a low-rank feature representation for each batch, (ii) applies a Fast MaxVol sampler to select a small, diverse subset that spans the batch's dominant subspace, and (iii) dynamically adjusts the subset size using a gradient-approximation criterion. By operating in low-rank subspaces and training on carefully chosen examples instead of full batches, GRAFT preserves the training trajectory while reducing wall-clock time, energy consumption, and $\\mathrm{CO}_2$ emissions. Across multiple benchmarks, GRAFT matches or exceeds recent selection baselines in both accuracy and efficiency, providing a favorable trade-off between accuracy, efficiency, and emissions.",
    "label": "human"
  },
  {
    "text": "Efficient task offloading is crucial for reducing latency and ensuring timely decision-making in intelligent transportation systems within the rapidly evolving Internet of Vehicles (IoV) landscape. This paper introduces a novel Quantum-Inspired Artificial Bee Colony (QABC) algorithm specifically designed for latency-sensitive task offloading involving cloud servers, Roadside Units (RSUs), and vehicular nodes. By incorporating principles from quantum computing, such as quantum state evolution and probabilistic encoding, QABC enhances the classical Artificial Bee Colony (ABC) algorithm's ability to avoid local optima and explore high-dimensional solution spaces. This research highlights the potential of quantum-inspired heuristics to optimize real-time offloading strategies in future vehicular networks.",
    "label": "human"
  },
  {
    "text": "Precise localization of GUI elements is crucial for the development of GUI agents. Traditional methods rely on bounding box or center-point regression, neglecting spatial interaction uncertainty and visual-semantic hierarchies. Recent methods incorporate attention mechanisms but still face two key issues: (1) ignoring processing background regions causes attention drift from the desired area, and (2) uniform labeling fails to distinguish between center and edges of the target UI element, leading to click imprecision. Inspired by how humans visually process and interact with GUI elements, we propose the Valley-to-Peak (V2P) method to address these issues. To mitigate background distractions, V2P introduces a suppression attention mechanism that minimizes the model's focus on irrelevant regions to highlight the intended region. For the issue of center-edge distinction, V2P applies a Fitts' Law-inspired approach by modeling GUI interactions as 2D Gaussian heatmaps where the weight gradually decreases from the center towards the edges. The weight distribution follows a Gaussian function, with the variance determined by the target's size. Consequently, V2P effectively isolates the target area and teaches the model to concentrate on the most essential point of the UI element. The model trained by V2P achieves the performance with 92.3% and 50.5% on two benchmarks ScreenSpot-v2 and ScreenSpot-Pro. Ablations further confirm each component's contribution, highlighting V2P's generalizability for precise GUI grounding tasks.",
    "label": "human"
  },
  {
    "text": "This systematic review critically evaluates publicly available abdominal CT datasets and their suitability for artificial intelligence (AI) applications in clinical settings. We examined 46 publicly available abdominal CT datasets (50,256 studies). Across all 46 datasets, we found substantial redundancy (59.1\\% case reuse) and a Western/geographic skew (75.3\\% from North America and Europe). A bias assessment was performed on the 19 datasets with >=100 cases; within this subset, the most prevalent high-risk categories were domain shift (63\\%) and selection bias (57\\%), both of which may undermine model generalizability across diverse healthcare environments -- particularly in resource-limited settings. To address these challenges, we propose targeted strategies for dataset improvement, including multi-institutional collaboration, adoption of standardized protocols, and deliberate inclusion of diverse patient populations and imaging technologies. These efforts are crucial in supporting the development of more equitable and clinically robust AI models for abdominal imaging.",
    "label": "human"
  },
  {
    "text": "Large models, renowned for superior performance, outperform smaller ones even without billion-parameter scales. While mobile network servers have ample computational resources to support larger models than client devices, privacy constraints prevent clients from directly sharing their raw data. Federated Learning (FL) enables decentralized clients to collaboratively train a shared model by exchanging model parameters instead of transmitting raw data. Yet, it requires a uniform model architecture and multiple communication rounds, which neglect resource heterogeneity, impose heavy computational demands on clients, and increase communication overhead. To address these challenges, we propose FedOL, to construct a larger and more comprehensive server model in one-shot settings (i.e., in a single communication round). Instead of model parameter sharing, FedOL employs knowledge distillation, where clients only exchange model prediction outputs on an unlabeled public dataset. This reduces communication overhead by transmitting compact predictions instead of full model weights and enables model customization by allowing heterogeneous model architectures. A key challenge in this setting is that client predictions may be biased due to skewed local data distributions, and the lack of ground-truth labels in the public dataset further complicates reliable learning. To mitigate these issues, FedOL introduces a specialized objective function that iteratively refines pseudo-labels and the server model, improving learning reliability. To complement this, FedOL incorporates a tailored pseudo-label generation and knowledge distillation strategy that effectively integrates diverse knowledge. Simulation results show that FedOL significantly outperforms existing baselines, offering a cost-effective solution for mobile networks where clients possess valuable private data but limited computational resources.",
    "label": "human"
  },
  {
    "text": "Recent Mamba-based models have shown promise in speech enhancement by efficiently modeling long-range temporal dependencies. However, models like Speech Enhancement Mamba (SEMamba) remain limited to single-speaker scenarios and struggle in complex multi-speaker environments such as the cocktail party problem. To overcome this, we introduce AVSEMamba, an audio-visual speech enhancement model that integrates full-face visual cues with a Mamba-based temporal backbone. By leveraging spatiotemporal visual information, AVSEMamba enables more accurate extraction of target speech in challenging conditions. Evaluated on the AVSEC-4 Challenge development and blind test sets, AVSEMamba outperforms other monaural baselines in speech intelligibility (STOI), perceptual quality (PESQ), and non-intrusive quality (UTMOS), and achieves \\textbf{1st place} on the monaural leaderboard.",
    "label": "human"
  },
  {
    "text": "Causal inference often hinges on strong assumptions - such as no unmeasured confounding or perfect compliance - that are rarely satisfied in practice. Partial identification offers a principled alternative: instead of relying on unverifiable assumptions to estimate causal effects precisely, it derives bounds that reflect the uncertainty inherent in the data. Despite its theoretical appeal, partial identification remains underutilized in applied work, in part due to the fragmented nature of existing methods and the lack of practical guidance. This thesis addresses these challenges by systematically comparing a diverse set of bounding algorithms across multiple causal scenarios. We implement, extend, and unify state-of-the-art methods - including symbolic, optimization-based, and information-theoretic approaches - within a common evaluation framework. In particular, we propose an extension of a recently introduced entropy-bounded method, making it applicable to counterfactual queries such as the Probability of Necessity and Sufficiency (PNS). Our empirical study spans thousands of randomized simulations involving both discrete and continuous data-generating processes. We assess each method in terms of bound tightness, computational efficiency, and robustness to assumption violations. To support practitioners, we distill our findings into a practical decision tree for algorithm selection and train a machine learning model to predict the best-performing method based on observable data characteristics.   All implementations are released as part of an open-source Python package, CausalBoundingEngine, which enables users to apply and compare bounding methods through a unified interface.",
    "label": "human"
  },
  {
    "text": "Document Visual Question Answering (Document VQA) faces significant challenges when processing long documents in low-resource environments due to context limitations and insufficient training data. This paper presents AdaDocVQA, a unified adaptive framework addressing these challenges through three core innovations: a hybrid text retrieval architecture for effective document segmentation, an intelligent data augmentation pipeline that automatically generates high-quality reasoning question-answer pairs with multi-level verification, and adaptive ensemble inference with dynamic configuration generation and early stopping mechanisms. Experiments on Japanese document VQA benchmarks demonstrate substantial improvements with 83.04\\% accuracy on Yes/No questions, 52.66\\% on factual questions, and 44.12\\% on numerical questions in JDocQA, and 59\\% accuracy on LAVA dataset. Ablation studies confirm meaningful contributions from each component, and our framework establishes new state-of-the-art results for Japanese document VQA while providing a scalable foundation for other low-resource languages and specialized domains. Our code available at: https://github.com/Haoxuanli-Thu/AdaDocVQA.",
    "label": "human"
  },
  {
    "text": "Effective climate policy requires integrating mitigation, adaptation, and equitable governance to address the multifaceted risks of anthropogenic warming. Policies that combine carbon pricing with targeted investments in low-carbon infrastructure create incentives for emissions reductions while mitigating distributional impacts. Adaptation strategies — such as resilient urban planning, climate-informed agriculture, and ecosystem-based approaches — reduce vulnerability to extreme events and slow-onset changes. Crucially, governance structures must incorporate transparent decision-making, inclusive stakeholder engagement, and mechanisms for accountability to ensure legitimacy and compliance. International cooperation remains essential, given the global nature of greenhouse gas externalities, but multilevel approaches that empower cities and regions can accelerate implementation. Finally, iterative policy design, supported by monitoring and evidence synthesis, helps policymakers adjust interventions in response to emergent scientific understanding and socio-economic shifts.",
    "label": "ai"
  },
  {
    "text": "The integration of educational technology into formal schooling has the potential to personalize learning trajectories and expand access to pedagogical resources. Adaptive learning platforms leverage analytics to identify learners’ misconceptions and offer targeted remediation, which can improve learning efficiency when aligned with sound instructional design. However, effective implementation depends on teacher professional development, equitable access to devices and connectivity, and attention to data privacy. Evidence from randomized trials indicates that technology interventions are most effective when they complement high-quality classroom instruction rather than replace it. Policy frameworks should therefore prioritize blended models, continuous evaluation, and infrastructure investments that reduce digital divides. Furthermore, ethical considerations related to algorithmic bias and students’ data rights must be foregrounded in governance regimes for educational technologies.",
    "label": "ai"
  },
  {
    "text": "Maximizing vaccination uptake requires multifaceted strategies that combine evidence-based communication, community engagement, and accessible delivery systems. Behavioral interventions that leverage trusted local messengers and frame benefits in culturally relevant narratives increase acceptance more than top-down mandates alone. Supply-side factors such as clinic hours, transportation, and integration with routine health services influence convenience and thereby coverage. Surveillance systems and real-time data analytics enable targeted outreach to under-vaccinated populations and rapid identification of coverage gaps. Policy instruments — including incentives, school-entry requirements, and mobile clinics — must be implemented alongside transparent monitoring to preserve public trust. Ultimately, sustained investments in primary health care infrastructure and community partnerships are foundational to resilient immunization programs.",
    "label": "ai"
  },
  {
    "text": "Ethical governance of artificial intelligence requires aligning design, deployment, and oversight with principles of fairness, transparency, and accountability. Technical measures such as algorithmic auditing, bias mitigation, and explainable models can reduce harm but are insufficient without institutional safeguards. Regulatory strategies should mandate impact assessments for high-risk applications, require provenance documentation of training data, and establish redress mechanisms for affected individuals. Multistakeholder participation, including civil society and domain experts, enhances normative legitimacy and helps surface contextual harms that purely technical assessments may miss. International coordination is necessary to harmonize standards while preserving regulatory flexibility for localized conditions. Lastly, investment in education and workforce transition programs can mitigate socio-economic disruptions arising from automation.",
    "label": "ai"
  },
  {
    "text": "Universal basic income (UBI) proposals are grounded in objectives of poverty reduction, income security, and simplification of welfare systems. Empirical pilots have demonstrated heterogeneous effects on labor supply, consumption, and well-being, often contingent on program design, benefit size, and local labor market conditions. Proponents argue that UBI can reduce administrative costs and provide a safety net in the face of technological displacement, while critics highlight fiscal sustainability concerns and potential labor supply distortions. Hybrid schemes, such as negative income tax models or targeted basic income for specific demographics, may reconcile distributional goals with budgetary constraints. Rigorous evaluation using randomized trials and quasi-experimental methods is essential to quantify trade-offs and inform policy calibration.",
    "label": "ai"
  },
  {
    "text": "Memory consolidation is a process through which labile memory traces are stabilized and integrated into long-term storage, involving coordinated neural activity across hippocampal and neocortical networks. Experimental evidence implicates replay during sleep and offline rest as mechanisms that promote synaptic plasticity and systems-level reorganization. Molecular processes, including protein synthesis and neuromodulatory signaling, interact with network dynamics to facilitate consolidation. Computational models suggest that complementary learning systems reduce catastrophic interference by segregating rapid hippocampal encoding and slower cortical integration. Understanding the temporal interplay between encoding, consolidation, and retrieval has implications for educational practice and clinical interventions for memory disorders. Future research should integrate multi-scale measurements and causal manipulations to delineate mechanistic pathways.",
    "label": "ai"
  },
  {
    "text": "Transit-oriented development (TOD) promotes compact, mixed-use neighborhoods centered on high-quality public transit to reduce car dependency and support sustainable urban growth. TOD strategies include increased residential density near transit nodes, pedestrian-friendly street design, and zoning reforms that allow for mixed land uses. Empirical studies show that well-implemented TOD can reduce vehicle miles traveled, improve air quality, and enhance social inclusion by expanding access to jobs and services. However, without complementary affordable housing policies, TOD may precipitate displacement and gentrification. Successful TOD requires coordination across transportation agencies, land-use planners, and housing authorities, along with financing mechanisms to support infrastructure and social equity measures.",
    "label": "ai"
  },
  {
    "text": "CRISPR-based gene editing has transformed biomedical research and holds promise for therapeutic interventions, agricultural improvement, and basic science inquiry. The technology’s precision and scalability enable targeted modification of genomic sequences, but off-target effects and delivery challenges remain central technical barriers. Ethical considerations are paramount when contemplating germline editing due to heritability and potential societal impacts. Regulatory frameworks should balance innovation with robust oversight, transparent risk assessment, and public engagement. In agriculture, gene editing can accelerate development of disease-resistant crops and improve nutritional profiles, yet governance must address ecological risks and intellectual property concerns. Interdisciplinary research combining molecular biology, bioethics, and policy analysis is critical to responsible translation.",
    "label": "ai"
  },
  {
    "text": "Social capital refers to networks, norms, and trust that facilitate coordination and cooperation for mutual benefit within communities. High levels of social capital are associated with improved civic participation, collective problem-solving, and dissemination of information, which can bolster resilience to economic and environmental shocks. However, social capital can also produce exclusionary practices or reinforce unequal power structures if networks are homogenous and insular. Measurement approaches range from survey-based indicators of trust and reciprocity to analysis of organizational membership and network structure. Policy interventions that strengthen inclusive civic institutions, foster diverse associational life, and lower barriers to participation are likely to yield broad social returns while minimizing exclusivity risks.",
    "label": "ai"
  },
  {
    "text": "Biodiversity conservation requires integrated strategies that protect habitats, mitigate drivers of species loss, and engage local communities in stewardship. Protected areas are foundational but insufficient alone; conservationists must address habitat fragmentation, invasive species, pollution, and climate change. Payment for ecosystem services and community-based conservation programs can align local incentives with biodiversity objectives, though careful design is required to ensure equitable benefit-sharing. Landscape-level planning that maintains connectivity and genetic flow is essential for long-term species persistence. Monitoring frameworks that combine remote sensing with field-based biodiversity assessment support adaptive management. Cross-sectoral policies that mainstream biodiversity considerations into agriculture, infrastructure, and urban development enhance policy coherence and conservation outcomes.",
    "label": "ai"
  },
  {
    "text": "The Industrial Revolution initiated a profound transformation of production processes, labor relations, and urbanization patterns beginning in the late eighteenth century. Mechanization, the expansion of factory systems, and advancements in energy harnessing accelerated output and reshaped social organization. These changes generated productivity gains and economic growth but also produced new forms of labor exploitation, spatial segregation, and public health challenges in rapidly expanding cities. Technological diffusion varied regionally, contingent on capital markets, institutional structures, and access to resources. Historical inquiry highlights how policy interventions — such as labor regulations, public health reforms, and infrastructure investments — mediated the social costs of industrialization while enabling long-term improvements in living standards.",
    "label": "ai"
  },
  {
    "text": "The ethics of care emphasizes relational obligations, contextual judgment, and attentiveness to the needs of particular individuals, contrasting with abstract principles that dominate deontological and utilitarian frameworks. Rooted in feminist philosophical critiques, care ethics foregrounds how moral reasoning emerges from situated practices and emotional bonds rather than purely impartial calculation. Policy implications include prioritizing social investments in caregiving infrastructure, recognizing unpaid care work in economic metrics, and designing public services that support interdependence. Critics caution that care ethics must avoid essentializing gendered roles or limiting autonomy; proponents respond that a refined care framework can incorporate reciprocity and empowerment alongside responsibility. Integrating care ethics into public discourse enriches moral deliberation about social welfare and justice.",
    "label": "ai"
  },
  {
    "text": "Data privacy governance must reconcile the utility of large-scale data analytics with individuals’ rights to control personal information. Regulatory approaches—such as purpose limitation, data minimization, and informed consent—establish foundational principles, but enforcement mechanisms and technological safeguards determine practical protection. Technical strategies like differential privacy and federated learning can reduce re-identification risks while preserving analytic value, yet algorithmic transparency and provenance documentation are necessary to build trust. Cross-border data flows introduce jurisdictional complexity that calls for interoperable standards and international cooperation. Importantly, marginalized populations often face disproportionate privacy harms, so policy design should incorporate equity analyses and mechanisms for redress.",
    "label": "ai"
  },
  {
    "text": "Sustainable intensification aims to increase agricultural productivity while minimizing environmental impacts through improved resource use efficiency and ecosystem stewardship. Practices such as precision agriculture, integrated pest management, and conservation tillage can raise yields without proportional increases in land conversion. Agroecological approaches that promote biodiversity and soil health contribute to resilience against climatic variability. Policy instruments, including incentives for adoption, research-extension linkages, and secure land tenure, influence the scalability of sustainable intensification. Equally important are market structures and value chains that reward sustainable practices. Evaluations should consider social outcomes—such as smallholder livelihoods and labor demands—alongside biophysical indicators to ensure that intensification is equitable and sustainable.",
    "label": "ai"
  },
  {
    "text": "Decision making under risk involves trade-offs where outcomes are probabilistic and individuals must weigh expected benefits against uncertainty. Prospect theory documents systematic departures from expected utility maximization, including loss aversion and probability weighting, which shape behavioral responses in financial and health domains. Contextual framing, emotional states, and time preferences further modulate choices. Policy applications—such as default options in retirement savings or framing effects in public health messaging—leverage behavioral insights to improve welfare-aligned outcomes. Experimental methods and field interventions provide evidence on heterogeneity of responses across populations. Understanding cognitive and affective mechanisms underlying risk behavior can inform the design of institutions and interventions that mitigate biases without undermining autonomy.",
    "label": "ai"
  },
  {
    "text": "Advances in battery technology are central to decarbonizing transport and enabling renewable energy integration. Research on novel electrode materials, solid electrolytes, and cell architectures seeks to improve energy density, cycle life, and safety while reducing reliance on scarce critical minerals. Systems-level innovation includes second-life applications, recycling technologies, and supply-chain resilience strategies to address environmental and geopolitical risks. Lifecycle assessment frameworks help quantify trade-offs between performance improvements and material or energy inputs. Policy measures—such as research funding, standards for durability, and incentives for recycling—can accelerate market adoption of sustainable battery solutions. Collaboration across materials science, manufacturing, and policy domains is essential to realize scalable, low-impact energy storage technologies.",
    "label": "ai"
  },
  {
    "text": "Intellectual property (IP) regimes balance incentives for innovation against the social costs of restricted access to knowledge and technologies. Patents can spur investment in research and development but may also create monopolistic rents and slow follow-on innovation when applied too broadly or for excessive durations. Alternative models—such as prize funds, patent pools, and open licensing—offer mechanisms to realign incentives with public-interest objectives. The design of IP policy should be sensitive to sectoral differences: pharmaceutical innovation responds differently to exclusivity than software or foundational research. Empirical evaluation of IP impact on diffusion, access, and social welfare informs normative choices. International IP governance requires mechanisms to address global inequities in innovation capacity and technology transfer.",
    "label": "ai"
  },
  {
    "text": "Causal inference seeks to estimate the effect of interventions by accounting for confounding, selection bias, and measurement error. Randomized controlled trials provide a gold standard for causal identification, but observational studies necessitate careful design and analytic strategies such as propensity score methods, instrumental variables, and regression discontinuity designs. Structural models and potential outcomes frameworks clarify assumptions required for identification and guide sensitivity analyses. Transparent reporting of estimands, identification assumptions, and robustness checks enhances credibility. Moreover, triangulation across complementary methods and replication in diverse settings strengthens causal claims. Advances in machine learning provide flexible estimators for causal parameters but require integration with principled identification strategies to avoid spurious inference.",
    "label": "ai"
  },
  {
    "text": "Integrating large shares of variable renewable energy into electricity systems necessitates investments in grid flexibility, storage, and demand-side management. Market designs that value flexibility—through capacity markets, ancillary service procurement, or dynamic pricing—can incentivize technologies that balance supply variability. Transmission expansion and regional coordination enable spatial balancing of resource variability and access to diverse generation portfolios. Regulatory reforms should streamline permitting for grid upgrades and storage while ensuring consumer protections. Additionally, coordinated planning that considers electrification of transport and heating can harness synergies between sectors. Equity considerations require policies to mitigate disproportionate cost burdens and ensure benefits reach vulnerable communities.",
    "label": "ai"
  },
  {
    "text": "Performance measurement in public administration supports accountability, strategic planning, and continuous improvement when metrics are thoughtfully designed and used in a balanced manner. Indicators should align with organizational objectives, be sensitive to context, and combine quantitative and qualitative evidence. Excessive reliance on narrow metrics risks gaming and mission distortion, so performance frameworks must include safeguards such as composite indicators, stakeholder review, and periodic audits. Data infrastructure and institutional capacity for analysis are prerequisites for meaningful measurement. Moreover, participatory approaches that involve frontline staff and service users can surface relevant performance dimensions and foster ownership. Embedding evaluation culture and providing feedback loops enables adaptive management and better service delivery.",
    "label": "ai"
  },
  {
    "text": "Drug repurposing utilizes existing pharmacological compounds for new therapeutic indications, offering cost and time efficiencies compared with de novo drug development. Computational screening of molecular targets, real-world evidence, and phenotypic assays can identify promising candidates for clinical evaluation. Regulatory pathways for repurposed drugs often benefit from extant safety data, yet rigorous randomized trials remain necessary to establish efficacy in new contexts. Economic incentives and intellectual property considerations influence industry engagement in repurposing efforts; public–private partnerships and funding mechanisms can catalyze research where market incentives are misaligned with public health priorities. Ethical considerations include equitable access and transparent communication of evidence.",
    "label": "ai"
  },
  {
    "text": "Postcolonial literature interrogates the legacies of imperial domination, articulating voices that reclaim history, identity, and cultural memory. Authors within this tradition employ hybrid narrative forms, subaltern perspectives, and language strategies that resist colonial epistemologies. Themes of displacement, hybridity, and resistance surface across novels, poetry, and critical essays, revealing how power operates through representation and institutional structures. Scholarly engagement with postcolonial texts examines translation politics, memory studies, and the intersections of gender, class, and race. By centering marginalized perspectives, postcolonial narratives contribute to broader debates about decolonization in education, archival practices, and cultural policy.",
    "label": "ai"
  },
  {
    "text": "Contemporary supply chains face heightened volatility from geopolitical tensions, climate shocks, and demand-side disruptions, prompting firms to reconsider resilience and agility strategies. Diversification of suppliers, nearshoring, and investment in real-time visibility technologies enhance robustness but entail trade-offs in cost and complexity. Risk management frameworks that integrate scenario analysis, stress testing, and supply-chain mapping help identify critical nodes and single points of failure. Digitalization—through blockchain, IoT sensors, and advanced analytics—can improve traceability and predictive maintenance, supporting both efficiency and compliance. Policymakers and firms must coordinate on critical infrastructure protection and develop contingency plans for essential goods during acute disruptions.",
    "label": "ai"
  },
  {
    "text": "Coral reef resilience relies on biological diversity, favorable environmental conditions, and local management that reduces non-climatic stressors. Protecting herbivorous fish populations, reducing nutrient runoff, and limiting direct physical damage from coastal development increase reefs’ capacity to recover from bleaching events. Active restoration techniques, such as assisted evolution and coral gardening, offer potential to accelerate recovery but carry ecological uncertainties and require long-term monitoring. Climate mitigation remains the primary lever to preserve coral ecosystems at scale, while local interventions can enhance adaptive capacity. Collaborative governance involving fishers, tourism stakeholders, and conservation organizations is critical to reconcile livelihood needs with conservation objectives.",
    "label": "ai"
  },
  {
    "text": "Electoral systems shape political incentives, party systems, and representation outcomes. Majoritarian systems tend to produce fewer, larger parties and may yield greater governability, while proportional representation fosters multiparty pluralism and broader representation of political preferences. Mixed systems seek to balance these aims but introduce complexity in vote-to-seat translation. Electoral design affects strategic voting, candidate selection, and the inclusiveness of marginalized groups. Institutional reforms—such as threshold adjustments, district magnitudes, and ballot structures—carry normative trade-offs between stability and representation. Comparative empirical research helps illuminate how electoral rules interact with social cleavages and party organization to produce observable political dynamics.",
    "label": "ai"
  },
  {
    "text": "Projected sea-level rise poses significant risks to coastal systems through inundation, increased erosion, and salinization of freshwater resources. Thermal expansion and cryospheric melt contribute to global mean sea-level change, with regional variability driven by ocean dynamics and land subsidence. Adaptation strategies include hard engineering solutions, managed retreat, and nature-based approaches that restore coastal wetlands to buffer storm impacts. Economic assessments should account for long-term discounting, non-market ecosystem services, and distributional consequences for vulnerable populations. Integrated coastal zone management that brings together planning, finance, and community engagement enhances the feasibility of adaptation pathways under uncertainty. Robust monitoring of sea-level trends informs timely decision-making.",
    "label": "ai"
  },
  {
    "text": "Strengthening primary care systems improves population health outcomes, enhances equity, and reduces avoidable hospitalizations when structured around continuity, comprehensiveness, and coordination. Investments in primary care workforce training, remuneration models that incentivize preventive care, and integrated health information systems support service delivery. Community-oriented approaches and task-shifting can expand access in resource-constrained settings. Financing strategies — including pooled funding and capitation — influence provider behavior and access. Evaluation of primary care reforms should measure process, outcome, and equity indicators to capture broader system effects. Policies must also address social determinants of health through intersectoral collaboration to realize primary care’s full potential.",
    "label": "ai"
  },
  {
    "text": "Protecting critical infrastructure from cyber threats requires coordinated risk management, information sharing, and resilient design principles. Operators should implement defense-in-depth architectures, continuous monitoring, and incident response exercises to reduce vulnerability and enhance recovery capabilities. Public–private partnerships facilitate threat intelligence exchange and the development of sector-specific guidance. Regulatory frameworks can mandate baseline security controls and reporting of significant incidents while balancing operational confidentiality. Investment in workforce development and simulation-based training strengthens human capabilities for cyber defense. Cross-border collaboration is essential given the transnational nature of many cyber threats and the interconnectedness of infrastructure systems.",
    "label": "ai"
  },
  {
    "text": "Animal migration represents adaptive responses to seasonal resource variation, reproductive cycles, and climatic gradients. Migratory strategies vary across taxa in timing, route fidelity, and navigational mechanisms, which include celestial cues, geomagnetic sensing, and learned landmarks. Anthropogenic changes—such as habitat fragmentation, light pollution, and climate-driven phenological shifts—disrupt migratory connectivity and can lead to population declines. Conservation strategies include protection of critical stopover sites, habitat restoration along flyways, and international cooperation for transboundary species. Long-term monitoring using telemetry and integrated population models provides insights into demographic consequences of altered migration and informs targeted conservation interventions.",
    "label": "ai"
  },
  {
    "text": "Microfinance initiatives aim to expand access to credit and financial services for underserved populations, with mixed evidence on impacts for income, consumption smoothing, and entrepreneurial activity. Randomized evaluations indicate heterogeneous effects driven by loan product design, borrower characteristics, and local market conditions. While access to microcredit can enable investment in productive assets and risk management, high interest rates and over-indebtedness pose potential harms. Complementary services—such as financial literacy training, savings mechanisms, and market linkages—enhance positive outcomes. Policy and donor strategies should focus on consumer protection, transparent lending practices, and iterative impact assessment to ensure financial inclusion delivers sustainable benefits.",
    "label": "ai"
  },
  {
    "text": "Large-scale satellite constellations promise improved global connectivity and Earth observation capabilities but raise concerns about orbital congestion, space debris, and equitable access to spectrum. Regulatory frameworks must address coordination of frequency allocation, collision avoidance protocols, and end-of-life deorbiting requirements to mitigate long-term orbital sustainability risks. Equitable governance should consider the needs of developing countries and small operators to prevent concentration of space capabilities. Environmental assessments that include light pollution and impacts on astronomical observation are necessary. International collaboration through norms, transparency initiatives, and capacity-building can foster responsible development of space-based infrastructure.",
    "label": "ai"
  },
  {
    "text": "Transforming food systems to promote nutrition requires interventions across production, distribution, and consumption stages. Agricultural policies that incentivize diverse crop production, coupled with market mechanisms that improve affordability and accessibility of nutritious foods, can reduce micronutrient deficiencies and diet-related noncommunicable diseases. Food environment interventions—such as reformulation, labeling, and fiscal measures—affect consumer choices and public health outcomes. Supply-chain resilience and reduction of food loss contribute to both sustainability and food security. Policy coherence across health, agriculture, and trade sectors, informed by rigorous evaluation, is crucial to align food systems with nutritional and environmental objectives.",
    "label": "ai"
  },
  {
    "text": "Building agricultural resilience to climate variability involves diversifying cropping systems, adopting drought-tolerant varieties, and implementing water-efficient practices such as drip irrigation. Agroecological diversification—integrating trees, livestock, and crop rotations—enhances soil health and buffers against yield shocks. Access to credit, weather-indexed insurance, and timely climate information services supports farmers’ adaptive capacity, especially among smallholders. Institutional arrangements that strengthen farmer organizations and extension services facilitate knowledge diffusion. Evaluations should consider socio-economic heterogeneity and gender dynamics, as adaptive strategies may have differential effects across households. Scaling up resilience interventions requires aligning incentives, investments, and policy support across sectors.",
    "label": "ai"
  },
  {
    "text": "Model interpretability is essential for trust, accountability, and debugging in high-stakes machine learning applications. Approaches range from inherently interpretable models—such as sparse linear models and decision trees—to post hoc explanation techniques like feature-attribution and counterfactual analysis. Interpretability must be evaluated relative to stakeholder needs; technical explanations that are precise for developers may be incomprehensible for end users or regulators. Moreover, explanation methods can be misleading if they oversimplify model behavior or fail to account for confounding in the data. Combining rigorous evaluation of explanation fidelity with participatory design processes helps ensure interpretability serves practical governance and ethical objectives.",
    "label": "ai"
  },
  {
    "text": "Demographic aging presents fiscal, labor market, and service delivery challenges for many countries, driven by declining fertility and increasing longevity. Policy responses include promoting longer workforce participation through retraining and age-friendly workplace policies, reforming pension systems to balance sustainability and adequacy, and investing in long-term care infrastructure. Migration and labor-force participation changes can partly offset demographic pressures but carry social and political considerations. Health system adaptations that prioritize geriatric care, preventive services, and integrated social supports increase quality of life for older adults. Cross-sector planning that anticipates demographic shifts enables smoother transitions and protects vulnerable cohorts.",
    "label": "ai"
  },
  {
    "text": "Carbon markets can harness market mechanisms to reduce greenhouse gas emissions by creating price signals for carbon-intensive activities. Cap-and-trade systems and emissions trading schemes allocate allowances and allow trading to achieve emissions targets cost-effectively, provided robust monitoring, reporting, and verification. Design considerations—such as allowance allocation, price floors, and linkage across jurisdictions—affect efficiency and distributional outcomes. Market-based approaches should be complemented by complementary policies, including investments in low-carbon technology and targeted support for affected workers and communities. Attention to additionality, leakage, and cost containment mechanisms ensures environmental integrity without undue economic disruption.",
    "label": "ai"
  },
  {
    "text": "Medical imaging innovations improve diagnostic accuracy and enable minimally invasive interventions by integrating advances in hardware, signal processing, and computational analysis. Techniques such as MRI, CT, and ultrasound each offer trade-offs in resolution, contrast, and safety, while hybrid modalities expand diagnostic capabilities. Machine learning has enhanced image interpretation through automated segmentation and anomaly detection, though clinical validation and regulatory clearance remain critical. Ensuring interoperability with electronic health records and clinician workflows supports adoption. Ethical concerns include algorithmic bias, informed consent for data use, and equitable access to advanced imaging technologies. Multi-disciplinary collaboration accelerates translation from prototype to clinical utility.",
    "label": "ai"
  },
  {
    "text": "High-quality early childhood education (ECE) yields long-term benefits for cognitive development, socio-emotional skills, and future educational attainment. Program quality—characterized by trained staff, appropriate curriculum, and low child-to-teacher ratios—mediates impacts. Universal access policies can reduce inequality, but implementation requires sustained funding and quality monitoring to avoid under-resourced scaling. Parental engagement and integrated services that address health and nutrition amplify developmental outcomes, particularly in low-resource settings. Rigorous program evaluation using longitudinal designs informs policy choices about investment levels, curricular models, and workforce development to maximize ECE’s lifelong returns.",
    "label": "ai"
  },
  {
    "text": "Managing invasive species requires prevention, early detection, rapid response, and long-term control strategies tailored to ecological contexts. Risk assessment frameworks prioritize pathways of introduction and identify vulnerable ecosystems. Biological control, mechanical removal, and habitat restoration are complementary tactics although each carries ecological trade-offs that necessitate risk–benefit analysis. Effective management engages stakeholders, including landowners and local communities, to implement surveillance and containment measures. Policy instruments such as border biosecurity, public awareness campaigns, and funding mechanisms for eradication efforts contribute to sustained control. Adaptive management underpinned by monitoring data improves decision-making over time.",
    "label": "ai"
  },
  {
    "text": "The proliferation of digital surveillance technologies raises complex human rights questions related to privacy, freedom of expression, and due process. Surveillance tools—ranging from mass data collection to facial recognition—can serve legitimate public safety objectives but also enable disproportionate state control and discrimination. Legal frameworks should enforce necessity and proportionality standards, oversight mechanisms, and avenues for individual redress. Transparency about surveillance practices and independent auditing enhance accountability. International human rights norms provide guidance for balancing security with civil liberties, and civic engagement is critical to democratic governance of surveillance technologies.",
    "label": "ai"
  },
  {
    "text": "Afforestation and reforestation can sequester carbon and contribute to climate mitigation while delivering co-benefits such as habitat restoration and soil conservation. However, relying solely on tree planting without attention to species selection, ecosystem compatibility, and land-use trade-offs risks negative ecological and social consequences. Monocultural plantations may reduce biodiversity and alter hydrological cycles, while competing land claims can threaten food security. Integrated landscape approaches that combine natural regeneration, biodiversity protection, and community tenure arrangements improve the sustainability of afforestation initiatives. Measurement, reporting, and verification systems must account for permanence and potential leakage to ensure credible carbon sequestration claims.",
    "label": "ai"
  },
  {
    "text": "Antimicrobial resistance (AMR) threatens the effectiveness of treatments for infectious diseases and requires coordinated action across human health, veterinary medicine, and agriculture. Stewardship programs that optimize antibiotic use, along with infection prevention and control measures, reduce selection pressure for resistant strains. Surveillance systems and genomic epidemiology provide insights into resistance emergence and spread. Incentivizing research and development for new antimicrobials and diagnostics addresses the pipeline shortfall, but sustainable access and appropriate stewardship must accompany any novel therapies. Global governance mechanisms should support capacity-building in low-resource settings and equitable access to effective treatments.",
    "label": "ai"
  },
  {
    "text": "Panel data methods exploit temporal and cross-sectional variation to control for unobserved heterogeneity and estimate dynamic relationships. Fixed-effects estimators remove time-invariant confounders, while random-effects models impose distributional assumptions to improve efficiency. Difference-in-differences designs leverage staggered or discrete policy changes but require careful attention to parallel trends and treatment heterogeneity. Instrumental variable techniques can address endogenous regressors within panel settings, provided valid instruments exist. Recent methodological advances address issues of inference with many units or time periods, nonstationarity, and treatment effect heterogeneity. Robust empirical practice entails sensitivity analyses, transparent reporting, and alignment of econometric strategies with theoretical identification assumptions.",
    "label": "ai"
  },
  {
    "text": "Offshore wind energy presents significant potential for large-scale renewable generation, benefiting from higher and more consistent wind speeds than onshore sites. Project development involves complex permitting, grid connection logistics, and environmental impact assessments relating to marine ecosystems and fisheries. Technological innovations in turbine design, floating foundations, and installation methods reduce costs and expand feasible deployment zones. Integrating offshore wind into coastal energy systems requires investments in transmission infrastructure and coordination with marine spatial planning. Community engagement and benefit-sharing arrangements can address local concerns and distribute economic gains from offshore projects.",
    "label": "ai"
  },
  {
    "text": "Digital mental health interventions—including cognitive behavioral therapy apps, telepsychiatry, and computerized screening—expand access to care and can complement traditional services. Efficacy depends on evidence-based content, user engagement strategies, and integration with clinical pathways for severe cases. Data security and ethical safeguards around patient privacy are paramount, as is clarity about evidence limitations. Implementation research highlights that blended care models, where digital tools support clinician-delivered therapy, often yield the best outcomes. Equity considerations require addressing the digital divide and cultural adaptation of content to ensure inclusivity. Continuous monitoring and rigorous trials inform scale-up decisions.",
    "label": "ai"
  },
  {
    "text": "Widespread adoption of electric vehicles (EVs) is contingent upon vehicle affordability, charging infrastructure availability, and complementary policies such as incentives and fuel economy standards. Total cost of ownership parity with internal combustion vehicles accelerates consumer uptake, aided by economies of scale in battery production. Policy measures—such as purchase subsidies, investments in public charging networks, and building codes that require EV-ready parking—address demand- and supply-side barriers. Grid impacts necessitate smart charging strategies and integration with renewable generation to maximize environmental benefits. Equity-focused policies ensure that low-income households and disadvantaged communities share in EV transitions’ benefits.",
    "label": "ai"
  },
  {
    "text": "Preservation of cultural heritage involves safeguarding tangible and intangible practices, artifacts, and knowledge systems while respecting community custodianship and evolving cultural expression. Collaborative approaches that engage indigenous and local stakeholders in decision-making enhance cultural relevance and ethical stewardship. Adaptive reuse, documentation, and digitization complement in situ preservation, but strategies should avoid commodification that divorces heritage from living cultural contexts. Legal instruments, funding mechanisms, and education programs contribute to sustainable preservation, and attention to restitution debates and access rights is necessary to address historical injustices. Interdisciplinary methods integrate archaeological, ethnographic, and archival perspectives.",
    "label": "ai"
  },
  {
    "text": "Behavioral insights inform tax policy design by recognizing cognitive biases and heuristics that influence compliance and economic decisions. Simplified filing procedures, salient withholding mechanisms, and timely reminders increase voluntary compliance and reduce administrative burdens. Framing tax benefits or liabilities can affect perceived fairness and willingness to pay, while default options in pension contributions harness inertia to boost savings. Policymakers should conduct randomized evaluations to assess behavioral interventions’ effectiveness and consider distributional impacts to avoid regressive outcomes. Transparency and clear communication about tax expenditures strengthen public legitimacy and informed debate.",
    "label": "ai"
  },
  {
    "text": "Designing effective human–robot interaction (HRI) systems requires understanding human cognitive models, social cues, and ergonomic constraints to promote safety and acceptance. Interface design, shared autonomy paradigms, and predictable robot behaviors enhance trust and coordination in collaborative settings. Domain-specific considerations—such as assistive robotics for healthcare versus industrial automation—demand tailored evaluation metrics and user-centered design processes. Ethical considerations include liability, privacy, and the implications of social robots on human relationships. Iterative field testing and multidisciplinary collaboration between engineers, psychologists, and ethicists improve the alignment of robotic technologies with societal needs.",
    "label": "ai"
  },
  {
    "text": "Circular economy principles aim to decouple economic activity from resource extraction by promoting reuse, remanufacturing, and material recovery. Policy levers—such as extended producer responsibility, eco-design standards, and incentives for secondary markets—encourage firms to internalize lifecycle impacts. Circular business models require coordination across supply chains and consumer engagement to ensure material quality and return flows. Measuring circularity involves indicators for material circularity, product lifespan, and recycling efficiency. Transitioning to circular systems can yield environmental benefits and economic opportunities, but requires investment in infrastructure, innovation in product design, and supportive regulatory frameworks.",
    "label": "ai"
  },
  {
    "text": "Population genomics leverages large-scale sequencing to study genetic variation, population structure, and evolutionary processes across species. Analyses of genomic data inform conservation strategies, reveal demographic histories, and identify loci under selection. Statistical methods must account for linkage disequilibrium, population stratification, and sampling biases to avoid spurious inferences. Ethical considerations include consent, benefit sharing, and the potential misuse of genetic information. Integrating genomic insights with ecological and phenotypic data enhances understanding of adaptive potential and informs management decisions for biodiversity conservation and public health applications.",
    "label": "ai"
  },
  {
    "text": "Demand response programs shift electricity consumption in time to improve system flexibility and integrate variable renewable generation. Price-based mechanisms, such as time-of-use tariffs, and incentive-based programs that compensate load reductions both contribute to peak management. Effective programs require enabling technologies—smart meters, automated control systems—and consumer engagement to realize participation. Evaluation of demand response should consider rebound effects, equity in participation, and impacts on comfort and productivity. Policies that coordinate demand response with distributed storage and localized generation amplify grid benefits while providing cost-effective alternatives to generation-side investments.",
    "label": "ai"
  },
  {
    "text": "Gentrification involves socio-economic transformation of urban neighborhoods characterized by rising property values, in-migration of higher-income residents, and displacement risks for incumbent communities. Causes include changes in housing markets, amenity-driven preferences, and policy incentives that alter land-use constraints. Outcomes are heterogeneous: gentrification can bring infrastructure investment and reduced crime but also erode social networks and affordability. Policy responses include inclusionary zoning, tenant protections, and community land trusts to preserve affordability and social diversity. Empirical analysis should disaggregate effects across demographic groups and consider long-term trajectories to inform urban policy design.",
    "label": "ai"
  },
  {
    "text": "Designing effective protected areas requires consideration of representativeness, connectivity, and resilience to environmental change. Spatial prioritization tools incorporate species distributions, threat layers, and socio-economic constraints to identify cost-effective networks. Governance arrangements that involve indigenous peoples and local communities often yield better compliance and ecological outcomes. Buffer zones and ecological corridors enhance genetic exchange and adaptation potential under climate change. Metrics for protected area success should include ecological indicators, governance quality, and benefits to local livelihoods. Adaptive management, underpinned by robust monitoring, allows for iterative adjustments as conditions evolve.",
    "label": "ai"
  },
  {
    "text": "Public debt sustainability depends on growth prospects, interest rate dynamics, and fiscal policy credibility. Macroeconomic frameworks that incorporate contingent liabilities and realistic revenue projections improve fiscal planning. Structural reforms to broaden tax bases, strengthen public financial management, and enhance expenditure efficiency contribute to sustainable debt trajectories. In low-income countries, concessional financing and debt relief mechanisms can provide breathing space for investment in growth-promoting sectors, but must be paired with governance reforms. Transparent reporting and stress testing against adverse scenarios bolster market confidence and policy adaptability in the face of shocks.",
    "label": "ai"
  },
  {
    "text": "Microbiome research explores the diverse microbial communities that inhabit hosts and environments, elucidating roles in health, nutrient cycling, and ecosystem functioning. Metagenomic and metabolomic approaches reveal taxonomic composition and functional potential, but establishing causality requires experimental manipulation and mechanistic models. Host–microbe interactions mediate immune development, metabolic processes, and disease susceptibility, suggesting translational opportunities for probiotics and microbiome-targeted therapies. Standardization of sampling, data processing, and reporting enhances comparability across studies. Ethical considerations include data sharing, consent, and implications for personalized interventions based on microbiome profiles.",
    "label": "ai"
  },
  {
    "text": "The gig economy reshapes labor markets by increasing flexible, task-based work mediated through digital platforms, raising questions about worker classification, benefits, and income volatility. Platforms can lower transaction costs and expand market access, yet workers often face precarious earnings, limited social protections, and algorithmic management. Policy responses range from portable benefits and minimum standards to reinterpretation of employment status and collective bargaining frameworks adapted to platform contexts. Empirical research should measure heterogeneity in worker experiences and the implications for long-term career trajectories, skill accumulation, and inequality.",
    "label": "ai"
  },
  {
    "text": "Vaccine development integrates antigen discovery, adjuvant design, and delivery platforms to elicit protective immune responses with acceptable safety profiles. Preclinical models and immunogenicity assays guide candidate selection, while phased clinical trials evaluate efficacy and adverse events. Platform technologies—such as mRNA, viral vectors, and protein subunits—offer differing advantages in speed, scalability, and immune mechanisms. Cold-chain requirements and manufacturing capacity influence global distribution equity. Post-licensure surveillance and pharmacovigilance are essential for ongoing safety monitoring, and regulatory harmonization can expedite access while maintaining rigorous standards. Collaborative funding models support rapid response to emerging pathogens.",
    "label": "ai"
  },
  {
    "text": "Income inequality reflects unequal distributions of wages, capital returns, and access to opportunities, with implications for social mobility and political stability. Drivers include technological change, globalization, educational disparities, and institutional settings such as tax and labor market policies. Policy responses encompass progressive taxation, investments in education and training, and social safety nets to mitigate inequality’s adverse effects. Empirical work emphasizes measurement challenges—such as top-income capture and in-kind transfers—and the need for longitudinal data to assess intergenerational mobility. Addressing inequality requires coordinated fiscal, labor, and social policies tailored to country-specific contexts.",
    "label": "ai"
  },
  {
    "text": "Optimization algorithms underpin decision-making across engineering, economics, and machine learning by identifying parameter sets that minimize or maximize objective functions under constraints. Convex optimization offers strong theoretical guarantees, while nonconvex problems—such as those in deep learning—require heuristic approaches and careful initialization. Gradient-based methods, stochastic optimization, and metaheuristics each provide trade-offs between convergence speed and global optimality. Advances in algorithmic acceleration, distributed optimization, and adaptive learning rates enhance scalability for large datasets. Rigorous benchmarking and sensitivity analysis inform algorithm selection for practical applications.",
    "label": "ai"
  },
  {
    "text": "Water security integrates availability, quality, and access dimensions to support human well-being and ecosystem health. Integrated water resources management promotes coordinated planning across sectors, demand management, and investments in infrastructure for storage, treatment, and distribution. Nature-based solutions—such as watershed restoration and wetlands conservation—augment conventional infrastructure to enhance resilience. Governance mechanisms that allocate water equitably, incorporate stakeholder participation, and incentivize efficient use are central to sustainability. Climate change exacerbates hydrological variability, underscoring the need for adaptive management and robust monitoring systems to inform policy interventions.",
    "label": "ai"
  },
  {
    "text": "Autonomous vehicles (AVs) promise reductions in human-error-driven crashes but raise complex technical, regulatory, and ethical questions. Ensuring safety requires rigorous testing across diverse operational domains, fail-safe system design, and standards for sensor redundancy and cybersecurity. Regulatory frameworks must define liability, certification pathways, and data-sharing requirements for incident analysis. Equity considerations include geographic deployment patterns and impacts on employment in driving professions. Public acceptance depends on demonstrable safety performance and transparent communication about capabilities and limitations. Interdisciplinary research bridging engineering, human factors, and policy supports responsible AV integration.",
    "label": "ai"
  },
  {
    "text": "Enzyme engineering harnesses directed evolution and rational design to optimize catalytic properties for industrial, medical, and environmental applications. Altering substrate specificity, thermal stability, and turnover rates expands enzymes’ utility in biocatalysis and therapeutic contexts. Advances in high-throughput screening and computational protein design accelerate the identification of beneficial mutations. Integration with process engineering ensures scalability and cost-effectiveness in industrial settings. Ethical and safety considerations include containment of engineered organisms and assessment of ecological impacts if released. Cross-disciplinary collaboration between chemists, biologists, and engineers supports responsible innovation in enzyme technologies.",
    "label": "ai"
  },
  {
    "text": "Trade policy shapes comparative advantage, market access, and domestic regulatory autonomy through tariffs, non-tariff measures, and trade agreements. Liberalized trade can spur economic growth by expanding markets and fostering competition, but distributional consequences necessitate domestic adjustment policies for affected workers and regions. Trade agreements increasingly encompass regulatory cooperation, digital trade, and standards harmonization, raising governance questions about democratic accountability and policy space. Empirical assessments should consider general equilibrium effects and heterogeneous impacts across sectors. Strategic use of industrial policy and selective protection can complement trade openness when designed transparently and temporally limited.",
    "label": "ai"
  },
  {
    "text": "Urban green spaces provide ecosystem services—such as air purification, heat mitigation, and recreational opportunities—that improve urban livability and public health. Equitable distribution of parks and green corridors addresses environmental justice by ensuring access for underserved communities. Planning decisions should incorporate multifunctional design that supports biodiversity, stormwater management, and social cohesion. Green infrastructure investments yield co-benefits for climate adaptation and mental well-being, but long-term maintenance funding and inclusive governance are necessary to sustain benefits. Participatory design processes engage local stakeholders and align green space amenities with community needs.",
    "label": "ai"
  },
  {
    "text": "Neural prosthetics interface with the nervous system to restore lost sensory or motor functions, leveraging advances in electrode design, signal processing, and closed-loop control. Clinical applications include cochlear implants, brain–computer interfaces for motor restoration, and deep brain stimulation for movement disorders. Challenges include biocompatibility, long-term signal stability, and decoding complex neural representations. Ethical concerns span consent, identity, and equitable access to expensive neurotechnologies. Translational pathways require rigorous clinical trials, regulatory oversight, and multidisciplinary teams that integrate engineering, neuroscience, and rehabilitation expertise to maximize patient-centered outcomes.",
    "label": "ai"
  },
  {
    "text": "Digital inequality encompasses disparities in access to devices and connectivity, as well as differences in digital skills and meaningful use. Beyond infrastructure, disparities manifest in participation in the digital economy, access to telehealth, and educational outcomes. Policy responses include targeted connectivity subsidies, digital literacy programs, and inclusive platform design that considers accessibility and language diversity. Measurement should capture nuanced dimensions of access and use to inform interventions. Addressing digital inequality requires collaboration across governments, civil society, and private sector actors to ensure that technological advances do not exacerbate existing social stratification.",
    "label": "ai"
  },
  {
    "text": "Concrete production contributes substantially to global CO2 emissions, motivating research into low-carbon alternatives such as blended cements, supplementary cementitious materials, and alternative binders. Performance considerations—durability, strength, and workability—must be maintained while reducing clinker content or incorporating recycled aggregates. Lifecycle analyses help quantify emissions reductions and material trade-offs. Standards and procurement policies that incentivize low-carbon concrete adoption accelerate market uptake. Innovations in carbon capture and utilization at cement plants present complementary mitigation pathways but require cost reductions and supportive policy frameworks for large-scale deployment.",
    "label": "ai"
  },
  {
    "text": "Assessment reform aims to align evaluation methods with learning objectives that emphasize critical thinking, collaboration, and applied skills. Moving beyond high-stakes standardized testing, alternative assessments—such as portfolio-based evaluation, performance tasks, and formative assessments—provide richer diagnostic information and support personalized instruction. Implementation demands teacher training, supportive curricula, and scalable scoring systems. Balancing reliability with authenticity is a persistent challenge, and mixed-method evaluation frameworks can reconcile standardized metrics with contextualized assessment. Ensuring equity requires attention to cultural relevance and accommodations for diverse learners.",
    "label": "ai"
  },
  {
    "text": "Health systems resilience refers to the capacity to absorb shocks, maintain core functions, and adapt to new challenges, as evidenced during pandemics and natural disasters. Core components include surge capacity, supply-chain redundancy, robust surveillance, and flexible financing mechanisms. Workforce well-being, surge staffing protocols, and cross-sectoral coordination with emergency services and social care systems bolster resilience. Investments in primary care and community health workers enhance early detection and response. Post-event learning, institutional memory, and simulation exercises support continual improvement and preparedness for future crises.",
    "label": "ai"
  },
  {
    "text": "The expansion of genomic technologies raises ethical, legal, and social implications (ELSI) related to consent, privacy, and discrimination. Policy frameworks should protect individuals from genetic discrimination in employment and insurance while facilitating beneficial research through safeguards for de-identification and controlled data access. Community engagement, particularly with groups historically marginalized in research, supports ethical governance and equitable benefit sharing. Educational initiatives improve public understanding of genomic risks and limitations. International collaboration on standards for data sharing and harmonized consent protocols enhances transnational research while respecting local norms.",
    "label": "ai"
  },
  {
    "text": "Monetary policy in persistently low-interest environments faces constraints on conventional interest-rate tools, prompting reliance on unconventional measures such as quantitative easing, forward guidance, and negative nominal rates. Central banks must weigh trade-offs between stimulating demand and financial stability risks, including asset price inflation and compressed bank margins. Macroprudential tools can complement monetary policy by addressing systemic vulnerabilities. Clear communication and conditionality around exit strategies reduce market uncertainty. Fiscal policy coordination becomes increasingly important when monetary policy space is limited, particularly for supporting demand and public investment.",
    "label": "ai"
  },
  {
    "text": "Soil carbon sequestration presents opportunities to mitigate climate change while enhancing soil fertility and agricultural productivity. Practices such as cover cropping, reduced tillage, and organic amendments increase soil organic matter and carbon stocks over time. Measurement challenges include spatial heterogeneity and temporal lag between management changes and measurable sequestration. Carbon accounting frameworks must incorporate permanence, baseline selection, and potential leakage to ensure credible claims. Incentive structures—such as carbon markets or payments for ecosystem services—can motivate adoption but require verification systems and attention to equity for smallholders and marginal lands.",
    "label": "ai"
  },
  {
    "text": "Universal broadband access is foundational for economic development, education, and civic participation in the digital age. Policy interventions include infrastructure subsidies, public–private partnerships, and regulatory measures to encourage competition and reduce deployment costs in underserved areas. Affordability programs and digital literacy initiatives complement infrastructure investments to promote meaningful use. Spectrum allocation policies and municipal broadband options can expand access while preserving open internet principles. Monitoring connectivity metrics and consumer outcomes supports targeted interventions and accountability.",
    "label": "ai"
  },
  {
    "text": "Wildlife corridors facilitate gene flow, seasonal migrations, and range shifts necessary for species persistence under habitat fragmentation and climate change. Designing corridors requires spatial analysis of habitat suitability, movement ecology, and land-use compatibility. Legal mechanisms and incentives—such as conservation easements and payment schemes—encourage private-land stewardship. Multilevel governance, involving local communities, regional planners, and conservation organizations, supports corridor implementation and maintenance. Monitoring corridor effectiveness through telemetry and population metrics informs adaptive management and long-term conservation planning.",
    "label": "ai"
  },
  {
    "text": "Debates about machine moral agency interrogate whether artificial agents can bear responsibility, possess moral status, or merely function as tools for human moral action. Some philosophers argue that agency requires capacities for autonomy, understanding, and intentionality—attributes not currently instantiated in narrow AI systems—whereas others caution that advanced systems may create novel moral actors. Pragmatic approaches emphasize assigning accountability to designers, deployers, and institutions rather than machines themselves. Ethical design principles and governance frameworks should clarify responsibility chains and ensure human oversight in high-stakes contexts where moral judgments are consequential.",
    "label": "ai"
  },
  {
    "text": "Pollinator decline threatens crop yields, biodiversity, and ecosystem services critical to food systems. Drivers include habitat loss, pesticide exposure, disease, and climate change. Conservation measures encompass creating pollinator-friendly habitats, reducing pesticide reliance through integrated pest management, and supporting landscape connectivity. Agricultural policies that incentivize diversified cropping systems and habitat set-asides can coexist with productive agriculture. Monitoring pollinator populations and cross-disciplinary research on disease ecology and agroecological practices inform effective interventions. Engaging farmers, conservationists, and communities fosters scalable and locally appropriate solutions.",
    "label": "ai"
  },
  {
    "text": "Noncommunicable diseases (NCDs) such as cardiovascular disease, diabetes, and cancers constitute major global health burdens driven by behavioral, environmental, and metabolic risk factors. Population-level interventions—taxation of unhealthy products, regulations on advertising, and urban design promoting physical activity—can reduce risk exposure. Health systems require capacity for early detection, chronic disease management, and integrated care pathways. Surveillance systems that track risk factor prevalence and outcomes support policy evaluation. Addressing social determinants of health and inequities in access to prevention and care is essential to reduce NCD burdens sustainably.",
    "label": "ai"
  },
  {
    "text": "Distributed systems enable scalable, fault-tolerant computation by coordinating multiple processes across networked nodes. Key challenges include consistency, consensus, and latency management under partial failures. Architectural patterns—such as replication, sharding, and eventual consistency models—balance performance with correctness guarantees. Formal verification, monitoring, and observability tools assist in diagnosing complex system behaviors. Security considerations involve authentication, access control, and resilient protocols against adversarial faults. Design choices must account for workload characteristics, failure modes, and operational constraints to achieve robust distributed services.",
    "label": "ai"
  },
  {
    "text": "Habit formation research investigates how repeated behaviors become automatic through contextual cues and reinforcement. Interventions that structure environments—such as implementing prompts, simplifying desired actions, and linking new behaviors to existing routines—facilitate habit acquisition. Measurement of habit strength and decay informs design of interventions that foster maintenance, while incentives can help initial adoption but may undermine intrinsic motivation if poorly framed. Tailoring strategies to individual differences and leveraging social norms enhance effectiveness. Applications span public health, energy conservation, and organizational behavior where sustained behavioral change yields societal benefits.",
    "label": "ai"
  },
  {
    "text": "Scaling finance for climate adaptation requires mobilizing public budgets, private capital, and international climate funds to address vulnerability, particularly in low-income countries. Financial instruments—such as resilience bonds, concessional loans, and blended finance structures—can leverage additional investment while managing risk. Project pipelines must incorporate rigorous climate risk assessments, cost-benefit analysis, and community engagement to ensure local relevance and equitable outcomes. Capacity-building for project preparation and fiduciary management enhances absorptive capacity. Transparent tracking of adaptation finance flows improves accountability and alignment with national adaptation priorities.",
    "label": "ai"
  },
  {
    "text": "Language shift occurs as speech communities transition from one dominant language to another due to socio-economic pressures, education policies, and intergenerational change. Factors that accelerate shift include urbanization, media influence, and perceived economic advantages of dominant languages. Revitalization efforts—such as immersion education, community media, and institutional recognition—support maintenance of minority languages but require sustained resources and community ownership. Documentation and corpus development preserve linguistic heritage while informing pedagogical materials. Sociolinguistic research examines the interplay of identity, policy, and structural forces that shape language trajectories.",
    "label": "ai"
  },
  {
    "text": "Nanomaterials exhibit unique physicochemical properties that enable novel applications but also raise questions about human health and environmental safety. Risk assessment frameworks must consider exposure pathways, particle persistence, and bioaccumulation potential. Standardized toxicity testing protocols and life-cycle assessments help characterize risks from production to disposal. Regulatory approaches should balance innovation incentives with precautionary principles, enabling safe commercialization while collecting post-market surveillance data. Transparent labeling and worker-protection standards reduce occupational hazards. Interdisciplinary research integrating toxicology, materials science, and environmental monitoring improves evidence-based governance of nanotechnologies.",
    "label": "ai"
  },
  {
    "text": "Evidence-based policymaking integrates rigorous research—including randomized trials, quasi-experiments, and systematic reviews—into the policy cycle to improve decision quality and outcomes. Institutionalizing evaluation units, fostering data infrastructure, and requiring pre-analysis plans increase transparency and credibility. Policymakers must balance methodological rigor with relevance and timeliness, recognizing the value of mixed-methods that combine quantitative impact estimates with qualitative contextual insights. Capacity-building in government and international collaboration on best practices support sustained uptake of evidence. Ethical use of evidence involves transparency about limitations and trade-offs in policy choices.",
    "label": "ai"
  },
  {
    "text": "Exposure to ambient air pollution is a leading environmental health risk linked to respiratory, cardiovascular, and developmental outcomes. Policy interventions—such as emissions standards for industry and vehicles, fuel-switching, and urban planning that reduces traffic congestion—lower population exposures. Monitoring networks, low-cost sensors, and modeling tools inform targeted interventions and public advisories. Co-benefits of air quality improvements include reduced healthcare costs and climate mitigation when fossil-fuel combustion is curtailed. Equitable policies address disproportionate exposures in disadvantaged communities and ensure inclusive participation in decision-making.",
    "label": "ai"
  },
  {
    "text": "Federated learning enables decentralized model training across distributed devices while keeping raw data localized, offering privacy advantages for sensitive applications. Challenges include communication efficiency, statistical heterogeneity across clients, and incentives for participation. Robust aggregation algorithms and personalization techniques mitigate issues arising from non-iid data, while secure aggregation and differential privacy protect confidentiality. Federated approaches are promising for healthcare and mobile applications but require validation of model performance and fairness across diverse subpopulations. Governance frameworks must clarify data governance models and liability in federated deployments.",
    "label": "ai"
  },
  {
    "text": "Nudging employs subtle changes to choice architecture to steer behavior without restricting options, leveraging insights about cognitive biases and decision heuristics. Examples include defaults, salience manipulations, and simplified information provision. Effective nudges are evidence-driven and context-sensitive, and should be evaluated for both effectiveness and ethical acceptability. Critics question paternalism and transparency; proponents advocate for transparent nudges and democratic oversight. Combining nudges with structural interventions often yields more durable outcomes, as individual-level cues may not address underlying constraints like affordability or access.",
    "label": "ai"
  },
  {
    "text": "Marine heatwaves—prolonged, anomalously warm sea surface temperature events—have profound impacts on marine ecosystems, fisheries, and coastal communities. Their frequency and intensity have increased with global warming, leading to coral bleaching, range shifts, and altered productivity. Early warning systems, adaptive fisheries management, and protection of thermal refugia can reduce ecological and economic damages. Research integrating observational datasets, climate models, and ecological responses improves predictive capacity. Policy responses should incorporate ecosystem-based adaptation and support for affected communities dependent on marine resources.",
    "label": "ai"
  },
  {
    "text": "Interoperability among health information systems enables coordinated care, reduces duplication, and supports data-driven decision-making. Standards-based data models, APIs, and terminology services facilitate semantic interoperability, while governance frameworks address consent, access control, and data stewardship. Challenges include legacy systems, variable data quality, and institutional silos that impede seamless information exchange. Incentives for interoperability—such as certification requirements, reimbursement linked to data sharing, and technical assistance—promote adoption. Patient-centered approaches that ensure portability and control over personal health data enhance trust and utility.",
    "label": "ai"
  },
  {
    "text": "The resource curse describes paradoxical outcomes where countries with abundant natural resources experience slower economic growth, governance challenges, and conflict risks. Mechanisms include rent-seeking behavior, volatile commodity revenues, and weakened institutional development. Policy responses encompass sovereign wealth funds to smooth revenue volatility, transparency initiatives like the Extractive Industries Transparency Initiative, and investments in human capital and economic diversification. Strengthening governance, accountability, and rule of law reduces vulnerability to resource-driven distortions. Context-specific strategies that engage local stakeholders and mitigate environmental impacts support sustainable resource management.",
    "label": "ai"
  },
  {
    "text": "Bayesian methods provide a coherent framework for probabilistic inference by combining prior information with observed data to produce posterior distributions. Hierarchical modeling facilitates pooling of information across related units and quantification of uncertainty at multiple levels. Computational advances—such as Markov chain Monte Carlo and variational inference—have expanded Bayesian methods’ applicability to large and complex datasets. Prior specification and sensitivity analysis are essential to assess robustness, particularly in settings with limited data. Bayesian approaches support decision-making under uncertainty by producing probabilistic statements and predictive distributions that inform policy and scientific interpretation.",
    "label": "ai"
  },
  {
    "text": "Improving maternal health requires strengthening antenatal care, skilled birth attendance, and postpartum services, alongside addressing socio-economic determinants such as nutrition and education. Emergency obstetric care, referral systems, and access to family planning reduce maternal morbidity and mortality. Health workforce training and respectful maternity care improve utilization and outcomes. Data systems that monitor maternal indicators and near-miss events inform quality improvement. Equity-focused strategies target marginalized groups and address barriers such as cost, distance, and social norms that impede service uptake.",
    "label": "ai"
  },
  {
    "text": "Protein folding research elucidates how linear amino acid sequences attain functional three-dimensional structures through energetically favorable pathways. Experimental techniques—such as X-ray crystallography, NMR, and cryo-electron microscopy—complement computational modeling to resolve folding intermediates and misfolding mechanisms implicated in disease. Energy landscape theory and molecular dynamics simulations provide frameworks for understanding folding kinetics and stability. Advances in computational prediction have accelerated structure discovery, enabling insights into function and drug design. Integrating experimental validation with predictive models strengthens confidence in inferred mechanisms and therapeutic targets.",
    "label": "ai"
  },
  {
    "text": "Designing carbon pricing policies that are both effective and equitable requires attention to distributional impacts and compensatory measures. Revenue recycling—through lump-sum dividends, targeted transfers, or investments in low-income energy access—can offset regressive burdens and enhance political acceptability. Sectoral exemptions and border adjustments address competitiveness concerns but may dilute emission-reduction incentives if poorly designed. Complementary policies—such as energy efficiency programs and targeted support for transition-affected workers—ensure broader social protections. Transparent communication about revenue use and equity rationales strengthens public support for carbon pricing instruments.",
    "label": "ai"
  },
  {
    "text": "Swarm robotics studies decentralized coordination among multiple simple agents to achieve complex collective behaviors inspired by biological systems. Applications include environmental monitoring, search and rescue, and agricultural automation where robustness to individual failures and scalability are advantageous. Key technical challenges involve designing local interaction rules that yield desired global properties, communication constraints, and robustness to adversarial conditions. Simulation and formal verification assist in validating emergent behaviors before field deployment. Ethical and safety considerations include ensuring accountability for collective actions and preventing unintended environmental impacts.",
    "label": "ai"
  },
  {
    "text": "Effective substance use policy combines prevention, harm reduction, treatment access, and regulatory measures to address complex public health challenges. Harm-reduction interventions—such as needle-exchange programs and supervised consumption sites—reduce transmission of infectious diseases and overdose mortality. Evidence-based treatment modalities include medication-assisted therapies and integrated mental health services. Regulatory frameworks should balance control of supply with access to treatment and destigmatizing public communication. Surveillance, evaluation, and community engagement guide policy adaptation to evolving consumption patterns and emerging substances.",
    "label": "ai"
  },
  {
    "text": "Structural econometric models represent economic agents’ behavior and market equilibria to conduct counterfactual analysis and inform policy design. Identification of structural parameters relies on economic theory, functional forms, and exclusion restrictions, making transparency about assumptions critical. Calibration and estimation strategies—such as simulated method of moments and maximum likelihood—address computational challenges in complex models. Structural approaches facilitate welfare analysis and policy simulations but require robustness checks against model misspecification and sensitivity to parameter choices. Combining structural insights with reduced-form evidence provides a balanced empirical strategy.",
    "label": "ai"
  },
  {
    "text": "Sustainable fisheries management integrates stock assessments, catch limits, and ecosystem-based approaches to prevent overfishing while supporting coastal livelihoods. Rights-based management, such as individual transferable quotas or community quotas, can align incentives for conservation but require careful design to avoid concentration of access. Monitoring, control, and surveillance—using remote sensing and vessel-tracking—improve compliance. Adaptive management that accounts for environmental variability and multispecies interactions enhances resilience. Socio-economic support for fishers during transitions and co-management arrangements with local communities foster legitimacy and long-term sustainability.",
    "label": "ai"
  },
  {
    "text": "Responsible data governance for AI encompasses principles of consent, fairness, quality, and accountability across data lifecycles. Data provenance tracking, bias auditing, and documentation practices—such as datasheets for datasets—improve transparency and reproducibility. Governance frameworks should delineate roles and responsibilities for data stewards, maintainers, and downstream modelers, and provide mechanisms for stakeholder engagement. Data minimization and privacy-enhancing techniques reduce exposure risks while preserving analytic utility. Regulatory approaches must adapt to rapid technological change and ensure avenues for remedy when harms arise from data-driven systems.",
    "label": "ai"
  },
  {
    "text": "Neuroplasticity describes the brain’s capacity to reorganize structure and function in response to experience, learning, and injury. Mechanisms include synaptic plasticity, dendritic remodeling, and neurogenesis in select regions. Plasticity underlies rehabilitation, skill acquisition, and critical-period effects across the lifespan. Interventions—such as targeted training, noninvasive brain stimulation, and enriched environments—can leverage plasticity for therapeutic benefit, though individual variability influences outcomes. Understanding the balance between adaptive and maladaptive plasticity informs clinical strategies for neurodevelopmental and neurodegenerative conditions.",
    "label": "ai"
  },
  {
    "text": "A hydrogen economy envisions hydrogen as an energy carrier for hard-to-abate sectors, produced via low-carbon pathways such as electrolysis powered by renewables or fossil reforming with carbon capture. Economic viability depends on scaling electrolyzer manufacturing, reducing renewable electricity costs, and developing transport and storage infrastructure. Sector-specific applications—industrial feedstocks, heavy transport, and seasonal energy storage—have differing technical and economic requirements. Policy instruments, such as production subsidies, standards, and strategic infrastructure investment, can catalyze market formation. Lifecycle emissions accounting ensures that hydrogen deployment delivers genuine decarbonization benefits.",
    "label": "ai"
  },
  {
    "text": "Social movements mobilize collective action to contest power structures, frame grievances, and advocate for social change. Movement outcomes depend on resource mobilization, political opportunities, organizational capacity, and framing strategies that resonate with broader publics. Digital platforms have transformed recruitment and diffusion dynamics but also present challenges related to surveillance and ephemeral engagement. Comparative research examines how movements influence policy, public discourse, and institutional reforms, highlighting both disruptive and integrative pathways. Intersectional analyses emphasize how social identities shape participation and leadership within movements.",
    "label": "ai"
  },
  {
    "text": "Biomaterials design interfaces biology and materials engineering to create implants, tissue scaffolds, and drug-delivery systems that interact compatibly with physiological environments. Key considerations include biocompatibility, mechanical properties, degradation kinetics, and immunogenicity. Advanced fabrication methods—such as 3D bioprinting and electrospinning—enable complex architectures that support cell growth and function. Regulatory pathways require preclinical safety and efficacy evidence, and clinical translation depends on scalable manufacturing and sterilization processes. Interdisciplinary collaboration among clinicians, engineers, and material scientists advances biomaterials toward meaningful therapeutic applications.",
    "label": "ai"
  },
  {
    "text": "Addressing plastic pollution involves upstream interventions—such as design for recyclability, reduction of single-use plastics, and extended producer responsibility—and downstream measures like waste management infrastructure and circular material flows. Policy mixes that combine regulatory bans, economic incentives, and public awareness campaigns influence producer and consumer behavior. Innovation in materials science and sorting technologies improves recyclability and market value of secondary materials. International cooperation addresses transboundary plastic movement and fosters harmonized standards for production and waste handling. Equity considerations ensure that informal waste workers and low-income communities are protected during transitions.",
    "label": "ai"
  },
  {
    "text": "High-dimensional inference addresses estimation and hypothesis testing when the number of parameters rivals or exceeds sample size, common in genomics and machine learning. Regularization techniques—such as LASSO and ridge regression—impose sparsity or shrinkage to stabilize estimates, while post-selection inference methods provide valid uncertainty quantification. Dimension-reduction and debiasing approaches help recover interpretable effects. Cross-validation and stability selection assist in tuning and assessing robustness. Methodological rigor requires careful consideration of model selection bias, overfitting, and the reproducibility of results in high-dimensional settings.",
    "label": "ai"
  },
  {
    "text": "Rapid urbanization in emerging economies presents opportunities for agglomeration benefits and economic growth but also challenges in infrastructure provision, housing affordability, and environmental management. Integrated urban planning that anticipates population growth, invests in public transport, and expands basic services can harness urbanization for inclusive development. Informal settlements require tailored upgrading approaches that secure tenure and provide essential services. Financing mechanisms—such as municipal bonds and land value capture—support infrastructure investments when paired with governance reforms. Participatory planning processes increase legitimacy and responsiveness to local needs.",
    "label": "ai"
  },
  {
    "text": "Wetland restoration restores hydrological regimes, vegetation communities, and habitat functions that support biodiversity and provide ecosystem services like flood attenuation and water purification. Successful restoration requires understanding historical baselines, source-sink dynamics, and landscape connectivity. Socio-economic valuation of wetland services helps secure funding and stakeholder buy-in, while governance arrangements address competing land uses. Monitoring and adaptive management detect recovery trajectories and inform corrective actions. Climate change considerations, such as altered hydrological cycles, should inform restoration targets and long-term stewardship plans.",
    "label": "ai"
  },
  {
    "text": "Investment in education yields returns both at the individual level—through higher earnings and employment prospects—and at the societal level via productivity gains and innovation. Returns vary by education level, quality, and labor market conditions, with early childhood and secondary education often producing high social returns in low- and middle-income contexts. Complementary investments in health and nutrition enhance learning outcomes. Cost-effectiveness analyses guide resource allocation across interventions, while equity-focused policies ensure access for marginalized groups. Longitudinal data linking education to labor market outcomes improve the evidence base for policy prioritization.",
    "label": "ai"
  },
  {
    "text": "Administrative law governs public agencies’ decision-making processes, ensuring legality, transparency, and procedural fairness. Reforms that streamline administrative procedures, enhance judicial review mechanisms, and implement standards for evidence-based rulemaking improve governance quality. Digitalization of administrative services increases accessibility but requires safeguards for due process and data protection. Capacity-building for civil servants in regulatory impact assessment and stakeholder consultation strengthens policy outcomes. Ensuring avenues for public participation and appeals preserves accountability and trust in administrative institutions.",
    "label": "ai"
  },
  {
    "text": "Suicide prevention strategies integrate upstream approaches—such as restricting access to means, promoting help-seeking behavior, and reducing stigma—with clinical interventions like crisis services and evidence-based psychotherapies. Surveillance systems that identify high-risk populations and hot spots enable targeted interventions. School- and community-based programs that build coping skills and social support networks reduce risk among youth. Health system integration ensures continuity of care after attempts and supports for families. Multisectoral collaboration, including media guidelines and workplace policies, contributes to comprehensive prevention frameworks.",
    "label": "ai"
  },
  {
    "text": "Domain adaptation addresses performance degradation when computer vision models trained on one data distribution are deployed in differing target environments. Techniques include unsupervised and semi-supervised learning, adversarial alignment of feature representations, and data augmentation to bridge domain gaps. Evaluating adaptation methods requires realistic benchmarks that capture variability in lighting, sensor characteristics, and scene composition. Robust domain adaptation enhances model generalizability for applications in robotics, medical imaging, and remote sensing where data shifts are common. Combining domain knowledge with algorithmic strategies improves transfer effectiveness.",
    "label": "ai"
  },
  {
    "text": "Fisheries co-management shares governance responsibilities between government agencies and local fishing communities to align conservation objectives with livelihoods. Co-management arrangements can improve compliance, local stewardship, and context-sensitive regulation when communities have recognized rights and decision-making capacity. Capacity-building, conflict resolution mechanisms, and inclusive representation are essential to equitable co-management. Integration of scientific stock assessments with traditional ecological knowledge enhances legitimacy and practical management. Adaptive co-management supports iterative learning and adjustment to environmental and socio-economic changes.",
    "label": "ai"
  },
  {
    "text": "Cognitive aging involves heterogeneous changes in memory, processing speed, and executive function across individuals, influenced by genetics, lifestyle, and vascular health. Interventions—such as physical activity, cognitive training, and cardiovascular risk management—show promise in preserving cognitive function. Lifecourse approaches that address education, social engagement, and chronic disease control contribute to cognitive resilience. Biomarker research and longitudinal cohorts elucidate trajectories and mechanisms, informing early intervention strategies. Policies that support aging-in-place and accessible services enhance quality of life for older adults experiencing cognitive decline.",
    "label": "ai"
  },
  {
    "text": "Recycling rare earth elements from electronic waste and end-of-life products reduces reliance on primary extraction and mitigates supply-chain vulnerabilities. Technical challenges include efficient separation, recovery from complex matrices, and economic viability given low concentrations. Research into hydrometallurgical and bioleaching techniques improves recovery rates, while product design that facilitates disassembly enhances recyclability. Policy measures—such as extended producer responsibility and incentives for secondary-material markets—support recycling infrastructure development. International cooperation addresses trade flows and environmental standards for recycling operations.",
    "label": "ai"
  },
  {
    "text": "Labor market polarization describes the expansion of high- and low-wage occupations at the expense of middle-skill jobs, driven by technological change, routinization, and globalization. Policies to address polarization include upskilling programs, vocational training, and incentives for job creation in middle-skill areas. Social safety nets and active labor market policies support displaced workers during transitions. Educational systems that emphasize both cognitive and technical skills, as well as lifelong learning frameworks, increase adaptability. Empirical assessment of polarization requires disaggregated occupational data and attention to geographic heterogeneity.",
    "label": "ai"
  },
  {
    "text": "Antenatal care provides opportunities to monitor maternal and fetal health, deliver preventive interventions, and prepare for safe childbirth. Evidence-based antenatal packages include screening for hypertensive disorders, anemia, and infectious disease, along with nutritional supplementation and counseling. Quality antenatal care depends on trained providers, continuity of services, and referral systems for complications. Community outreach and task-sharing with trained ancillary health workers increase coverage in resource-limited settings. Monitoring quality indicators and birth outcomes informs program improvement and resource allocation.",
    "label": "ai"
  },
  {
    "text": "Remote sensing technologies provide synoptic observations for monitoring land-cover change, vegetation health, and water resources at multiple scales. Satellite and airborne sensors generate time-series data that inform environmental assessments and policy decisions, such as deforestation tracking and drought early warning. Integration with ground-based measurements and machine learning-based classification improves accuracy and contextual interpretation. Open data policies and capacity-building enable wider use by policymakers, researchers, and local stakeholders. Sensor calibration, atmospheric correction, and validation protocols are critical for reliable application of remote sensing products.",
    "label": "ai"
  },
  {
    "text": "Inequality of opportunity focuses on disparities arising from circumstances beyond individuals’ control—such as parental socio-economic status, race, or place of birth—that shape life chances. Policy interventions include early childhood programs, equitable school funding, and targeted scholarships to mitigate inherited disadvantages. Measuring opportunity often entails decomposition methods that distinguish between effort-dependent and circumstance-driven inequalities. Longitudinal data and intergenerational studies illuminate persistence of disadvantage and the effectiveness of policy levers to promote upward mobility.",
    "label": "ai"
  },
  {
    "text": "Algorithmic fairness examines criteria and techniques to ensure that automated decision systems do not produce unjust disparate impacts across protected groups. Fairness definitions—such as demographic parity, equalized odds, and individual fairness—are context-dependent and may be mutually incompatible, requiring normative judgments. Technical mitigation strategies include pre-processing, in-processing, and post-processing adjustments, alongside transparency and audit mechanisms. Stakeholder engagement clarifies fairness priorities and acceptable trade-offs. Legal and regulatory frameworks provide complementary protections by defining prohibited discriminatory outcomes and avenues for redress.",
    "label": "ai"
  },
  {
    "text": "Disaster risk reduction (DRR) reduces vulnerability through preparedness, resilient infrastructure, and land-use planning that avoids high-risk zones. Early warning systems, community drills, and contingency financing instruments such as catastrophe bonds enhance readiness and rapid response. Integrating DRR into development planning avoids lock-in of exposures and supports recovery that builds back better. Multi-hazard approaches that account for cascading risks and climate change improve robustness. Engagement with local knowledge and inclusive planning ensures culturally appropriate and equitable DRR strategies.",
    "label": "ai"
  },
  {
    "text": "Cost-effectiveness analysis (CEA) evaluates health interventions by comparing costs to health outcomes, often using quality-adjusted life years or disability-adjusted life years as common metrics. CEA informs priority-setting by identifying interventions that provide the greatest health gains per unit cost, though distributional concerns require supplementary equity analyses. Transparent reporting of assumptions, discount rates, and sensitivity analyses enhances policy relevance. Contextual factors—such as health system capacity and affordability thresholds—affect decision-making. Combining economic evaluation with stakeholder preferences supports legitimate resource allocation in health systems.",
    "label": "ai"
  },
  {
    "text": "Valuing pollination services quantifies the contribution of animal pollinators to crop production and ecosystem function, informing conservation investment decisions. Methods range from production-function approaches that link pollinator presence to yield outcomes to contingent valuation surveys capturing willingness to pay for conservation. Valuation must account for spatial heterogeneity, crop dependency on pollinators, and substitution possibilities. Integrating ecological and economic models supports policy design that balances agricultural productivity with habitat conservation. Consideration of non-market values and cultural significance enriches valuation exercises beyond purely economic metrics.",
    "label": "ai"
  },
  {
    "text": "Corporate environmental reporting enhances transparency about firms’ environmental impacts, resource use, and climate-related risks, informing investors and stakeholders. Standardized frameworks—such as the Task Force on Climate-related Financial Disclosures—promote comparability and risk disclosure that feed into capital allocation. Quality of reporting depends on reliable data, third-party assurance, and integration with corporate strategy. Regulatory mandates for disclosure, coupled with capacity-building for small and medium enterprises, expand uptake. Clear reporting supports accountability and incentivizes emission reductions through market and reputational channels.",
    "label": "ai"
  },
  {
    "text": "Understanding and managing fire regimes requires ecological knowledge of fuel dynamics, historical fire patterns, and climate influences. Fire suppression policies have altered fuel loads in many ecosystems, increasing the risk of high-intensity fires. Integrated management approaches combine controlled burning, mechanical fuel reduction, and community preparedness to reduce catastrophic fires and maintain ecological functions. Indigenous fire stewardship practices offer valuable insights into landscape-scale management. Monitoring and adaptive strategies informed by fire ecology improve resilience to shifting climatic conditions and human development pressures.",
    "label": "ai"
  },
  {
    "text": "Reproducible research practices—such as version-controlled code, open data, and literate programming—improve transparency and credibility in statistical analysis. Pre-registration of analysis plans reduces researcher degrees of freedom and publication bias, while containerization ensures computational environments are portable. Journals and funders that mandate data and code sharing foster a culture of reproducibility, although ethical constraints and privacy concerns require controlled-access solutions for sensitive datasets. Training researchers in reproducible workflows enhances methodological rigor and accelerates cumulative science.",
    "label": "ai"
  },
  {
    "text": "Affordable housing policies address supply shortages and rising housing costs through mixed approaches: inclusionary zoning, housing vouchers, supportive housing programs, and incentives for nonprofit developers. Land-use reform to increase density near transit corridors expands supply while preserving accessibility. Financing mechanisms—such as low-income housing tax credits and public land disposition—support project viability. Tenant protections and anti-displacement strategies maintain community stability. Effective policy requires integration with transportation, economic development, and social services to address multidimensional needs of low-income households.",
    "label": "ai"
  },
  {
    "text": "Kelp forests are productive coastal ecosystems that support fisheries, biodiversity, and coastal protection. Threats include warming waters, herbivore outbreaks, and pollution that reduce kelp resilience. Restoration efforts involve re-establishing herbivore predators, kelp replanting, and water-quality improvements. Ecosystem-based management that protects trophic interactions and addresses land-based pollution yields more sustainable outcomes than single-species interventions. Monitoring ecological indicators and engaging coastal communities in stewardship fosters local support and adaptive management strategies.",
    "label": "ai"
  },
  {
    "text": "Sequence alignment algorithms facilitate comparative genomics, phylogenetics, and functional annotation by identifying homologous regions across DNA, RNA, or protein sequences. Heuristic methods—such as BLAST—balance speed and sensitivity for large databases, while dynamic programming approaches provide optimal alignments for smaller datasets. Multiple sequence alignment and profile-based methods underpin evolutionary inference and motif discovery. Challenges include handling repetitive regions, structural variation, and scaling to pangenome analyses. Benchmarking alignment tools and integrating structural information enhance accuracy for downstream biological interpretation.",
    "label": "ai"
  },
  {
    "text": "Conditional cash transfer (CCT) programs provide income support contingent on behaviors such as school attendance or health checkups, aiming to reduce poverty while promoting human capital accumulation. Evidence from randomized evaluations shows positive effects on attendance and health service utilization, with longer-term benefits in some contexts for education and earnings. Program design—including transfer size, conditionality enforcement, and targeting—affects effectiveness. Administrative capacity and social acceptability are key for scaling. Complementary investments in service quality ensure that conditions translate into improved outcomes rather than merely financial compliance.",
    "label": "ai"
  },
  {
    "text": "Hypersonic flight poses engineering challenges in thermal protection, propulsion, and materials that withstand extreme temperatures and pressures at speeds exceeding Mach 5. Scramjet engines, advanced ceramics, and active cooling systems are areas of active research to address sustained hypersonic propulsion and vehicle integrity. Guidance, navigation, and control systems must compensate for complex aerothermal environments and variable atmospheric conditions. Regulatory, safety, and defense considerations intersect with civilian applications such as rapid transport, requiring rigorous testing and international coordination on safety standards and permissible uses.",
    "label": "ai"
  },
  {
    "text": "Survival analysis models time-to-event data accounting for censoring and time-varying covariates common in clinical and epidemiological research. Parametric, semi-parametric (e.g., Cox proportional hazards), and flexible spline-based models provide different trade-offs in interpretability and fit. Competing risks and recurrent event frameworks extend analysis to complex event structures. Proper handling of censoring mechanisms and model diagnostics ensures valid inference. Recent developments integrate machine learning for risk prediction while maintaining interpretability essential for clinical decision-making.",
    "label": "ai"
  },
  {
    "text": "Trophic cascades occur when changes at one trophic level propagate through food webs, altering community composition and ecosystem functions. Classic examples include predator removal leading to herbivore overabundance and vegetation decline. Management interventions that restore apex predators or control herbivore populations can reverse cascade effects but must consider social acceptability and broader ecological dynamics. Empirical studies combining experimental manipulations and long-term monitoring elucidate cascade mechanisms and inform conservation strategies that maintain ecosystem balance.",
    "label": "ai"
  },
  {
    "text": "Transfer learning improves model performance in data-scarce target domains by leveraging representations learned from related source tasks. Fine-tuning pretrained models, domain-adaptive pretraining, and feature-based transfer are common approaches that reduce training costs and improve generalization. Selection of appropriate source tasks and mitigation of negative transfer are critical to success. Transfer learning has enabled advances in natural language processing, computer vision, and speech recognition, particularly where massive labeled datasets exist for pretraining. Evaluation should examine robustness, fairness, and applicability across diverse target populations.",
    "label": "ai"
  },
  {
    "text": "Industrial siting decisions often concentrate pollution in marginalized communities, raising environmental justice concerns about exposure disparities and attendant health effects. Decision-making processes that lack meaningful community input perpetuate inequities. Policies that require cumulative impact assessments, community benefit agreements, and stricter permitting standards address distributional harms. Remediation funds and targeted health interventions can mitigate legacy effects, while inclusive planning reforms prevent future disparities. Integrating environmental justice into land-use and economic development planning promotes equitable outcomes.",
    "label": "ai"
  },
  {
    "text": "Green catalysis develops chemical processes that minimize environmental impacts by improving selectivity, reducing energy consumption, and enabling benign solvents or solvent-free reactions. Heterogeneous catalysts that are easily separable and recyclable reduce waste streams, while biocatalysis offers mild reaction conditions and high enantioselectivity for complex molecules. Lifecycle assessment guides selection of catalytic routes that balance performance with environmental footprints. Scaling green catalytic processes requires integration with process intensification, reactor design, and supply-chain considerations to deliver sustainable industrial chemistry solutions.",
    "label": "ai"
  },
  {
    "text": "Promoting active transport—walking and cycling—improves public health, reduces emissions, and enhances urban livability. Infrastructure investments such as protected bike lanes, pedestrianized streets, and traffic calming measures increase safety and uptake. Integrating active transport with public transit and land-use planning supports multimodal trips. Behavioral campaigns and incentives encourage modal shift, while equity considerations ensure safe access for all neighborhoods. Monitoring usage and safety outcomes informs iterative planning and resource allocation.",
    "label": "ai"
  },
  {
    "text": "Species reintroduction aims to restore extirpated populations and reestablish ecosystem functions but requires careful assessment of habitat suitability, genetic diversity, and socio-political feasibility. Pre-release planning includes disease screening, stakeholder engagement, and post-release monitoring to evaluate survival, reproduction, and ecological interactions. Reintroductions may conflict with human land use or perceptions of risk, necessitating mitigation strategies and compensation mechanisms. Adaptive management frameworks enable learning from outcomes and adjusting protocols for future translocations.",
    "label": "ai"
  },
  {
    "text": "Regulatory impact assessment (RIA) systematically evaluates the economic, social, and environmental implications of proposed regulations to inform policymaking. RIAs compare policy options, estimate costs and benefits, and consider distributional effects and compliance feasibility. Institutionalizing RIA requires analytical capacity, stakeholder consultation, and transparent reporting. While RIA enhances evidence-informed regulation, maintaining independence and quality assurance prevents politicization. Iterative review and post-implementation evaluation close the feedback loop for policy improvement.",
    "label": "ai"
  },
  {
    "text": "Attention mechanisms allocate cognitive resources to relevant stimuli, enhancing perception and decision-making. Neural substrates include frontoparietal networks and modulatory neurotransmitter systems that dynamically prioritize information processing. Computational models of attention inform artificial systems by implementing selective weighting and gating mechanisms. Clinical disorders such as ADHD illustrate how attentional dysregulation affects functioning and responsiveness to pharmacological and behavioral interventions. Interdisciplinary research connecting cognitive neuroscience, computational models, and applied therapies advances understanding and treatment of attentional disorders.",
    "label": "ai"
  },
  {
    "text": "Green supply chains integrate environmental considerations into procurement, production, and logistics to reduce ecological footprints and enhance resource efficiency. Practices include supplier sustainability assessments, low-emission transport modes, and circular product design. Corporate procurement policies and regulatory incentives drive adoption, while life-cycle assessment identifies hotspots for intervention. Transparency through supply-chain traceability and reporting supports stakeholder scrutiny and continuous improvement. Engaging suppliers, particularly small and medium enterprises, with capacity-building and incentives fosters broader sustainability adoption across value chains.",
    "label": "ai"
  },
  {
    "text": "Vector-borne disease control combines environmental management, vector surveillance, and biomedical interventions such as insecticide-treated nets and vaccines. Integrated vector management tailors interventions to local ecology and transmission dynamics, balancing chemical, biological, and environmental measures to reduce resistance development. Community participation in source reduction and behavior change complements technical interventions. Surveillance that links entomological data with case reporting enables targeted responses. Cross-sector collaboration with urban planning and water management reduces breeding habitats and supports sustained control efforts.",
    "label": "ai"
  },
  {
    "text": "Island biogeography theory links species richness to island area and isolation, shaping understanding of colonization, extinction, and speciation dynamics. Conservation on islands requires attention to endemism, invasive species threats, and small population vulnerabilities. Biosecurity measures, habitat protection, and control of invasive predators are central to preserving island biodiversity. Restoration efforts incorporate genetic management and captive-breeding where necessary, with long-term monitoring to assess demographic and ecological recovery.",
    "label": "ai"
  },
  {
    "text": "Reinforcement learning (RL) trains agents to maximize cumulative rewards through interaction with environments, enabling applications in robotics, game-playing, and resource allocation. Challenges include sample efficiency, credit assignment, and safety in real-world deployments. Model-based RL, hierarchical approaches, and imitation learning improve learning speed and robustness. Reward specification and alignment with human values are critical to avoid unintended behaviors. Benchmarking in realistic simulation environments and incorporating human feedback enhance practical transfer to deployed systems.",
    "label": "ai"
  },
  {
    "text": "Decentralization transfers authority and resources from central governments to subnational entities to improve public service responsiveness and accountability. Design choices—such as fiscal, administrative, or political decentralization—determine capacity requirements and potential trade-offs between local autonomy and national cohesion. Successful decentralization involves capacity-building, clear revenue-sharing arrangements, and mechanisms for oversight to prevent capture. Empirical evaluations show that outcomes depend on institutional quality, intergovernmental coordination, and local governance practices.",
    "label": "ai"
  },
  {
    "text": "Aerosols influence climate through scattering and absorbing radiation and by modifying cloud properties, producing net cooling effects that partially offset greenhouse warming in some regions. Spatially heterogeneous aerosol distributions complicate attribution of regional climate impacts and precipitation changes. Reductions in aerosol emissions improve air quality but may unmask additional warming, requiring integrated mitigation strategies that consider both pollutants and greenhouse gases. Robust observational networks and coupled climate–chemistry models improve understanding of aerosol–cloud interactions and inform policy trade-offs between air quality and climate objectives.",
    "label": "ai"
  },
  {
    "text": "Urban labor markets concentrate diverse industries, skills, and productive interactions that drive agglomeration economies and innovation clusters. Spatial mismatch between jobs and affordable housing can limit labor-market access for low-income workers. Policies such as targeted transit investments, workforce development programs, and inclusive zoning promote spatial equity in job access. Local economic development strategies that nurture small businesses and sectoral clustering support job creation while complementing regional planning to manage commuting patterns and environmental externalities.",
    "label": "ai"
  },
  {
    "text": "Genetic rescue introduces new genetic variation into small, inbred populations to reduce inbreeding depression and increase adaptive potential. Translocation strategies require careful genetic assessment to avoid outbreeding depression and to preserve locally adapted traits. Monitoring demographic and genetic outcomes post-introduction assesses efficacy and informs future interventions. Ethical and logistical considerations include stakeholder consent, regulatory approval, and balancing genetic interventions with habitat conservation to address root causes of population decline.",
    "label": "ai"
  },
  {
    "text": "Work–life balance policies address the interplay between employment demands and personal responsibilities, affecting well-being, productivity, and gender equity. Flexible work arrangements, parental leave, and access to affordable childcare support workforce participation and reduce career penalties associated with caregiving. Organizational cultures that normalize boundary setting and equitable distribution of domestic labor reinforce policy measures. Evaluations should assess heterogeneity of impacts across occupations, income levels, and family structures to inform tailored interventions.",
    "label": "ai"
  },
  {
    "text": "Cultural ecosystem services—such as spiritual, recreational, and aesthetic values—are challenging to quantify but crucial for holistic valuation of ecosystems. Participatory methods, qualitative assessments, and non-market valuation techniques capture diverse stakeholder values that inform conservation priorities. Recognizing cultural services in planning supports community identity and equitable heritage protection. Integrating cultural valuations with ecological and provisioning service analyses ensures comprehensive decision-making that respects intangible benefits of nature.",
    "label": "ai"
  },
  {
    "text": "Hospital antibiotic stewardship programs optimize antimicrobial use through guidelines, prospective audit with feedback, and dose optimization to reduce resistance and improve patient outcomes. Diagnostics that enable rapid pathogen identification support targeted therapy and shorter courses. Education for prescribers, formulary restrictions, and computerized decision support enhance stewardship adherence. Monitoring antibiotic consumption patterns and resistance trends guides policy adjustments. Integrating stewardship with infection prevention and control creates synergistic effects in reducing healthcare-associated infections.",
    "label": "ai"
  },
  {
    "text": "Behavioral finance integrates psychological insights into financial decision-making, explaining anomalies such as overconfidence, herd behavior, and market inefficiencies. Investor biases affect asset pricing, portfolio allocation, and market volatility. Regulatory and product-design interventions—such as standardized disclosure formats, default investment options, and nudges toward diversification—mitigate behavioral pitfalls. Empirical testing in laboratory and field settings refines theoretical models and informs financial education initiatives to improve market outcomes and investor welfare.",
    "label": "ai"
  },
  {
    "text": "Photovoltaic innovation focuses on improving efficiency, lowering production costs, and enhancing durability of solar cells. Developments in perovskite materials, tandem cell architectures, and thin-film manufacturing show promise for next-generation photovoltaics. Stability, scaling of deposition techniques, and environmental considerations in material choice are central to commercialization. Integration with building materials and storage systems maximizes utility across application contexts. Policy incentives, research funding, and supportive supply chains accelerate deployment while addressing lifecycle sustainability.",
    "label": "ai"
  },
  {
    "text": "Migration studies examine the causes, processes, and consequences of human mobility, encompassing labor migration, forced displacement, and transnational diaspora dynamics. Factors driving migration include economic differentials, conflict, environmental change, and social networks that facilitate movement. Host-country integration policies, remittance flows, and transnational ties shape migrants’ socio-economic trajectories. Ethical research practices prioritize migrants’ agency and safety, while policy debates focus on rights, labor protections, and pathways to regularization. Comparative and longitudinal studies deepen understanding of migration’s long-term impacts.",
    "label": "ai"
  },
  {
    "text": "Developmental plasticity refers to organisms’ capacity to alter phenotype in response to environmental conditions during critical developmental windows, influencing life-history trajectories and fitness. Epigenetic mechanisms and hormonal signaling mediate persistent changes that can have adaptive or maladaptive consequences. Understanding plastic responses informs conservation under rapid environmental change and human health interventions targeting early-life conditions. Experimental and longitudinal studies elucidate constraints, trade-offs, and the evolutionary implications of plasticity across taxa.",
    "label": "ai"
  },
  {
    "text": "A just transition framework seeks to equitably manage workforce and community impacts of decarbonization by providing retraining, income support, and regional economic diversification. Policymakers should co-design transition strategies with affected workers, unions, and local stakeholders to ensure legitimacy and tailored support. Investment in clean industries, infrastructure, and education creates alternative employment pathways, while social protection measures mitigate short-term hardships. Monitoring and transparent allocation of transition funds enhance accountability and equitable outcomes.",
    "label": "ai"
  },
  {
    "text": "Metrics for restoration ecology should capture ecological function, biodiversity recovery, and resilience rather than solely structural proxies. Indicators include species composition, trophic interactions, soil health, and ecosystem services provisioning. Baseline establishment and reference sites inform realistic targets, while adaptive monitoring reveals trajectories and informs management adjustments. Socio-economic metrics that reflect benefits to local communities complement ecological indicators and support sustained stewardship. Transparent reporting of outcomes facilitates learning across restoration projects and bioregions.",
    "label": "ai"
  },
  {
    "text": "Sleep plays a critical role in cognitive processes including memory consolidation, emotional regulation, and metabolic restoration. Sleep architecture—comprising REM and non-REM stages—supports distinct neural processes that underlie consolidation and synaptic homeostasis. Chronic sleep deprivation impairs attention, executive function, and mood, with public health implications for occupational safety and learning. Interventions that improve sleep hygiene, address sleep disorders, and optimize scheduling in shift work contexts contribute to cognitive health. Multimodal research combining neuroimaging, polysomnography, and behavioral assays elucidates sleep–cognition relationships.",
    "label": "ai"
  },
  {
    "text": "Electrification of heating—through heat pumps and district heating systems—reduces reliance on fossil fuels and complements renewable electricity deployment. Efficiency standards, building codes, and incentives for retrofits accelerate adoption while ensuring consumer protection. Grid implications include increased winter peak demand, calling for demand-side management and thermal storage integration. Equity policies mitigate potential cost burdens for low-income households and support targeted subsidies. Whole-systems planning that aligns building electrification with decarbonized power supplies maximizes climate benefits.",
    "label": "ai"
  },
  {
    "text": "Global justice debates address moral obligations across national boundaries regarding poverty alleviation, resource distribution, and human rights. Theories range from cosmopolitan egalitarianism, which emphasizes distributive duties to individuals irrespective of citizenship, to statist accounts that prioritize domestic obligations grounded in political communities. Practical policy considerations involve trade, aid, migration, and institutional reforms that reduce global inequalities. Philosophical analyses inform normative criteria for fairness and the design of international institutions that balance sovereignty with global solidarity.",
    "label": "ai"
  },
  {
    "text": "Industrial policy seeks to promote structural transformation and productivity growth through targeted support for sectors with comparative advantages or strategic importance. Instruments include subsidies for R&D, targeted tax incentives, skill development programs, and infrastructure investments. Successful industrial policy requires credible selection mechanisms, competition safeguards to avoid rent-seeking, and evaluation frameworks to assess outcomes. Coordination across fiscal, trade, and education policies enhances effectiveness, while flexibility enables adaptation to technological change and global market dynamics.",
    "label": "ai"
  },
  {
    "text": "Urban ecology studies interactions between ecological processes and urban environments, examining how cities can support biodiversity, ecosystem services, and human well-being. Green infrastructure, wildlife-friendly design, and habitat corridors enhance urban biodiversity while providing benefits such as air quality improvement and thermal regulation. Citizen science and participatory monitoring engage residents in data collection and stewardship. Interdisciplinary approaches integrating planning, ecology, and social sciences inform interventions that reconcile urban development with ecological sustainability.",
    "label": "ai"
  },
  {
    "text": "Wearable sensors enable continuous physiological and environmental monitoring for healthcare, occupational safety, and fitness applications. Advances in flexible electronics, low-power communication, and miniaturized biosensors support noninvasive measurement of vital signs, biomarkers, and activity patterns. Data privacy, battery life, and calibration across diverse user populations remain technical and ethical challenges. Integration with clinical pathways and standards for data interoperability facilitate translation into healthcare settings. User-centered design ensures acceptability and sustained engagement with wearable technologies.",
    "label": "ai"
  },
  {
    "text": "Translating evidence into policy requires effective knowledge brokering, timely synthesis, and communication tailored to decision-makers’ contexts. Policymaker demand for succinct, actionable briefs, scenario analyses, and implementation guidance enhances uptake. Institutional mechanisms—such as policy labs, embedded researchers, and contract evaluation units—bridge the research–policy divide. Co-production of research with stakeholders increases relevance and buy-in. Monitoring the impact of evidence use informs improvements in translation practices and supports evidence-informed governance cultures.",
    "label": "ai"
  },
  {
    "text": "Host–pathogen coevolution describes reciprocal adaptations that influence virulence, immunity, and epidemiological dynamics. Trade-offs between transmissibility and pathogenicity, genetic diversity in host resistance, and environmental contexts shape evolutionary trajectories. Understanding coevolutionary processes informs vaccine design, antimicrobial strategies, and ecological interventions to manage disease emergence. Integrating genomic surveillance with experimental evolution studies reveals mechanisms of adaptation and potential intervention points to limit harmful pathogen evolution.",
    "label": "ai"
  },
  {
    "text": "Carbon leakage occurs when climate policies in one jurisdiction lead to emissions shifting to less-regulated regions, undermining global mitigation efforts. Border carbon adjustments, coordinated international carbon pricing, and technology transfer can mitigate leakage risks while maintaining competitiveness. Policy design must account for measurement challenges, administrative feasibility, and compliance with trade rules. Supporting domestic industries through innovation and low-carbon competitiveness strategies reduces incentives for offshoring emissions-intensive production.",
    "label": "ai"
  },
  {
    "text": "Language acquisition research investigates how infants and children acquire phonology, syntax, and vocabulary through exposure and interaction. Statistical learning, social-pragmatic cues, and innate predispositions interact to facilitate learning across developmental stages. Cross-linguistic studies illuminate universal and language-specific processes, informing theories of cognitive architecture. Early interventions that support language-rich environments and parental responsive interactions promote robust language development and long-term educational outcomes.",
    "label": "ai"
  },
  {
    "text": "Renewable energy subsidies accelerate deployment by reducing upfront costs and bridging commercialization gaps, but require careful design to avoid market distortions and fiscal burdens. Technology-neutral auctions, declining price caps, and sunset clauses encourage cost reductions and competition. Complementary policies—such as grid integration and storage incentives—ensure system reliability as renewables scale. Transparent subsidy allocation and evaluation allow policymakers to adjust instruments in response to market maturity and technological progress.",
    "label": "ai"
  },
  {
    "text": "Shifts in family demographics—including declining fertility, delayed marriage, and diverse household forms—reflect socio-economic changes, gender norms, and policy environments. Implications for social policy include childcare provision, pension systems, and housing demand. Understanding variations across cultural and economic contexts aids policymaking that supports families’ diverse needs. Longitudinal studies capture transitions and inform interventions that promote family stability and child well-being.",
    "label": "ai"
  },
  {
    "text": "Telemedicine regulation must balance access expansion with quality assurance, privacy protections, and equitable reimbursement frameworks. Licensing across jurisdictions, standards for clinical appropriateness, and integration with electronic health records facilitate safe telehealth delivery. Policies that reimburse telemedicine at parity with in-person care can sustain service availability post-emergency use, while attention to digital inclusion ensures vulnerable populations are not left behind. Monitoring outcomes and patient satisfaction informs regulatory refinement and best-practice development.",
    "label": "ai"
  },
  {
    "text": "Conservation prioritization employs spatial optimization, cost–benefit analysis, and species vulnerability assessments to allocate limited conservation resources effectively. Incorporating climate change projections and socio-economic constraints yields robust prioritization under uncertainty. Stakeholder engagement and transparent criteria enhance legitimacy and local adoption. Prioritization frameworks should balance biodiversity representation, ecosystem services, and threats to maximize conservation returns while respecting community needs.",
    "label": "ai"
  },
  {
    "text": "Sustainable textiles innovation seeks to reduce environmental impacts across fiber production, dyeing, and end-of-life disposal by promoting low-impact fibers, closed-loop dyeing processes, and recycling technologies. Life-cycle assessment identifies hotspots for intervention, while circular business models facilitate reuse and remanufacture. Policy levers—such as extended producer responsibility and labeling standards—encourage industry shifts. Consumer behavior, affordability, and supply-chain transparency influence adoption of sustainable textile solutions at scale.",
    "label": "ai"
  },
  {
    "text": "Preventing noncommunicable diseases involves population-level strategies targeting tobacco control, unhealthy diets, physical inactivity, and harmful alcohol use. Policy tools include taxation, advertising restrictions, and urban designs that promote active lifestyles. Health promotion campaigns and community programs tailored to local contexts support behavior change. Integrating prevention into primary care and workplace wellness programs expands reach. Monitoring risk factor trends and evaluating interventions ensure efficient allocation of public health resources.",
    "label": "ai"
  },
  {
    "text": "Soft robotics leverages compliant materials and bioinspired designs to create robots capable of safe interaction with humans and delicate environments. Applications include medical devices, wearable assistive technologies, and exploratory platforms that navigate unstructured terrains. Challenges include actuation mechanisms, material durability, and control strategies for deformable structures. Advances in soft actuators, sensors, and embedded control systems improve functionality, while rigorous testing ensures reliability and safety in human-centered applications.",
    "label": "ai"
  },
  {
    "text": "Behavioral public economics incorporates psychological realism into analyses of taxation, welfare programs, and public goods provision, accounting for framing effects, present bias, and social preferences. Policy designs that use defaults, commitment devices, and tailored information can increase program effectiveness while enhancing welfare. Evaluations should test behavioral interventions in real-world settings to assess generalizability and distributional impacts. Ethical considerations about paternalism and transparency guide responsible application of behavioral tools in public policy.",
    "label": "ai"
  },
  {
    "text": "Grassland restoration focuses on reestablishing native vegetation, soil health, and grazing regimes that support biodiversity and carbon storage. Restoration practices include reseeding with native species, controlled burns, and regenerative grazing that enhance ecosystem resilience. Engaging pastoralist communities and aligning restoration with sustainable livelihoods encourages adoption. Monitoring plant community composition, soil carbon, and faunal responses assesses restoration success and guides adaptive management.",
    "label": "ai"
  },
  {
    "text": "Regulatory sandboxes provide controlled environments for innovation by allowing firms to test novel products under relaxed regulatory conditions with supervision. Sandboxes accelerate learning, inform regulatory adaptation, and reduce time-to-market for emerging technologies such as fintech and digital health. Design considerations include clear eligibility criteria, consumer protection safeguards, and evaluation metrics to assess systemic risks and benefits. Outcomes should feed into regulatory reform to balance innovation facilitation with public interest protections.",
    "label": "ai"
  },
  {
    "text": "Urban heat islands result from surface modifications, reduced vegetation, and heat-absorbing materials that increase local temperatures relative to rural surroundings. Mitigation strategies include expanding urban tree canopy, implementing cool roofs and pavements, and preserving green spaces. Urban design that enhances ventilation and reduces impervious surfaces moderates heat accumulation. Heat mitigation has co-benefits for energy demand reduction and public health, particularly for vulnerable populations during heatwaves. Integrating heat considerations into planning and building codes supports long-term urban resilience.",
    "label": "ai"
  },
  {
    "text": "Antiviral strategies encompass direct-acting agents, host-targeted therapies, and prophylactic measures such as vaccines. High-throughput screening and rational drug design identify inhibitors of viral entry, replication, and assembly. Host-targeted approaches aim to reduce viral replication by modifying cellular pathways but must balance efficacy with safety. Resistance development necessitates combination therapies and surveillance. Integration of basic virology, genomics, and clinical research accelerates translation of antiviral candidates into safe and effective therapies.",
    "label": "ai"
  },
  {
    "text": "Careful experimental design ensures valid causal inference by controlling for confounders, randomizing treatment assignment, and determining appropriate sample sizes to achieve desired power. Factorial and block designs increase efficiency and allow estimation of interaction effects, while cluster-randomized designs accommodate group-level interventions. Pre-specifying analysis plans and handling attrition transparently reduce bias. Ethical considerations guide participant protection and informed consent procedures. Combining experimental rigor with contextual understanding enhances external validity and policy relevance.",
    "label": "ai"
  },
  {
    "text": "Conservation strategies encompass in situ measures—protecting species within their natural habitats—and ex situ approaches such as captive breeding and seed banks. In situ conservation maintains ecological interactions and evolutionary processes but may be constrained by habitat loss, while ex situ methods provide insurance against extinction. Integrating both strategies allows for contingency planning, reintroduction, and genetic preservation. Ethical and logistical considerations include resource allocation, genetic management, and long-term commitments to maintenance and eventual reintegration where feasible.",
    "label": "ai"
  },
  {
    "text": "Health insurance markets face challenges of adverse selection, moral hazard, and affordability that influence coverage levels and financial protection. Regulatory interventions—such as community rating, mandates, and risk adjustment—mitigate market failures but require careful balance to preserve incentives and control costs. Subsidies and public options expand access, while payment reforms encourage value-based care. Monitoring market dynamics and adjusting policy levers in response to evidence supports sustainable and equitable insurance systems.",
    "label": "ai"
  },
  {
    "text": "Efficient hydrogen storage is critical for realizing hydrogen as an energy carrier, with approaches including compressed gas, cryogenic liquids, and material-based storage such as metal hydrides and porous sorbents. Materials research focuses on increasing gravimetric and volumetric density, improving adsorption kinetics, and ensuring reversibility under practical conditions. Safety, cycling stability, and integration with fueling infrastructure are key engineering considerations. Lifecycle analyses inform trade-offs between storage methods and guide deployment decisions in transport and stationary applications.",
    "label": "ai"
  },
  {
    "text": "Integrating oral health into primary healthcare improves access to preventive services, reduces disease burden, and addresses shared risk factors with systemic conditions. Training primary care providers in basic oral screening, promoting fluoride use, and ensuring referral pathways for dental care expand reach. Financing reforms and inclusion of dental benefits in universal health coverage reduce cost barriers. Community-based oral-health promotion and school programs support early-life prevention and lifelong oral health equity.",
    "label": "ai"
  },
  {
    "text": "Agroforestry integrates trees and shrubs into agricultural landscapes to deliver multifunctional benefits—soil conservation, carbon sequestration, and diversified incomes. Design options vary from alley cropping to silvopasture, tailored to local agroecological conditions and farmer objectives. Scaling agroforestry requires secure land tenure, extension services, and market development for tree products. Monitoring socio-ecological outcomes informs practices that balance productivity with ecological restoration and resilience to climate variability.",
    "label": "ai"
  },
  {
    "text": "Self-supervised learning exploits inherent structure in unlabeled data to learn useful representations through pretext tasks, reducing dependence on labeled data for downstream tasks. Techniques such as contrastive learning, masked modeling, and predictive coding have advanced performance in vision and language domains. Challenges include designing tasks that capture semantically relevant features and ensuring robustness across domains. Self-supervised pretraining combined with fine-tuning yields state-of-the-art results while improving sample efficiency in resource-constrained settings.",
    "label": "ai"
  },
  {
    "text": "Social protection floors provide basic income security and access to essential health care, forming a foundation for inclusive social protection systems. Implementing floors involves phased approaches, fiscal planning, and targeting mechanisms to reach vulnerable populations. Complementary active labor market policies and sectoral programs support livelihoods and reduce dependency risks. Monitoring coverage and adequacy ensures that social protection contributes to poverty reduction and resilience against shocks such as economic downturns and natural disasters.",
    "label": "ai"
  },
  {
    "text": "Marine protected areas (MPAs) can conserve biodiversity and replenish fish stocks when effectively designed, enforced, and networked. No-take zones often produce the strongest ecological benefits, but achieving compliance depends on community engagement and alternative livelihood options. Connectivity between MPAs supports larval dispersal and population recovery. Monitoring biological indicators, fisheries yield, and socio-economic impacts informs assessment of MPA effectiveness and guides adaptive management to balance conservation with human needs.",
    "label": "ai"
  },
  {
    "text": "Time series analysis models temporal dependencies in data, addressing challenges such as seasonality, trend, and autocorrelation. ARIMA, state-space models, and spectral analysis provide frameworks for forecasting and inference, while modern approaches incorporate machine learning for nonlinear patterns. Proper stationarity assessment, model diagnostics, and out-of-sample validation ensure reliable forecasting. Applications span economics, climate science, and epidemiology, where temporal dynamics are central to understanding and prediction.",
    "label": "ai"
  },
  {
    "text": "Incentive-based fisheries management—such as catch shares, performance payments, and conservation subsidies—aligns economic interests with sustainability objectives. Well-designed incentives reward stewardship and innovation while avoiding perverse outcomes that could undermine ecological goals. Piloting incentive mechanisms and rigorous evaluation of ecological and socio-economic impacts informs scale-up. Combining incentives with regulatory baselines and enforcement strengthens overall fisheries governance frameworks.",
    "label": "ai"
  },
  {
    "text": "Neural coding investigates how populations of neurons represent sensory inputs, motor plans, and cognitive variables through spiking patterns and population dynamics. Techniques such as electrophysiology, calcium imaging, and computational modeling reveal coding strategies including rate codes, temporal codes, and population-level representations. Understanding neural coding informs brain–machine interface design and theories of perception and action. Challenges include recording from sufficiently large neural populations and linking coding schemes to behaviorally relevant signals.",
    "label": "ai"
  },
  {
    "text": "Health equity entails reducing avoidable and unjust disparities in health outcomes across socio-economic groups, geographies, and demographic categories. Policy approaches include universal coverage, targeted interventions for marginalized populations, and addressing social determinants such as housing, education, and income. Data disaggregation and community engagement guide tailored interventions, while accountability mechanisms monitor progress. Embedding equity considerations into policy design improves overall population health and social justice.",
    "label": "ai"
  },
  {
    "text": "Estimating price elasticities informs demand forecasting, taxation policy, and antitrust analysis by quantifying consumer responsiveness to price changes. Identification strategies include instrumental variables, natural experiments, and structural demand models that address endogeneity from price-setting behaviors. Heterogeneity in elasticities across income groups and product categories affects welfare analyses and distributional consequences. Robustness checks and sensitivity analyses are essential to validate estimates used in policy and business decisions.",
    "label": "ai"
  },
  {
    "text": "Privacy-preserving computation enables analysis over sensitive data without exposing raw inputs, using techniques such as secure multi-party computation, homomorphic encryption, and trusted execution environments. These methods facilitate collaborative analytics across organizations in domains like healthcare and finance while maintaining confidentiality. Trade-offs include computational overhead, complexity of protocol design, and trust assumptions. Practical deployment hinges on standardization, usability, and integration with legal frameworks governing data sharing and consent.",
    "label": "ai"
  },
  {
    "text": "Climate-smart agriculture (CSA) aims to increase productivity, enhance resilience, and reduce greenhouse gas emissions through practices such as improved nutrient management, agroforestry, and precision irrigation. CSA emphasizes context-specific solutions that balance mitigation and adaptation while supporting smallholder livelihoods. Scaling CSA requires institutional support, access to climate information services, and incentives aligned with environmental and development objectives. Monitoring and evaluation frameworks assess emissions, productivity, and socio-economic outcomes to inform policy and investment decisions.",
    "label": "ai"
  },
  {
    "text": "Anti-corruption strategies combine transparency measures, accountability institutions, and incentives for ethical behavior to reduce rent-seeking and improve governance. Mechanisms include open contracting, asset disclosure, independent oversight bodies, and digital public services that reduce discretionary decision-making. Building political will, strengthening judicial independence, and protecting whistleblowers are essential components. Evaluating anti-corruption interventions requires realistic outcome measures and long-term commitment to institutional reforms.",
    "label": "ai"
  },
  {
    "text": "Phenological shifts—changes in timing of biological events such as flowering and migration—result from climate change and can disrupt ecological interactions like pollination and predator–prey dynamics. Long-term monitoring networks and citizen science programs document phenological trends, supporting models that predict ecological mismatches. Conservation strategies that increase habitat heterogeneity and connectivity provide buffers against timing shifts, while management plans incorporate phenological data into adaptive conservation actions.",
    "label": "ai"
  },
  {
    "text": "Asset pricing anomalies—such as momentum, value, and size effects—challenge standard efficient-market models and motivate theories incorporating behavioral biases, market frictions, or risk-based explanations. Empirical work tests anomaly persistence across markets and time, while asset-pricing models that integrate microstructure and investor behavior provide explanatory frameworks. Practical implications influence portfolio construction and risk management, but strategies exploiting anomalies must account for transaction costs and model robustness.",
    "label": "ai"
  },
  {
    "text": "Vaccine hesitancy arises from confidence, complacency, and convenience factors that influence individuals’ decisions to vaccinate. Interventions that build trust—through transparent communication, engagement with community leaders, and addressing misinformation—improve uptake. Reducing access barriers and simplifying service delivery enhance convenience. Monitoring attitudes and tailoring strategies to local socio-cultural contexts ensures relevance. Ethical considerations emphasize respecting autonomy while promoting public health through empathetic and evidence-based outreach.",
    "label": "ai"
  },
  {
    "text": "Adversarial robustness studies vulnerabilities of machine learning models to perturbed inputs designed to induce misclassification, with implications for security-critical applications. Defense strategies include adversarial training, input preprocessing, and certified robustness methods that provide provable guarantees under specified threat models. Evaluating robustness requires realistic threat scenarios and metrics that reflect transferability of attacks. Balancing robustness with model accuracy and computational tractability remains an active research area with practical deployment challenges.",
    "label": "ai"
  },
  {
    "text": "Urban flood risk increases with impervious surface expansion, inadequate drainage, and intensifying precipitation patterns due to climate change. Integrated flood risk management combines gray infrastructure—such as upgraded drainage and retention basins—with nature-based solutions like permeable pavements, green roofs, and restored floodplains. Land-use planning that avoids development in flood-prone zones, early warning systems, and insurance mechanisms enhance resilience. Inclusive planning addresses the needs of vulnerable populations who face disproportionate flood exposure and limited recovery resources.",
    "label": "ai"
  }
]